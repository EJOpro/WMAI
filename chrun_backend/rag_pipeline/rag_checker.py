"""
RAG ê¸°ë°˜ ìœ„í—˜ë„ ì²´í¬ ëª¨ë“ˆ

ìƒˆë¡œìš´ ê¸€ì´ ë“¤ì–´ì˜¤ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³ , ê° ë¬¸ì¥ì„ ì„ë² ë”©í•˜ì—¬
ë²¡í„°DBì—ì„œ í™•ì¸ëœ(confirmed=true) ìœ ì‚¬ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ê²€ìƒ‰í•´
ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import time

# ì„¤ì • ìƒìˆ˜
MAX_SENTENCES_PER_QUERY = 3  # ë¬¸ì¥ë‹¹ ê²€ìƒ‰í•  ìµœëŒ€ ìœ ì‚¬ ë¬¸ì¥ ìˆ˜
MAX_TOTAL_EVIDENCE = 10      # ì „ì²´ ê·¼ê±°ë¡œ ì‚¬ìš©í•  ìµœëŒ€ ë¬¸ì¥ ìˆ˜
MIN_SIMILARITY_SCORE = 0.3   # ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜ (ì´ ì´ìƒë§Œ ê·¼ê±°ë¡œ ì‚¬ìš©)
MAX_SENTENCE_LENGTH = 500    # ì²˜ë¦¬í•  ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ (ë„ˆë¬´ ê¸´ ë¬¸ì¥ ì œì™¸)

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


def check_new_post(text: str, user_id: str, post_id: str, created_at: str) -> Dict[str, Any]:
    """
    ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì˜ ìœ„í—˜ë„ë¥¼ ì²´í¬í•˜ê¸° ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ë¶„ì„í•  ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸
        user_id (str): ì‚¬ìš©ì ID
        post_id (str): ê²Œì‹œë¬¼ ID  
        created_at (str): ìƒì„± ì‹œê°„ (ISO í˜•ì‹)
        
    Returns:
        Dict[str, Any]: ìœ„í—˜ë„ ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸
        {
            "post": {
                "user_id": str,
                "post_id": str, 
                "created_at": str,
                "original_text": str
            },
            "evidence": [
                {
                    "sentence": str,           # ì›ë³¸ ë¬¸ì¥
                    "risk_score": float,      # ì›ë˜ ìœ„í—˜ ì ìˆ˜
                    "matched_score": float,   # ìœ ì‚¬ë„ ì ìˆ˜
                    "matched_sentence": str,  # ë§¤ì¹­ëœ í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥
                    "matched_post_id": str,   # ë§¤ì¹­ëœ ë¬¸ì¥ì˜ ê²Œì‹œë¬¼ ID
                    "matched_created_at": str # ë§¤ì¹­ëœ ë¬¸ì¥ì˜ ìƒì„± ì‹œê°„
                },
                ...
            ],
            "stats": {
                "total_sentences": int,     # ì „ì²´ ë¬¸ì¥ ìˆ˜
                "total_matches": int,       # ë§¤ì¹­ëœ ë¬¸ì¥ ìˆ˜
                "max_score": float,         # ìµœê³  ìœ ì‚¬ë„ ì ìˆ˜
                "avg_score": float,         # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜
                "has_high_risk": bool       # ê³ ìœ„í—˜ ë¬¸ì¥ í¬í•¨ ì—¬ë¶€ (ìœ ì‚¬ë„ 0.7 ì´ìƒ)
            }
        }
    """
    
    logger.info(f"ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ì‹œì‘: post_id={post_id}, user_id={user_id}")
    
    # ì…ë ¥ ê²€ì¦
    if not text or not text.strip():
        logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return _create_empty_response(user_id, post_id, created_at, text)
    
    try:
        # 1. í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = _split_text_to_sentences(text, user_id, post_id, created_at)
        
        if not sentences:
            logger.warning("ë¶„í• ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return _create_empty_response(user_id, post_id, created_at, text)
        
        logger.info(f"í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(sentences)}ê°œ ë¬¸ì¥")
        
        # 2. ê° ë¬¸ì¥ë³„ë¡œ ìœ ì‚¬í•œ í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ ê²€ìƒ‰
        all_evidence = []
        
        for sentence_data in sentences:
            sentence = sentence_data.get('sentence', '')
            
            # ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ì œì™¸
            if len(sentence) > MAX_SENTENCE_LENGTH:
                logger.debug(f"ë¬¸ì¥ì´ ë„ˆë¬´ ê¹€: {len(sentence)}ê¸€ì (ìµœëŒ€: {MAX_SENTENCE_LENGTH})")
                continue
            
            # â­ ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ê°•í™”)
            prev_sentence = sentence_data.get('prev_sentence', '')
            next_sentence = sentence_data.get('next_sentence', '')
            
            # ë¬¸ì¥ë³„ ìœ ì‚¬ ìœ„í—˜ ë¬¸ì¥ ê²€ìƒ‰ (ë¬¸ë§¥ ì£¼ì…)
            evidence_for_sentence = _search_similar_risk_sentences(
                sentence=sentence,
                prev_sentence=prev_sentence,
                next_sentence=next_sentence,
                use_contextual=True  # ë¬¸ë§¥ ì£¼ì… í™œì„±í™”
            )
            
            # ChromaDBê°€ ì—†ì„ ë•Œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            if not evidence_for_sentence:
                evidence_for_sentence = _generate_test_evidence(sentence)
            
            # ì›ë³¸ ë¬¸ì¥ ì •ë³´ ì¶”ê°€
            for evidence in evidence_for_sentence:
                evidence['sentence'] = sentence
                
            all_evidence.extend(evidence_for_sentence)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_evidence = _deduplicate_and_sort_evidence(all_evidence)
        
        # 4. ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        final_evidence = unique_evidence[:MAX_TOTAL_EVIDENCE]
        
        # 5. í†µê³„ ê³„ì‚°
        stats = _calculate_stats(sentences, final_evidence)
        
        # 6. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = {
            "post": {
                "user_id": user_id,
                "post_id": post_id,
                "created_at": created_at,
                "original_text": text
            },
            "evidence": final_evidence,
            "stats": stats
        }
        
        # 7. LLM ê¸°ë°˜ ìµœì¢… ê²°ì • (â­ í•­ìƒ í˜¸ì¶œ - Evidence ì—†ì–´ë„ ì›ë¬¸ìœ¼ë¡œ íŒë‹¨)
        decision = None
        try:
            from .rag_decider import decide_with_rag
            
            # Evidence ìœ ë¬´ì— ë”°ë¥¸ ë¡œê·¸ ì¶œë ¥
            if not final_evidence:
                logger.info("[RAG] ChromaDBì— ìœ ì‚¬ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. LLMì´ ì›ë¬¸ë§Œìœ¼ë¡œ íŒë‹¨í•©ë‹ˆë‹¤.")
            else:
                logger.info(f"[RAG] {len(final_evidence)}ê°œ ìœ ì‚¬ ë¬¸ì¥ì„ ì°¸ê³ í•˜ì—¬ LLMì´ íŒë‹¨í•©ë‹ˆë‹¤.")
            
            # LLM í˜¸ì¶œ (Evidence ì—†ì–´ë„ ì›ë¬¸ ë¶„ì„ ê°€ëŠ¥)
            decision = decide_with_rag(context)
            logger.info(f"LLM ê²°ì • ì™„ë£Œ: risk_score={decision.get('risk_score')}, priority={decision.get('priority')}")
            
        except Exception as llm_error:
            logger.error(f"LLM ê²°ì • ì¤‘ ì˜¤ë¥˜: {llm_error}")
            import traceback
            traceback.print_exc()
            
            # LLM ì‹¤íŒ¨ ì‹œì—ë§Œ fallback ì‚¬ìš©
            if final_evidence:
                decision = _create_basic_decision(context)
            else:
                decision = _create_safe_decision()
        
        # 8. ìµœì¢… ì‘ë‹µ êµ¬ì„±
        final_response = {
            "post": context["post"],
            "decision": decision,
            "evidence": final_evidence
        }
        
        logger.info(f"ìœ„í—˜ë„ ì²´í¬ ì™„ë£Œ: {len(final_evidence)}ê°œ ê·¼ê±°, ìµœì¢… ìœ„í—˜ë„: {decision.get('risk_score', 0):.3f}")
        
        return final_response
        
    except Exception as e:
        logger.error(f"ìœ„í—˜ë„ ì²´í¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ì‘ë‹µ ë°˜í™˜
        return _create_empty_response(user_id, post_id, created_at, text, error=str(e))


def _split_text_to_sentences(text: str, user_id: str, post_id: str, created_at: str) -> List[Dict[str, Any]]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ë¶„í• í•  í…ìŠ¤íŠ¸
        user_id (str): ì‚¬ìš©ì ID
        post_id (str): ê²Œì‹œë¬¼ ID
        created_at (str): ìƒì„± ì‹œê°„
        
    Returns:
        List[Dict[str, Any]]: ë¶„í• ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from .text_splitter import split_text_to_sentences
        from datetime import datetime
        
        # created_at ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
        try:
            dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        except:
            dt = datetime.now()
        
        sentences = split_text_to_sentences(
            text=text,
            user_id=user_id,
            post_id=post_id,
            created_at=dt
        )
        
        return sentences
        
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def _search_similar_risk_sentences(
    sentence: str,
    prev_sentence: str = "",
    next_sentence: str = "",
    use_contextual: bool = True
) -> List[Dict[str, Any]]:
    """
    ì£¼ì–´ì§„ ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    (â­ ë¬¸ë§¥ ì£¼ì… ì„ë² ë”© ì§€ì›)
    
    Args:
        sentence (str): ê²€ìƒ‰í•  í•µì‹¬ ë¬¸ì¥
        prev_sentence (str): ì´ì „ ë¬¸ì¥ (ë¬¸ë§¥ ì œê³µ)
        next_sentence (str): ë‹¤ìŒ ë¬¸ì¥ (ë¬¸ë§¥ ì œê³µ)
        use_contextual (bool): ë¬¸ë§¥ ì£¼ì… ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        
    Returns:
        List[Dict[str, Any]]: ìœ ì‚¬í•œ ìœ„í—˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ì„ë² ë”© ìƒì„± (ë¬¸ë§¥ ì£¼ì… ë°©ì‹ ìš°ì„ )
        embed_start = time.perf_counter()
        
        if use_contextual and (prev_sentence or next_sentence):
            # â­ ë¬¸ë§¥ ì£¼ì… ì„ë² ë”© ì‚¬ìš©
            from .embedding_service import get_contextual_embedding
            embedding = get_contextual_embedding(
                sentence=sentence,
                prev_sentence=prev_sentence,
                next_sentence=next_sentence,
                context_format="structured"
            )
            logger.info("[RAG] ë¬¸ë§¥ ì£¼ì… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        else:
            # ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©
            from .embedding_service import get_embedding
            embedding = get_embedding(sentence)
            logger.info("[RAG] ê¸°ë³¸ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        embed_elapsed = (time.perf_counter() - embed_start) * 1000
        logger.info("[RAG] ì„ë² ë”© ìƒì„± ì‹œê°„: %.2f ms", embed_elapsed)
        
        # ë²¡í„°DBì—ì„œ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰
        from .vector_db import get_client, search_similar
        client = get_client()
        search_start = time.perf_counter()
        similar_results = search_similar(
            client=client,
            embedding=embedding,
            top_k=MAX_SENTENCES_PER_QUERY,
            min_score=MIN_SIMILARITY_SCORE
        )
        search_elapsed = (time.perf_counter() - search_start) * 1000
        logger.info(
            "[RAG] ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ì™„ë£Œ: %dê±´ (%.2f ms)",
            len(similar_results),
            search_elapsed
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        evidence_list = []
        for result in similar_results:
            metadata = result.get('metadata', {})
            
            evidence = {
                "risk_score": float(metadata.get('risk_score', 0.0)),
                "matched_score": float(result.get('score', 0.0)),
                "matched_sentence": result.get('document', ''),
                "matched_post_id": metadata.get('post_id', ''),
                "matched_created_at": metadata.get('created_at', ''),
                "matched_user_id": metadata.get('user_id', ''),
                "vector_chunk_id": result.get('id', '')
            }
            evidence_list.append(evidence)
        
        logger.debug(f"ë¬¸ì¥ '{sentence[:30]}...'ì— ëŒ€í•´ {len(evidence_list)}ê°œ ìœ ì‚¬ ë¬¸ì¥ ë°œê²¬")
        
        return evidence_list
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def _generate_test_evidence(sentence: str) -> List[Dict[str, Any]]:
    """
    ChromaDBê°€ ì—†ì„ ë•Œ í…ŒìŠ¤íŠ¸ìš© ì¦ê±° ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    í†¤ ë¶„ì„ê³¼ ëŒ€ì•ˆ ì œì‹œ ê°ì§€ë¥¼ í†µí•´ ë” ì •í™•í•œ ì¦ê±°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        sentence (str): ë¶„ì„í•  ë¬¸ì¥
        
    Returns:
        List[Dict[str, Any]]: í…ŒìŠ¤íŠ¸ ì¦ê±° ë¦¬ìŠ¤íŠ¸
    """
    evidence_list = []
    sentence_lower = sentence.lower()
    
    # ğŸ¯ í†¤ ë¶„ì„: ë†ë‹´/ì¥ë‚œ ê°ì§€
    joke_indicators = ['ã…‹', 'ã…', '~', 'êµ¬ë¼', 'ë°©êµ¬', 'ë¿¡', 'í—¤í—¤', 'í˜¸í˜¸', 'ì¥ë‚œ', 'ã„±ã…‡ã„¹ã…‡']
    is_joking = any(indicator in sentence for indicator in joke_indicators)
    
    # ğŸ¯ ëŒ€ì•ˆ ì œì‹œ ê°ì§€ (ê°•ë ¥í•œ ì´íƒˆ ì‹ í˜¸)
    alternative_keywords = ['ë‹¤ë¥¸ ê³³', 'ì•„ë‹ˆì–´ë„', 'ë§ì•„ìš”', 'ë‹¤ë¥¸ ë°', 'ì˜®ê¸¸', 'ì´ë™í• ', 'ë– ë‚ ']
    has_alternative = any(keyword in sentence_lower for keyword in alternative_keywords)
    
    # ğŸ¯ ì‹¤ë§/ë¶ˆë§Œ í‘œí˜„ ê°ì§€
    disappointment_keywords = ['ì‹¤ë§', 'ì–´ì´ì—†', 'ì•ˆí•´ì£¼', 'ì™œ ì•ˆ', 'ì§„ì§œ', 'ì •ë§', 'ì•„ë‹ˆ']
    is_disappointed = any(keyword in sentence_lower for keyword in disappointment_keywords)
    
    # ëŒ€ì•ˆ ì œì‹œê°€ ìˆìœ¼ë©´ ë†’ì€ ìœ„í—˜ë„ ì¦ê±° ì¶”ê°€ (ìš°ì„ ìˆœìœ„ ìµœìƒ)
    if has_alternative:
        evidence_list.append({
            "risk_score": 0.88,
            "matched_score": 0.92,
            "matched_sentence": "ì—¬ê¸°ê°€ ì•„ë‹ˆì–´ë„ í™œë™í•  ê³³ì€ ë§ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì»¤ë®¤ë‹ˆí‹°ë¡œ ì˜®ê¸°ë ¤ê³ ìš”.",
            "matched_post_id": "demo_post_alternative",
            "matched_created_at": "2024-10-31T14:00:00",
            "matched_user_id": "demo_user_alt",
            "vector_chunk_id": "test_alternative"
        })
    
    # ì‹¤ë§ + ëŒ€ì•ˆ ì¡°í•©ì€ ë” ë†’ì€ ìœ„í—˜
    if is_disappointed and has_alternative:
        evidence_list.append({
            "risk_score": 0.91,
            "matched_score": 0.90,
            "matched_sentence": "ì§„ì§œ ì‹¤ë§í–ˆì–´ìš”. ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ê°ˆë˜ìš”.",
            "matched_post_id": "demo_post_disappointed_alt",
            "matched_created_at": "2024-10-31T14:15:00",
            "matched_user_id": "demo_user_disappointed",
            "vector_chunk_id": "test_disappointed_alt"
        })
    
    # ë†ë‹´ í†¤ì´ë©´ ìœ„í—˜ë„ ë‚®ì€ í‚¤ì›Œë“œë¡œ ë§¤ì¹­
    if is_joking:
        risk_keywords = {
            "íƒˆí‡´": ["íƒˆí‡´í•œë‹¤ê³  ë†ë‹´í–ˆì–´ìš” ã…‹ã…‹ ì‚¬ì‹¤ ì¬ë¯¸ìˆì–´ìš”", 0.18, 0.35],
            "ê·¸ë§Œ": ["ê·¸ë§Œë‘”ë‹¤ê³  ì¥ë‚œì³¤ì–´ìš”~ ê³„ì† ì“¸ ê±°ì˜ˆìš”", 0.22, 0.38],
            "ë– ë‚ ": ["ë– ë‚œë‹¤ê³  í–ˆì§€ë§Œ ë†ë‹´ì´ì—ìš” ã…ã…", 0.20, 0.33],
        }
    else:
        # ì§„ì§€í•œ í†¤ì¼ ë•ŒëŠ” ì›ë˜ ìœ„í—˜ë„ ìœ ì§€
        risk_keywords = {
            "íƒˆí‡´": ["íƒˆí‡´í• ê¹Œ ìƒê°ì¤‘ì…ë‹ˆë‹¤ ì§„ì§œë¡œ ë” ì´ìƒ ì˜ë¯¸ê°€ ì—†ëŠ” ê²ƒ ê°™ì•„ìš”", 0.87, 0.85],
            "ê·¸ë§Œ": ["ì´ ì„œë¹„ìŠ¤ ê·¸ë§Œ ì“¸ê¹Œ ë´ìš” ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê²Œ ë‚˜ì„ ê²ƒ ê°™ì•„ì„œ", 0.79, 0.82],
            "ë– ë‚ ": ["ì—¬ê¸° ë” ìˆì–´ì•¼ í•  ì´ìœ ê°€ ì—†ë‹¤ê³  ìƒê°í•´ìš” ë– ë‚  ë•Œê°€ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤", 0.83, 0.78],
            "ì‚­ì œ": ["ê³„ì • ì‚­ì œí•˜ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•˜ë‚˜ìš”? ë” ì´ìƒ ì‚¬ìš©í•  ì¼ì´ ì—†ì„ ê²ƒ ê°™ì•„ìš”", 0.75, 0.80],
            "ê·¸ë§Œë‘˜": ["ì´ì œ ì •ë§ ê·¸ë§Œë‘˜ ë•Œê°€ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤ ë‹¤ë¥¸ ëŒ€ì•ˆì„ ì°¾ì•„ë³´ê³  ìˆì–´ìš”", 0.72, 0.75],
            "ì‚¬ìš©í•˜ê³  ì‹¶ì§€ ì•Š": ["ë” ì´ìƒ ì´ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ì§€ ì•Šì•„ìš”", 0.91, 0.88],
            "ë³„ë¡œ": ["ì„œë¹„ìŠ¤ í’ˆì§ˆì´ ë„ˆë¬´ ë–¨ì–´ì ¸ì„œ ì´íƒˆì„ ê³ ë ¤í•˜ê³  ìˆìŠµë‹ˆë‹¤", 0.68, 0.65],
            "ì•„ì‰¬ì›Œ": ["ì„œë¹„ìŠ¤ê°€ ì¢€ ì•„ì‰¬ì›Œìš” ê°œì„ ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤", 0.60, 0.55]
        }
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì¥ ì°¾ê¸°
    for keyword, (matched_sentence, risk_score, similarity) in risk_keywords.items():
        if keyword in sentence_lower:
            evidence = {
                "risk_score": risk_score,
                "matched_score": similarity,
                "matched_sentence": matched_sentence,
                "matched_post_id": f"demo_post_{hash(matched_sentence) % 1000}",
                "matched_created_at": "2024-10-31T13:45:00",
                "matched_user_id": f"demo_user_{hash(matched_sentence) % 100}",
                "vector_chunk_id": f"test_{hash(matched_sentence):08x}"
            }
            evidence_list.append(evidence)
    
    # ë¶ˆë§Œ í‘œí˜„ ê°ì§€ (ì§„ì§€í•œ í†¤ì¼ ë•Œë§Œ)
    if not is_joking and len(sentence) > 20:
        if any(word in sentence_lower for word in ["ì•ˆ", "ëª»", "ì–´ë ¤ì›Œ", "í˜ë“¤ì–´"]):
            evidence_list.append({
                "risk_score": 0.55,
                "matched_score": 0.60,
                "matched_sentence": "ì„œë¹„ìŠ¤ ì´ìš©ì´ ì–´ë ¤ì›Œì„œ ë‹¤ë¥¸ ê³³ì„ ì•Œì•„ë³´ê³  ìˆì–´ìš”",
                "matched_post_id": "demo_post_difficulty",
                "matched_created_at": "2024-10-31T12:30:00",
                "matched_user_id": "demo_user_difficulty",
                "vector_chunk_id": "test_difficulty"
            })
    
    # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
    evidence_list.sort(key=lambda x: x['matched_score'], reverse=True)
    
    # ìµœëŒ€ 5ê°œê¹Œì§€ ë°˜í™˜ (ë” ë§ì€ ì¦ê±° ì œê³µ)
    return evidence_list[:5]


def _deduplicate_and_sort_evidence(evidence_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ê·¼ê±° ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    
    Args:
        evidence_list (List[Dict[str, Any]]): ì›ë³¸ ê·¼ê±° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[Dict[str, Any]]: ì¤‘ë³µ ì œê±° ë° ì •ë ¬ëœ ê·¼ê±° ë¦¬ìŠ¤íŠ¸
    """
    # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ (matched_sentenceë¥¼ í‚¤ë¡œ ì‚¬ìš©)
    unique_evidence = {}
    
    for evidence in evidence_list:
        matched_sentence = evidence.get('matched_sentence', '')
        matched_score = evidence.get('matched_score', 0.0)
        
        # ë™ì¼í•œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ê²ƒì„ ìœ ì§€
        if matched_sentence in unique_evidence:
            existing_score = unique_evidence[matched_sentence].get('matched_score', 0.0)
            if matched_score > existing_score:
                unique_evidence[matched_sentence] = evidence
        else:
            unique_evidence[matched_sentence] = evidence
    
    # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_evidence = sorted(
        unique_evidence.values(),
        key=lambda x: x.get('matched_score', 0.0),
        reverse=True
    )
    
    logger.debug(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(evidence_list)} -> {len(sorted_evidence)}ê°œ")
    
    return sorted_evidence


def _calculate_stats(sentences: List[Dict[str, Any]], evidence: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ë¶„ì„ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        sentences (List[Dict[str, Any]]): ì›ë³¸ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        evidence (List[Dict[str, Any]]): ê·¼ê±° ë¦¬ìŠ¤íŠ¸
        
    Returns:
        Dict[str, Any]: í†µê³„ ì •ë³´
    """
    total_sentences = len(sentences)
    total_matches = len(evidence)
    
    if total_matches == 0:
        return {
            "total_sentences": total_sentences,
            "total_matches": 0,
            "max_score": 0.0,
            "avg_score": 0.0,
            "has_high_risk": False
        }
    
    scores = [e.get('matched_score', 0.0) for e in evidence]
    max_score = max(scores)
    avg_score = sum(scores) / len(scores)
    has_high_risk = max_score >= 0.7  # ìœ ì‚¬ë„ 0.7 ì´ìƒì„ ê³ ìœ„í—˜ìœ¼ë¡œ íŒë‹¨
    
    return {
        "total_sentences": total_sentences,
        "total_matches": total_matches,
        "max_score": max_score,
        "avg_score": avg_score,
        "has_high_risk": has_high_risk
    }


def _create_empty_response(user_id: str, post_id: str, created_at: str, text: str, error: Optional[str] = None) -> Dict[str, Any]:
    """
    ë¹ˆ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        user_id (str): ì‚¬ìš©ì ID
        post_id (str): ê²Œì‹œë¬¼ ID
        created_at (str): ìƒì„± ì‹œê°„
        text (str): ì›ë³¸ í…ìŠ¤íŠ¸
        error (str, optional): ì˜¤ë¥˜ ë©”ì‹œì§€
        
    Returns:
        Dict[str, Any]: ë¹ˆ ì‘ë‹µ
    """
    response = {
        "post": {
            "user_id": user_id,
            "post_id": post_id,
            "created_at": created_at,
            "original_text": text
        },
        "decision": _create_safe_decision(),
        "evidence": []
    }
    
    if error:
        response["error"] = error
    
    return response


def _create_safe_decision() -> Dict[str, Any]:
    """
    ì•ˆì „í•œ ê¸°ë³¸ ê²°ì •ì„ ìƒì„±í•©ë‹ˆë‹¤ (evidenceê°€ ì—†ì„ ë•Œ).
    
    Returns:
        Dict[str, Any]: ì•ˆì „í•œ ê¸°ë³¸ ê²°ì •
    """
    return {
        "risk_score": 0.1,
        "priority": "LOW",
        "reasons": [
            "ìœ ì‚¬í•œ ìœ„í—˜ ë¬¸ì¥ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ",
            "ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨ë¨"
        ],
        "actions": [
            "ì •ìƒ ëª¨ë‹ˆí„°ë§ ìœ ì§€",
            "ì¶”ê°€ ì¡°ì¹˜ ë¶ˆí•„ìš”"
        ],
        "evidence_ids": [],
        "confidence": "Uncertain"
    }


def _create_basic_decision(context: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLM ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ê¸°ë³¸ ê²°ì •ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        context (Dict[str, Any]): ë¶„ì„ ì»¨í…ìŠ¤íŠ¸
        
    Returns:
        Dict[str, Any]: ê¸°ë³¸ ê²°ì •
    """
    stats = context.get("stats", {})
    evidence_list = context.get("evidence", [])
    
    # í†µê³„ ê¸°ë°˜ ê°„ë‹¨í•œ ìœ„í—˜ë„ ê³„ì‚°
    max_score = stats.get("max_score", 0.0)
    total_matches = stats.get("total_matches", 0)
    has_high_risk = stats.get("has_high_risk", False)
    
    # ìœ„í—˜ë„ ì ìˆ˜ ê²°ì •
    if has_high_risk and max_score >= 0.8:
        risk_score = 0.8
        priority = "HIGH"
        reasons = [
            f"ê³ ìœ„í—˜ íŒ¨í„´ ê°ì§€ (ìœ ì‚¬ë„ {max_score:.3f})",
            f"í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ {total_matches}ê°œ ë§¤ì¹­",
            "ì¦‰ì‹œ ì£¼ì˜ í•„ìš”"
        ]
        actions = [
            "ì¦‰ì‹œ ê³ ê° ì§€ì›íŒ€ ì—°ë½",
            "ì‚¬ìš©ì í™œë™ ëª¨ë‹ˆí„°ë§ ê°•í™”",
            "ê°œì¸í™”ëœ ë¦¬í…ì…˜ í”„ë¡œê·¸ë¨ ì ìš©"
        ]
    elif total_matches >= 2 and max_score >= 0.5:
        risk_score = 0.6
        priority = "MEDIUM"
        reasons = [
            f"ì¤‘ê°„ ìœ„í—˜ë„ íŒ¨í„´ ê°ì§€ (ìœ ì‚¬ë„ {max_score:.3f})",
            f"ìœ„í—˜ ë¬¸ì¥ {total_matches}ê°œ ë§¤ì¹­"
        ]
        actions = [
            "ì˜ˆë°©ì  ì†Œí†µ í”„ë¡œê·¸ë¨ ì ìš©",
            "ê³ ê° ë§Œì¡±ë„ ì¡°ì‚¬ ì‹¤ì‹œ"
        ]
    elif total_matches >= 1:
        risk_score = 0.4
        priority = "MEDIUM"
        reasons = [
            f"ë‚®ì€ ìœ„í—˜ë„ íŒ¨í„´ ê°ì§€ (ìœ ì‚¬ë„ {max_score:.3f})",
            "ì£¼ì˜ ê´€ì°° í•„ìš”"
        ]
        actions = [
            "ì‚¬ìš©ì í™œë™ ëª¨ë‹ˆí„°ë§",
            "ì„œë¹„ìŠ¤ ê°œì„  í”¼ë“œë°± ìˆ˜ì§‘"
        ]
    else:
        return _create_safe_decision()
    
    # ì¦ê±° ID ìˆ˜ì§‘
    evidence_ids = [e.get("vector_chunk_id", "")[:8] for e in evidence_list[:3] if e.get("vector_chunk_id")]
    
    return {
        "risk_score": risk_score,
        "priority": priority,
        "reasons": reasons[:4],  # ìµœëŒ€ 4ê°œ
        "actions": actions[:4],  # ìµœëŒ€ 4ê°œ
        "evidence_ids": evidence_ids,
        "fallback_reason": "LLM ë¶„ì„ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ë¡œì§ ì‚¬ìš©"
    }


# í¸ì˜ í•¨ìˆ˜ë“¤
def get_vector_db_stats() -> Dict[str, Any]:
    """
    ë²¡í„°DB í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    
    Returns:
        Dict[str, Any]: ë²¡í„°DB í†µê³„
    """
    try:
        from .vector_db import get_client, get_collection_stats
        client = get_client()
        stats = get_collection_stats(client)
        return stats
    except Exception as e:
        logger.error(f"ë²¡í„°DB í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"error": str(e)}


def test_similarity_search(test_sentence: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    í…ŒìŠ¤íŠ¸ìš© ìœ ì‚¬ë„ ê²€ìƒ‰ í•¨ìˆ˜
    
    Args:
        test_sentence (str): í…ŒìŠ¤íŠ¸í•  ë¬¸ì¥
        top_k (int): ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
        
    Returns:
        List[Dict[str, Any]]: ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        evidence = _search_similar_risk_sentences(test_sentence)
        return evidence[:top_k]
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
