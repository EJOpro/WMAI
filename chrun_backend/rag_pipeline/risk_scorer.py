"""
ìœ„í—˜ ì ìˆ˜ ê³„ì‚°ê¸° ëª¨ë“ˆ
ê° ë¬¸ì¥ì— ëŒ€í•´ ì´íƒˆ ìœ„í—˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , ê³ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv
from pydantic_settings import BaseSettings

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
try:
    import openai
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    
    if OPENAI_API_KEY:
        openai.api_key = OPENAI_API_KEY
        print("[INFO] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("[WARN] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
except ImportError:
    print("[WARN] openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    openai = None

class RiskThresholdSettings(BaseSettings):
    rag_risk_threshold: Optional[float] = None
    risk_threshold: float = 0.70  # ê³ ìœ„í—˜ íŒë‹¨ ì„ê³„ê°’ (0.70 ì´ìƒ)

    class Config:
        env_file = ".env"
        case_sensitive = False
        extra = "allow"


def _resolve_threshold() -> float:
    env_candidates = [
        os.getenv("RAG_THRESHOLD"),
        os.getenv("RAG_RISK_THRESHOLD"),
        os.getenv("RISK_THRESHOLD"),
    ]
    for value in env_candidates:
        if value is None:
            continue
        try:
            return float(value)
        except ValueError:
            continue

    settings = RiskThresholdSettings()
    if settings.rag_risk_threshold is not None:
        return float(settings.rag_risk_threshold)
    return float(settings.risk_threshold)


# ê³ ìœ„í—˜ ë¬¸ì¥ íŒë‹¨ ì„ê³„ê°’
THRESHOLD = _resolve_threshold()
_THRESHOLD_LOGGED = False


class RiskScorer:
    """
    ë¬¸ì¥ë³„ ì´íƒˆ ìœ„í—˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        global _THRESHOLD_LOGGED
        if not _THRESHOLD_LOGGED:
            print(f"[INFO] RiskScorer THRESHOLD ì ìš©: {THRESHOLD:.2f}")
            _THRESHOLD_LOGGED = True

        # â­ LLM ë¶„ì„ ê²°ê³¼ ìºì‹œ (ê°™ì€ ë¬¸ì¥ì€ ê°™ì€ ê²°ê³¼ ë°˜í™˜)
        self._analysis_cache = {}
        
        # ìœ„í—˜ í‚¤ì›Œë“œ íŒ¨í„´ë“¤ (ì¶”í›„ í™•ì¥ ê°€ëŠ¥)
        # | êµ¬ë¶„        | ê°€ì¤‘ì¹˜ | ì˜ˆì‹œ í‚¤ì›Œë“œ(í™•ì¥ë¨)                                   |
        # | HIGH       | +0.45  | íƒˆí‡´, ê·¸ë§Œë‘˜, ìµœì•…, êº¼ì ¸, ì§€ì˜¥, í™˜ë©¸                  |
        # | ABUSIVE    | +0.35  | ê°œê°™, ë³‘ì‹ , ë¯¸ì¹œë†ˆ, ì£½ì–´, ì—¿ê°™, ë¹¡ì¹˜                  |
        # | MEDIUM     | +0.25  | í˜ë“¤ì–´, ë‹µë‹µí•´, í¬ê¸°í• ê¹Œ, ë‹¤ë¥¸ ê³³, ë¶ˆí¸               |
        # | LOW(ì™„ì¶©)  | -0.20  | ë§Œì¡±, ê³ ë§ˆì›Œ, ê¸°ëŒ€, ì¬ë°Œ, ì¶”ì²œ, ë„ì›€                 |
        self.keyword_profiles = [
            {
                "level": "HIGH",
                "weight": 0.45,
                "keywords": [
                    'ê·¸ë§Œë‘˜', 'í¬ê¸°', 'ë– ë‚ ', 'ë‚˜ê°ˆ', 'ì‹«ì–´', 'ì§œì¦', 'í™”ë‚˜', 'ì‹¤ë§',
                    'ì˜ë¯¸ì—†', 'ì†Œìš©ì—†', 'í—›ëœ', 'ì‹œê°„ë‚­ë¹„', 'ë³„ë¡œ', 'ìµœì•…', 'íƒˆí‡´', 'ì ‘ì„ê¹Œ',
                    'í™˜ë©¸', 'ì§€ì˜¥', 'ë¶ˆë§¤', 'ëª»í•´ë¨¹', 'ì°¨ë‹¨', 'ë°´', 'ê°•í‡´', 'ì«“ê²¨'
                ],
            },
            {
                "level": "ABUSIVE",
                "weight": 0.35,
                "keywords": [
                    'êº¼ì ¸', 'ì£½ì–´', 'ë¯¸ì¹œë†ˆ', 'ë¯¸ì¹œë…„', 'ê°œê°™', 'ë³‘ì‹ ', 'ì”¨ë°œ', 'ì¢†ê°™',
                    'ë¹¡ì¹˜', 'ì§€ë„', 'ì—¿ê°™', 'ëŒì•„ì´', 'ì“°ë ˆê¸°', 'ë©ì²­ì´', 'ë„ë', 'ê°œë¹¡ì¹˜'
                ],
            },
            {
                "level": "MEDIUM",
                "weight": 0.25,
                "keywords": [
                    'ì–´ë ¤ì›Œ', 'í˜ë“¤ì–´', 'ë³µì¡í•´', 'ëª¨ë¥´ê² ', 'ì´í•´ì•ˆë¼', 'ë‹µë‹µí•´',
                    'ì§€ì³', 'í”¼ê³¤í•´', 'ê·€ì°®ì•„', 'ë²ˆê±°ë¡œì›Œ', 'ì§œì¦ë‚˜', 'ì—´ë°›', 'ë‹¤ë¥¸ ì„œë¹„ìŠ¤',
                    'ëŒ€ì•ˆ', 'í¬ê¸°í• ê¹Œ', 'ê°ˆì•„íƒˆ', 'ë§ˆìŒì´ ë– ë‚¬', 'ë‹¤ë¥¸ ê³³', 'ì˜®ê¸¸', 'ì´ë™í• ',
                    'ì •ì§€', 'ì œì¬', 'ë¶ˆê³µì •', 'ì–µìš¸'
                ],
            },
            {
                "level": "LOW",
                "weight": -0.2,
                "keywords": [
                    'ê´œì°®', 'ì¢‹ì•„', 'ì¬ë¯¸ìˆ', 'í¥ë¯¸ë¡œ', 'ë„ì›€', 'ìœ ìš©í•´',
                    'ë§Œì¡±', 'í–‰ë³µ', 'ì¦ê±°ì›Œ', 'ê¸°ëŒ€ë¼', 'ê°ì‚¬', 'ì¶”ì²œ', 'ê³ ë§™', 'ì‚¬ë‘',
                    'ê¸°ì¨', 'ë¿Œë“¯', 'ë“ ë“ '
                ],
            },
        ]
        
    def score_sentences(
        self, 
        sentences: List[Dict[str, Any]], 
        store_high_risk: bool = False  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½ (DB ì €ì¥ ì•ˆí•¨)
    ) -> Dict[str, Any]:
        """
        ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ì´íƒˆ ìœ„í—˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶”ê°€
        
        Args:
            sentences (List[Dict]): ë¬¸ì¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
                ê° ë”•ì…”ë„ˆë¦¬ëŠ” ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•¨:
                - sentence: ë¬¸ì¥ ë‚´ìš©
                - user_id: ì‚¬ìš©ì ID (ì„ íƒ)
                - post_id: ê²Œì‹œê¸€ ID (ì„ íƒ)
                - created_at: ìƒì„± ì‹œê°„ (ì„ íƒ)
                - sentence_index: ë¬¸ì¥ ìˆœì„œ (ì„ íƒ)
            store_high_risk (bool): ê³ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„° DBì— ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
                
        Returns:
            Dict[str, Any]: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                - all_scored: ìœ„í—˜ ì ìˆ˜ê°€ ì¶”ê°€ëœ ëª¨ë“  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
                - high_risk_candidates: ì„ê³„ê°’ì„ ë„˜ì€ ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ ë¦¬ìŠ¤íŠ¸
        """
        scored_sentences = []
        high_risk_candidates = []  # ì„ê³„ê°’ì„ ë„˜ì€ ê³ ìœ„í—˜ ë¬¸ì¥ë“¤
        
        print(f"[INFO] {len(sentences)}ê°œ ë¬¸ì¥ì— ëŒ€í•œ ì´íƒˆ ìœ„í—˜ë„ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...", flush=True)
        
        for i, sentence_data in enumerate(sentences):
            sentence = sentence_data.get('sentence', '')
            
            print(f"[INFO] ë¬¸ì¥ {i+1}/{len(sentences)} ë¶„ì„ ì¤‘: {sentence[:50]}...", flush=True)
            
            # â­ ë¬¸ë§¥ ì •ë³´ ì¶”ì¶œ (ì œëª© + ì´ì „/ë‹¤ìŒ ë¬¸ì¥)
            # ì œëª©ì„ ì´ì „ ë¬¸ì¥ ì•ì— ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”í•œ ë¬¸ë§¥)
            title = sentence_data.get('title', '')
            prev_sentence = sentence_data.get('prev_sentence', '')
            next_sentence = sentence_data.get('next_sentence', '')
            
            # â­ ë””ë²„ê·¸: ì œëª© í™•ì¸
            if title:
                print(f"[DEBUG] ì œëª© ê°ì§€ë¨: '{title}' (ë¬¸ì¥: {sentence[:30]}...)", flush=True)
            
            # ì œëª©ì´ ìˆìœ¼ë©´ ë¬¸ë§¥ ê°•í™”
            if title and prev_sentence:
                prev_sentence = f"[ì œëª©: {title}] {prev_sentence}"
            elif title:
                prev_sentence = f"[ì œëª©: {title}]"
            
            # ì‹¤ì œ LLMì„ ì‚¬ìš©í•œ ìœ„í—˜ ì ìˆ˜ ê³„ì‚° (ë¬¸ë§¥ ì •ë³´ í¬í•¨)
            # ì´ ë¶€ë¶„ì€ ì‹¤ì œ LLM í˜¸ì¶œì´ë©°, ìš´ì˜ ì‹œ ë¹„ìš©ì´ ë“ ë‹¤
            analysis = self.score_sentence(sentence, prev_sentence, next_sentence)
            risk_score = analysis["risk_score"]
            risk_level = analysis["risk_level"]
            reasons = analysis["reasons"]
            
            # ê³ ìœ„í—˜ ë¬¸ì¥ íŒë‹¨
            is_high_risk = risk_score >= THRESHOLD
            
            # ê¸°ì¡´ ë°ì´í„°ì— ìœ„í—˜ ì ìˆ˜ ì •ë³´ ì¶”ê°€
            scored_data = sentence_data.copy()
            scored_data.update({
                'risk_score': risk_score,
                'risk_level': risk_level,
                'analyzed_at': datetime.now(),
                'is_high_risk': is_high_risk,
                'risk_factors': reasons,
                'reason': "; ".join(reasons)
            })
            
            scored_sentences.append(scored_data)
            
            # ê³ ìœ„í—˜ ë¬¸ì¥ì€ ë³„ë„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if is_high_risk:
                high_risk_candidates.append(scored_data)
                print(f"[WARN] ê³ ìœ„í—˜ ë¬¸ì¥ ë°œê²¬ (ì ìˆ˜: {risk_score:.3f}): {sentence[:100]}...", flush=True)
        
        print(f"[INFO] ë¶„ì„ ì™„ë£Œ. ì´ {len(scored_sentences)}ê°œ ë¬¸ì¥ ì¤‘ {len(high_risk_candidates)}ê°œê°€ ê³ ìœ„í—˜ìœ¼ë¡œ ë¶„ë¥˜ë¨", flush=True)
        
        # ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œìš© ì €ì¥ì†Œì— ì €ì¥
        if high_risk_candidates:
            self._save_to_high_risk_store(high_risk_candidates)
        
        # ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ë²¡í„° DBì— ì €ì¥ (ì˜µì…˜)
        if store_high_risk and high_risk_candidates:
            self._store_high_risk_sentences(high_risk_candidates)
            
        # ìƒˆë¡œìš´ ë¦¬í„´ í˜•íƒœ
        return {
            "all_scored": scored_sentences,
            "high_risk_candidates": high_risk_candidates
        }
    
    def score_sentence(self, sentence: str, prev_sentence: str = "", next_sentence: str = "", title: str = "") -> Dict[str, Any]:
        """
        ë‹¨ì¼ ë¬¸ì¥ì˜ ìœ„í—˜ ì ìˆ˜ì™€ ê·¼ê±°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            sentence (str): ë¶„ì„í•  ë¬¸ì¥
            prev_sentence (str, optional): ì´ì „ ë¬¸ì¥ (ë¬¸ë§¥ ì •ë³´)
            next_sentence (str, optional): ë‹¤ìŒ ë¬¸ì¥ (ë¬¸ë§¥ ì •ë³´)
            title (str, optional): ê¸€ ì œëª© (ì œëª©-ë³¸ë¬¸ ì¶©ëŒ ì²´í¬ìš©)
        """
        keyword_score, keyword_level, keyword_reasons = self._calculate_risk_score(sentence)
        llm_score = self._call_llm_for_risk_analysis(sentence, prev_sentence, next_sentence)
        
        # â­ RAG: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        similar_cases = self._search_similar_confirmed_cases(sentence)

        print(f"[DEBUG] score_sentence - keyword_score: {keyword_score:.3f}, llm_score: {llm_score:.3f}")

        # LLM ì ìˆ˜ê°€ ìˆìœ¼ë©´ LLMë§Œ ì‚¬ìš© (ë” ì •í™•í•¨)
        # LLMì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ì ìˆ˜ ì‚¬ìš©
        if llm_score > 0:
            final_score = llm_score
        else:
            final_score = keyword_score  # 0 ë˜ëŠ” ìŒìˆ˜ í¬í•¨

        final_score = max(0.0, min(1.0, final_score))
        final_level = self._score_to_level(final_score)
        
        # â­ ì‹ ë¢°ë„ ê³„ì‚°
        confidence, confidence_level = self._calculate_confidence(
            llm_score, similar_cases, keyword_score, final_score
        )
        
        # â­ ì œëª©-ë³¸ë¬¸ ì¶©ëŒ ì²´í¬
        title_conflict = False
        title_conflict_reason = ""
        if title and final_score >= THRESHOLD:  # ê³ ìœ„í—˜ì¼ ë•Œë§Œ ì²´í¬
            title_conflict, title_conflict_reason = self._check_title_content_conflict(
                title, sentence, final_score
            )
        
        print(f"[DEBUG] score_sentence - final_score: {final_score:.3f}, confidence: {confidence_level}, is_high_risk: {final_score >= THRESHOLD}")

        reasons = list(dict.fromkeys(keyword_reasons))  # ì¤‘ë³µ ì œê±° ìœ ì§€ ìˆœì„œ
        if llm_score > 0:
            reasons.append(f"LLM_í‰ê°€:{llm_score:.2f}")

        if not reasons:
            reasons.append("ëª…í™•í•œ ìœ„í—˜ ì‹ í˜¸ ì—†ìŒ")

        return {
            "risk_score": final_score,
            "risk_level": final_level,
            "reasons": reasons,
            "keyword_score": keyword_score,
            "llm_score": llm_score,
            "confidence": confidence,
            "confidence_level": confidence_level,
            "title_conflict": title_conflict,
            "title_conflict_reason": title_conflict_reason,
        }

    def _calculate_risk_score(self, sentence: str) -> tuple[float, str, List[str]]:
        """
        ë‹¨ì¼ ë¬¸ì¥ì— ëŒ€í•œ ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
        
        Args:
            sentence (str): ë¶„ì„í•  ë¬¸ì¥
            
        Returns:
            tuple: (ìœ„í—˜ì ìˆ˜, ìœ„í—˜ë ˆë²¨, ìœ„í—˜ìš”ì†Œë¦¬ìŠ¤íŠ¸)
        """
        if not sentence or not sentence.strip():
            return 0.0, 'low', []
            
        sentence_lower = sentence.lower()
        risk_factors = []
        base_score = 0.0

        for profile in self.keyword_profiles:
            matches = [kw for kw in profile["keywords"] if kw in sentence_lower]
            if not matches:
                continue

            delta = profile["weight"] * len(matches)
            base_score += delta

            label = profile["level"]
            if profile["weight"] > 0:
                risk_factors.extend([f"{label}_í‚¤ì›Œë“œ:{kw}" for kw in matches])
            else:
                risk_factors.extend([f"ì™„ì¶©_í‚¤ì›Œë“œ:{kw}" for kw in matches])
        
        # ë¬¸ì¥ ê¸¸ì´ ê³ ë ¤ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë¬¸ì¥ì€ ì ìˆ˜ ì¡°ì •)
        sentence_length = len(sentence.strip())
        if sentence_length < 10:
            base_score *= 0.5  # ì§§ì€ ë¬¸ì¥ì€ ì ìˆ˜ ê°ì†Œ
        elif sentence_length > 120:
            base_score *= 1.1  # ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ì•½ê°„ ì¦ê°€
            
        final_score = max(0.0, min(1.0, base_score))
        
        # ìœ„í—˜ ë ˆë²¨ ê²°ì • (THRESHOLD ê¸°ì¤€ ì ìš©)
        if final_score >= THRESHOLD:
            risk_level = 'high'
        elif final_score >= 0.4:
            risk_level = 'medium'
        else:
            risk_level = 'low'
            
        return final_score, risk_level, risk_factors

    @staticmethod
    def _score_to_level(score: float) -> str:
        if score >= THRESHOLD:
            return 'high'
        if score >= 0.4:
            return 'medium'
        return 'low'
    
    def _store_high_risk_sentences(self, high_risk_sentences: List[Dict[str, Any]]) -> None:
        """
        ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ë²¡í„° DBì— ì €ì¥
        
        Args:
            high_risk_sentences (List[Dict]): ê³ ìœ„í—˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë²¡í„° ìŠ¤í† ì–´ import (ì§€ì—° importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
            from .vector_store import get_vector_store
            
            vector_store = get_vector_store()
            
            for sentence_data in high_risk_sentences:
                sentence = sentence_data.get('sentence', '')
                
                # ì„ë² ë”© ìƒì„±
                embedding = self._get_embedding(sentence)
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata_dict = {
                    "user_id": sentence_data.get('user_id', 'unknown'),
                    "post_id": sentence_data.get('post_id', 'unknown'),
                    "sentence": sentence,
                    "risk_score": sentence_data.get('risk_score', 0.0),
                    "created_at": sentence_data.get('created_at', datetime.now().isoformat()),
                    "sentence_index": sentence_data.get('sentence_index', 0),
                    "risk_level": sentence_data.get('risk_level', 'unknown'),
                    "risk_factors": sentence_data.get('risk_factors', []),
                    "analyzed_at": sentence_data.get('analyzed_at', datetime.now().isoformat()),
                    "confirmed": False  # ìë™ ì €ì¥ëœ ë¬¸ì¥ì€ ë¯¸í™•ì¸ ìƒíƒœ
                }
                
                # ë²¡í„° DBì— ì €ì¥
                vector_store.upsert_high_risk_chunk(embedding, metadata_dict)
                
            print(f"[INFO] {len(high_risk_sentences)}ê°œì˜ ê³ ìœ„í—˜ ë¬¸ì¥ì„ ChromaDBì— ì €ì¥ ì™„ë£Œ")
                
        except ImportError as e:
            print(f"[WARN] ë²¡í„° ìŠ¤í† ì–´ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            print(f"[ERROR] ê³ ìœ„í—˜ ë¬¸ì¥ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _get_embedding(self, sentence: str) -> List[float]:
        """
        ë¬¸ì¥ì˜ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±
        
        Args:
            sentence (str): ì„ë² ë”©ì„ ìƒì„±í•  ë¬¸ì¥
            
        Returns:
            List[float]: ë²¡í„° ì„ë² ë”© (1536ì°¨ì›)
            
        Note:
            ì‹¤ì œ OpenAI ì„ë² ë”© ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        try:
            # embedding_serviceì—ì„œ ì‹¤ì œ ì„ë² ë”© ìƒì„±
            from .embedding_service import get_embedding
            embedding = get_embedding(sentence)
            
            print(f"[DEBUG] ì„ë² ë”© ìƒì„± ì™„ë£Œ: {sentence[:30]}... -> {len(embedding)}ì°¨ì›")
            return embedding
            
        except ImportError as e:
            print(f"[WARN] embedding_serviceë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            # fallback: ì„ì‹œ êµ¬í˜„ - 1536ì°¨ì› ë”ë¯¸ ë²¡í„° ìƒì„±
            embedding_dim = 1536
            embedding = [0.0] * embedding_dim
            print(f"[DEBUG] ë”ë¯¸ ì„ë² ë”© ìƒì„±: {sentence[:30]}... -> {embedding_dim}ì°¨ì›")
            return embedding
            
        except Exception as e:
            print(f"[ERROR] ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # fallback: ë”ë¯¸ ë²¡í„° ë°˜í™˜
            embedding_dim = 1536
            embedding = [0.0] * embedding_dim
            return embedding
    
    def _save_to_high_risk_store(self, high_risk_candidates: List[Dict[str, Any]]) -> None:
        """
        ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ì„ ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œìš© ì €ì¥ì†Œì— ì €ì¥
        
        Args:
            high_risk_candidates (List[Dict]): ê³ ìœ„í—˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # high_risk_store import (ì§€ì—° importë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
            from .high_risk_store import save_high_risk_chunk
            
            for sentence_data in high_risk_candidates:
                # ì €ì¥ì†Œìš© ë°ì´í„° ì¤€ë¹„
                chunk_dict = {
                    "user_id": sentence_data.get('user_id'),
                    "post_id": sentence_data.get('post_id'),
                    "sentence": sentence_data.get('sentence', ''),
                    "risk_score": sentence_data.get('risk_score', 0.0),
                    "created_at": sentence_data.get('created_at'),
                    "sentence_index": sentence_data.get('sentence_index'),
                    "risk_level": sentence_data.get('risk_level'),
                    "analyzed_at": sentence_data.get('analyzed_at')
                }
                
                # ê³ ìœ„í—˜ ì €ì¥ì†Œì— ì €ì¥
                chunk_id = save_high_risk_chunk(chunk_dict)
                print(f"[INFO] ê´€ë¦¬ì ëŒ€ì‹œë³´ë“œìš© ì €ì¥ ì™„ë£Œ: {chunk_id}")
                
        except ImportError as e:
            print(f"[WARN] high_risk_store ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            print(f"[ERROR] ê³ ìœ„í—˜ ë¬¸ì¥ ì €ì¥ì†Œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def _search_similar_confirmed_cases(self, sentence: str, top_k: int = 3, min_score: float = 0.7) -> List[Dict[str, Any]]:
        """
        ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ í™•ì •ëœ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰ (RAG)
        
        Args:
            sentence: ê²€ìƒ‰í•  ë¬¸ì¥
            top_k: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            min_score: ìµœì†Œ ìœ ì‚¬ë„ (0.0~1.0)
            
        Returns:
            ìœ ì‚¬ ì‚¬ë¡€ ë¦¬ìŠ¤íŠ¸ [{sentence, confirmed, similarity, risk_score}, ...]
        """
        try:
            from .embedding_service import get_embedding
            from .vector_db import get_client, search_similar
            
            # 1. ì„ë² ë”© ìƒì„±
            embedding = get_embedding(sentence)
            
            # 2. ë²¡í„°DBì—ì„œ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰
            client = get_client()
            if not client:
                print("[WARN] ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. RAG ê±´ë„ˆëœ€.")
                return []
            
            results = search_similar(
                client=client,
                embedding=embedding,
                top_k=top_k,
                min_score=min_score,
                collection_name="confirmed_risk"
            )
            
            # 3. ê²°ê³¼ í¬ë§·íŒ…
            similar_cases = []
            for result in results:
                metadata = result.get('metadata', {})
                similar_cases.append({
                    'sentence': result.get('document', ''),
                    'confirmed': metadata.get('confirmed', False),
                    'similarity': result.get('score', 0.0),
                    'risk_score': metadata.get('risk_score', 0.0),
                    'user_id': metadata.get('user_id', ''),
                    'created_at': metadata.get('created_at', '')
                })
            
            if similar_cases:
                print(f"[DEBUG] RAG: '{sentence[:30]}...'ì™€ ìœ ì‚¬í•œ ì‚¬ë¡€ {len(similar_cases)}ê±´ ë°œê²¬", flush=True)
            
            return similar_cases
            
        except Exception as e:
            print(f"[ERROR] ë²¡í„°DB ê²€ìƒ‰ ì‹¤íŒ¨: {e}", flush=True)
            return []
    
    def _calculate_confidence(
        self, 
        llm_score: float, 
        similar_cases: List[Dict[str, Any]], 
        keyword_score: float,
        final_score: float
    ) -> tuple[float, str]:
        """
        íŒë‹¨ ì‹ ë¢°ë„ ê³„ì‚°
        
        Args:
            llm_score: LLM ì ìˆ˜
            similar_cases: ìœ ì‚¬ ì‚¬ë¡€ ë¦¬ìŠ¤íŠ¸
            keyword_score: í‚¤ì›Œë“œ ì ìˆ˜
            final_score: ìµœì¢… ìœ„í—˜ ì ìˆ˜
            
        Returns:
            tuple: (ì‹ ë¢°ë„ ì ìˆ˜ 0.0~1.0, ì‹ ë¢°ë„ ë ˆë²¨ 'high'/'medium'/'low')
        """
        confidence = 0.0
        
        # 1. LLM ì ìˆ˜ ì¡´ì¬ ì—¬ë¶€ (+0.3)
        if llm_score > 0:
            confidence += 0.3
        
        # 2. ìœ ì‚¬ ì‚¬ë¡€ ê°œìˆ˜ ë° ìœ ì‚¬ë„ (ìµœëŒ€ +0.4)
        if similar_cases:
            num_cases = len(similar_cases)
            avg_similarity = sum(case.get('similarity', 0) for case in similar_cases) / num_cases
            
            # ìœ ì‚¬ ì‚¬ë¡€ ê°œìˆ˜ ê¸°ì—¬ë„
            if num_cases >= 3:
                confidence += 0.2
            elif num_cases >= 1:
                confidence += 0.1
            
            # í‰ê·  ìœ ì‚¬ë„ ê¸°ì—¬ë„
            if avg_similarity >= 0.8:
                confidence += 0.2
            elif avg_similarity >= 0.7:
                confidence += 0.1
        
        # 3. í‚¤ì›Œë“œ-LLM ì ìˆ˜ ì¼ì¹˜ë„ (+0.3)
        if llm_score > 0 and keyword_score > 0:
            score_diff = abs(llm_score - keyword_score)
            if score_diff < 0.1:
                confidence += 0.3  # ê±°ì˜ ì¼ì¹˜
            elif score_diff < 0.3:
                confidence += 0.2  # ì–´ëŠ ì •ë„ ì¼ì¹˜
            else:
                confidence += 0.1  # ë¶ˆì¼ì¹˜
        elif llm_score > 0:
            confidence += 0.15  # LLMë§Œ ìˆìŒ
        
        confidence = max(0.0, min(1.0, confidence))
        
        # ë ˆë²¨ ë¶„ë¥˜
        if confidence >= 0.75:
            level = "high"
        elif confidence >= 0.5:
            level = "medium"
        else:
            level = "low"
        
        return confidence, level
    
    def _check_title_content_conflict(
        self, 
        title: str, 
        content: str, 
        risk_score: float
    ) -> tuple[bool, str]:
        """
        ì œëª©ê³¼ ë³¸ë¬¸ì˜ ëŒ€ìƒì´ ë‹¤ë¥¸ì§€ ì²´í¬ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        
        Args:
            title: ê¸€ ì œëª©
            content: ë¶„ì„ ëŒ€ìƒ ë¬¸ì¥
            risk_score: ìœ„í—˜ ì ìˆ˜
            
        Returns:
            tuple: (ì¶©ëŒ ì—¬ë¶€, ì¶©ëŒ ì´ìœ )
        """
        if not title or not content:
            return False, ""
        
        title_lower = title.lower()
        content_lower = content.lower()
        
        # ì™¸ë¶€ ëŒ€ìƒì„ ì–¸ê¸‰í•˜ëŠ” í‚¤ì›Œë“œ (ë‹¤ë¥¸ ê³³/ì‚¬ì´íŠ¸/ì•±/ì„œë¹„ìŠ¤ ë“±)
        external_keywords = [
            "ë‹¤ë¥¸", "ë‹¤ì€", "ë”´", "íƒ€", "ì™¸ë¶€", "ë‹¤ë¥¸ê³³", "ë‹¤ë¥¸ì‚¬ì´íŠ¸", "ë‹¤ë¥¸ì•±", 
            "ë‹¤ë¥¸ì„œë¹„ìŠ¤", "ë‹¤ë¥¸í”Œë«í¼", "ê²½ìŸ", "ë¼ì´ë²Œ"
        ]
        
        # ì œëª©ì— ì™¸ë¶€ ëŒ€ìƒ ì–¸ê¸‰ì´ ìˆëŠ”ì§€
        title_has_external = any(kw in title_lower for kw in external_keywords)
        
        # ë³¸ë¬¸ì— ì´íƒˆ ì˜ë„ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€
        churn_keywords = [
            "íƒˆí‡´", "ê·¸ë§Œ", "ë– ë‚˜", "ì´íƒˆ", "ë‚˜ê°€", "ë– ë‚ ", "ì•ˆì“¸", "ì•ˆ ì“¸",
            "ê´€ë‘˜", "ê·¸ë§Œë‘˜", "ì•ˆí• ", "ì•ˆ í• ", "ê·¸ë§Œí• ", "í¬ê¸°"
        ]
        content_has_churn = any(kw in content_lower for kw in churn_keywords)
        
        # ì¶©ëŒ íŒì •: ì œëª©ì€ ì™¸ë¶€ ëŒ€ìƒ ë¹„íŒ, ë³¸ë¬¸ì€ ì´íƒˆ ì˜ë„
        if title_has_external and content_has_churn:
            return True, "ì œëª©ì€ ì™¸ë¶€ ëŒ€ìƒ ì–¸ê¸‰, ë³¸ë¬¸ì€ ì„œë¹„ìŠ¤ ì´íƒˆ ì˜ë„ í‘œí˜„ (ê²€í†  ê¶Œì¥)"
        
        return False, ""
    
    def _call_llm_for_risk_analysis(self, sentence: str, prev_sentence: str = "", next_sentence: str = "") -> float:
        """
        LLMì„ í˜¸ì¶œí•˜ì—¬ ë¬¸ì¥ì˜ ì´íƒˆ ìœ„í—˜ë„ë¥¼ ë¶„ì„ (ìºì‹± ë° ë¬¸ë§¥ ì •ë³´ ì§€ì›)
        
        ì´ ë¶€ë¶„ì€ ì‹¤ì œ LLM í˜¸ì¶œì´ë©°, ìš´ì˜ ì‹œ ë¹„ìš©ì´ ë“ ë‹¤.
        OpenAI GPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì˜ ì´íƒˆ ìœ„í—˜ë„ë¥¼ 0.0~1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ ê³„ì‚°í•œë‹¤.
        
        Args:
            sentence (str): ë¶„ì„í•  ë¬¸ì¥
            prev_sentence (str, optional): ì´ì „ ë¬¸ì¥ (ë¬¸ë§¥ ì •ë³´)
            next_sentence (str, optional): ë‹¤ìŒ ë¬¸ì¥ (ë¬¸ë§¥ ì •ë³´)
            
        Returns:
            float: 0.0~1.0 ì‚¬ì´ì˜ ìœ„í—˜ ì ìˆ˜ (ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ 0.0)
        """
        if not sentence or not sentence.strip():
            return 0.0
        
        # â­ ë²¡í„°DBì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (RAG)
        similar_cases = self._search_similar_confirmed_cases(sentence)
        
        # â­ ìºì‹œ í‚¤ ìƒì„± (ë¬¸ì¥ + ë¬¸ë§¥ + ìœ ì‚¬ ì‚¬ë¡€ ìˆ˜ë¡œ ê³ ìœ í•œ í‚¤ ìƒì„±)
        cache_key = f"{sentence}|{prev_sentence}|{next_sentence}|{len(similar_cases)}"
        
        # â­ ìºì‹œ í™•ì¸ (ê°™ì€ ë¬¸ì¥+ë¬¸ë§¥ì€ ê°™ì€ ê²°ê³¼ ë°˜í™˜)
        if cache_key in self._analysis_cache:
            cached_score = self._analysis_cache[cache_key]
            print(f"[DEBUG] ìºì‹œëœ ê²°ê³¼ ì‚¬ìš© - '{sentence[:30]}...' -> {cached_score:.3f}", flush=True)
            return cached_score
            
        # OpenAI APIê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if not openai or not OPENAI_API_KEY:
            print("[WARN] OpenAI APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 0.0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return 0.0
            
        try:
            # â­ ë¬¸ë§¥ ì •ë³´ êµ¬ì„± (ê²Œì‹œê¸€ ì œëª© + ì´ì „/ë‹¤ìŒ ë¬¸ì¥)
            context_info = ""
            has_context = prev_sentence or next_sentence
            
            if has_context:
                context_info = "\n\nğŸ“Œ ë¬¸ë§¥ ì •ë³´ (ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ê³ ë ¤):\n"
                if prev_sentence:
                    context_info += f"  â€¢ ì´ì „ ë¬¸ì¥: \"{prev_sentence}\"\n"
                context_info += f"  â€¢ í˜„ì¬ ë¬¸ì¥: \"{sentence}\" â† ì´ ë¬¸ì¥ì„ í‰ê°€í•˜ì„¸ìš”\n"
                if next_sentence:
                    context_info += f"  â€¢ ë‹¤ìŒ ë¬¸ì¥: \"{next_sentence}\"\n"
                context_info += "\nâš ï¸ í˜„ì¬ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•´ ë³´ì´ë©´ ë¬¸ë§¥ì„ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”.\n"
            
            # â­ RAG: ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€ (ë²¡í„°DB ê²€ìƒ‰ ê²°ê³¼)
            if similar_cases:
                context_info += "\n\nğŸ” ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ (ê´€ë¦¬ìê°€ í™•ì •í•œ íŒì •):\n"
                for i, case in enumerate(similar_cases[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ
                    confirmed_label = "âœ… ìœ„í—˜ ë§ìŒ" if case.get('confirmed') else "âŒ ìœ„í—˜ ì•„ë‹˜"
                    similarity = case.get('similarity', 0) * 100
                    context_info += f"  {i}. \"{case['sentence'][:60]}...\"\n"
                    context_info += f"     â†’ ìµœì¢… íŒì •: {confirmed_label} (ìœ ì‚¬ë„: {similarity:.0f}%)\n"
                context_info += "\nâš ï¸ ìœ„ ì‚¬ë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ë˜ê²Œ íŒë‹¨í•˜ì„¸ìš”.\n"
                print(f"[DEBUG] RAG: {len(similar_cases)}ê°œ ìœ ì‚¬ ì‚¬ë¡€ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€", flush=True)
            
            # ê°œì„ ëœ ì´íƒˆ ìœ„í—˜ë„ ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ëŒ€ìƒ ëª…ì‹œ ê°•í™”)
            prompt = f"""ì´ ë¬¸ì¥ì˜ ì»¤ë®¤ë‹ˆí‹°/ì„œë¹„ìŠ¤ ì´íƒˆ ìœ„í—˜ë„ë¥¼ 0.00~1.00 ì‚¬ì´ì˜ ìˆ«ìë¡œ í‰ê°€í•˜ì„¸ìš”.
{context_info}
ë¬¸ì¥: "{sentence}"

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” í‰ê°€ ì „ í•„ìˆ˜ í™•ì¸ì‚¬í•­:

âš ï¸ 1ë‹¨ê³„: **ë¸Œëœë“œ/ì œí’ˆëª… í™•ì¸** (ìµœìš°ì„ !)
   
   âŒ ë‹¤ìŒ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ìŒì‹ì /ì œí’ˆ ë¦¬ë·°ì…ë‹ˆë‹¤ (ì ìˆ˜ 0.10~0.20):
      - ìŒì‹ì : "í”¼ì", "ì¹˜í‚¨", "ë²„ê±°", "ì¹´í˜", "ë ˆìŠ¤í† ë‘", "ìŒì‹ì "
      - ë¸Œëœë“œ: "ë„ë¯¸ë…¸", "ë§¥ë„ë‚ ë“œ", "ìŠ¤íƒ€ë²…ìŠ¤", "ì´ì¬ëª¨" ë“±
      - ì œí’ˆ: "ì•„ì´í°", "ê°¤ëŸ­ì‹œ", "ë§¥ë¶", "ì—ì–´íŒŸ" ë“±
      - ì¥ì†Œ: "ì˜í™”ê´€", "í—¬ìŠ¤ì¥", "PCë°©", "ë…¸ë˜ë°©" ë“±
      
   âœ… ë¸Œëœë“œëª… ì—†ì´ ì„œë¹„ìŠ¤ ëª…ì‹œ:
      - "ì—¬ê¸°", "ì´ ì„œë¹„ìŠ¤", "ì´ ì»¤ë®¤ë‹ˆí‹°", "ì´ ì‚¬ì´íŠ¸"
      - "íƒˆí‡´", "ê³„ì • ì‚­ì œ", "ì´ í”Œë«í¼"

âš ï¸ 2ë‹¨ê³„: **ì œëª© í™•ì¸**
   
   - ì œëª©ì— ìœ„ ë¸Œëœë“œ/ì œí’ˆëª…ì´ ìˆìœ¼ë©´ â†’ ë¦¬ë·° ê¸€ (ì ìˆ˜ 0.10~0.20)
   - ì œëª©ì— "íƒˆí‡´", "ê·¸ë§Œë‘˜ê¹Œ" ë“±ì´ ìˆìœ¼ë©´ â†’ ì´íƒˆ ê¸€

âš ï¸ 3ë‹¨ê³„: **ëŒ€ìƒ ëª…ì‹œ í™•ì¸**
   
   âŒ ëŒ€ìƒ ë¶ˆëª…í™•í•œ ê²½ìš° (ì ìˆ˜ 0.00~0.30):
      - "ë‹¤ë¥¸ í”Œë«í¼ ì•Œì•„ë³´ëŠ” ì¤‘" â†’ ê³µë¶€/í•™ìŠµ í”Œë«í¼ì¼ ìˆ˜ë„
      - "ê·¸ë§Œë‘˜ ë•Œê°€ ëë‹¤" â†’ ì§ì¥/í•™êµì¼ ìˆ˜ë„
      - "ë‚˜ì€ ê³³ì´ ë§ë”ë¼" â†’ ì¥ì†Œì¼ ìˆ˜ë„
      - ëŒ€ìƒì´ ì „í˜€ ëª…ì‹œë˜ì§€ ì•Šì€ ì¼ë°˜ì  ë¶ˆë§Œ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ í‰ê°€ ë‹¨ê³„ë³„ ê°€ì´ë“œ:

1ï¸âƒ£ ëŒ€ìƒ ëª…ì‹œ í™•ì¸ (ìµœìš°ì„ !)
   âœ… ëª…ì‹œë¨: ìœ„ì— ì—´ê±°í•œ í‚¤ì›Œë“œë“¤
   âŒ ë¶ˆëª…í™•: ëŒ€ìƒ ì—†ëŠ” ì¼ë°˜ì  ê°ì • í‘œí˜„ â†’ ìœ„í—˜ë„ ìµœëŒ€ 0.30

2ï¸âƒ£ ì´íƒˆ ë‹¨ê³„ íŒë‹¨:
   [1ë‹¨ê³„] í™œë°œ ì°¸ì—¬ (0.00-0.15): ê¸ì •, ë§Œì¡±, ì ê·¹ ì°¸ì—¬
   [2ë‹¨ê³„] ì†Œê·¹ ì°¸ì—¬ (0.15-0.35): ë¬´ê´€ì‹¬, ê°€ë” ë°©ë¬¸
   [3ë‹¨ê³„] ê´€ê³„ ë‹¨ì ˆ (0.35-0.60): ì†Œí†µ ì•ˆë¼, ì‚¬ëŒë“¤ ë³„ë¡œ, ì‹¤ë§
   [4ë‹¨ê³„] ëŒ€ì•ˆ íƒìƒ‰ (0.60-0.80): ë‹¤ë¥¸ ê³³ ì•Œì•„ë´„, ê°ˆì•„íƒˆê¹Œ ê³ ë¯¼
   [5ë‹¨ê³„] ì´íƒˆ ê²°ì • (0.80-1.00): íƒˆí‡´, ê·¸ë§Œë‘ , í¬ê¸°, ë– ë‚¨

3ï¸âƒ£ í•µì‹¬ í‚¤ì›Œë“œ:
   ğŸ”´ HIGH (0.75+): íƒˆí‡´, ë– ë‚¨, ê·¸ë§Œë‘ , í¬ê¸°, ì†Œìš©ì—†, ì˜ë¯¸ì—†, ê°ˆì•„íƒ€
   ğŸŸ  MEDIUM (0.50+): ë‹¤ë¥¸ ê³³, í˜ë“¤ì–´, ì§€ì³, ë‹µë‹µ, ë¶ˆë§Œ
   ğŸŸ¡ LOW (0.30+): ì•„ì‰¬ì›Œ, ë¶ˆí¸í•´, ê°œì„  í•„ìš”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ í‰ê°€ ì˜ˆì‹œ (ë¸Œëœë“œ/ì œí’ˆëª… í™•ì¸ ìµœìš°ì„ !):

âŒ LOW - ë¸Œëœë“œëª… ê°ì§€ (ìŒì‹ì  ë¦¬ë·°):
[ì œëª©: "í”¼ì ë§›ì§‘"] "ë„ë¯¸ë…¸ í”¼ìê°€ ë” ë‚˜ì€ë“¯??"
â†’ ë¸Œëœë“œëª…: "ë„ë¯¸ë…¸", "í”¼ì" ê°ì§€
â†’ ìŒì‹ì  ë¦¬ë·° íŒì • â†’ ì ìˆ˜: 0.15

âŒ LOW - ë¸Œëœë“œëª… ê°ì§€ (ìŒì‹ì  ë¦¬ë·°):
[ì œëª©: "í”¼ì ë§›ì§‘"] "ì´ì¬ëª¨ í”¼ì ë§›ì—†ìŒ ã…‹ã…‹"
â†’ ë¸Œëœë“œëª…: "ì´ì¬ëª¨", "í”¼ì" ê°ì§€
â†’ ìŒì‹ì  ë¦¬ë·° íŒì • â†’ ì ìˆ˜: 0.15

âŒ LOW - ë¸Œëœë“œëª… ê°ì§€ + ë¶ˆëª…í™•í•œ í‘œí˜„:
[ì œëª©: "í”¼ì ë§›ì§‘"] "ì´ì œ ë°”ì´ë°”ì´ì„"
â†’ ì œëª©ì— "í”¼ì" ê°ì§€
â†’ "ë°”ì´ë°”ì´"ë§Œìœ¼ë¡œëŠ” ì„œë¹„ìŠ¤ ì´íƒˆ ë¶ˆëª…í™•
â†’ ìŒì‹ì  ë§¥ë½ â†’ ì ìˆ˜: 0.20

âœ… HIGH - ëŒ€ìƒ ëª…í™• + ì´íƒˆ ì˜ë„ ëª…í™•:
[ì œëª©: "íƒˆí‡´ ê³ ë¯¼"] "ì—¬ê¸° ìˆì–´ë´ì ì†Œìš©ì—†ì„ë“¯ìš”"
â†’ ë¸Œëœë“œëª…: ì—†ìŒ
â†’ ëŒ€ìƒ: "ì—¬ê¸°" = í˜„ì¬ ì„œë¹„ìŠ¤
â†’ ì´íƒˆ ì˜ë„: ëª…í™• â†’ ì ìˆ˜: 0.85

âœ… HIGH - ì œëª©ìœ¼ë¡œ ëŒ€ìƒ í™•ì¸:
[ì œëª©: "ë” ì´ìƒì€ ëª»í•˜ê² ìŠµë‹ˆë‹¤"] "ì´ì œ ì •ë§ ê·¸ë§Œë‘˜ ë•Œê°€ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤"
â†’ ë¸Œëœë“œëª…: ì—†ìŒ
â†’ ì œëª©ì—ì„œ ì„œë¹„ìŠ¤ ì´íƒˆ ëª…ì‹œ
â†’ ì ìˆ˜: 0.85

âŒ LOW - ëŒ€ìƒ ë¶ˆëª…í™• (ì§ì¥/í•™êµì¼ ìˆ˜ë„):
"ì´ì œ ì •ë§ ê·¸ë§Œë‘˜ ë•Œê°€ ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤"
â†’ ë¸Œëœë“œëª…: ì—†ìŒ
â†’ ì œëª© ì—†ìŒ, ëŒ€ìƒ ë¶ˆëª…í™•
â†’ ì ìˆ˜: 0.30

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ì¤‘ìš” ì›ì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):

1ï¸âƒ£ **ê³¼ê±° ìœ ì‚¬ ì‚¬ë¡€ ìµœìš°ì„  ì°¸ê³ !** â­ ê°€ì¥ ì¤‘ìš”!
   - ìœ„ì— ê³¼ê±° ì‚¬ë¡€ê°€ ìˆìœ¼ë©´ ê·¸ íŒì •ì„ ë”°ë¥´ì„¸ìš”
   - ìœ ì‚¬ë„ 70% ì´ìƒì´ë©´ ê±°ì˜ ë™ì¼í•œ ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤
   - ì¼ê´€ì„± ìœ ì§€ê°€ í•µì‹¬ì…ë‹ˆë‹¤

2ï¸âƒ£ **ë¸Œëœë“œ/ì œí’ˆëª… í™•ì¸**
   - í”¼ì, ì¹˜í‚¨, ë²„ê±°, ì¹´í˜ ë“± â†’ ìŒì‹ì  ë¦¬ë·° (0.10~0.20)
   - ë„ë¯¸ë…¸, ë§¥ë„ë‚ ë“œ, ì´ì¬ëª¨ ë“± â†’ ë¸Œëœë“œ ë¦¬ë·° (0.10~0.20)
   - ì•„ì´í°, ê°¤ëŸ­ì‹œ, ë§¥ë¶ ë“± â†’ ì œí’ˆ ë¦¬ë·° (0.10~0.20)
   - ì˜í™”ê´€, í—¬ìŠ¤ì¥, PCë°© ë“± â†’ ì¥ì†Œ ë¦¬ë·° (0.10~0.20)

3ï¸âƒ£ **ì œëª© í™•ì¸**
   - ì œëª©ì— ë¸Œëœë“œ/ì œí’ˆëª… ìˆìœ¼ë©´ â†’ ë¦¬ë·° ê¸€ (0.10~0.20)
   - ì œëª©ì— "íƒˆí‡´", "ê·¸ë§Œë‘˜ê¹Œ", "ë” ì´ìƒ" ë“± â†’ ì´íƒˆ ê¸€

4ï¸âƒ£ **ëŒ€ìƒ ëª…ì‹œ í•„ìˆ˜**
   - ë¸Œëœë“œëª… ì—†ì–´ë„ ëŒ€ìƒ ë¶ˆëª…í™•í•˜ë©´ â†’ ìµœëŒ€ 0.30
   - "ì—¬ê¸°", "ì´ ì„œë¹„ìŠ¤", "ì´ ì»¤ë®¤ë‹ˆí‹°", "íƒˆí‡´" ë“± í•„ìˆ˜

5ï¸âƒ£ **ë³´ìˆ˜ì  í‰ê°€**
   - ë¶ˆëª…í™•í•˜ë©´ ë‚®ê²Œ ì ìˆ˜ ë¶€ì—¬
   - ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ 0.20 ì´í•˜ë¡œ

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš¨ ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
- ë°˜ë“œì‹œ 0.00~1.00 ì‚¬ì´ì˜ ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”
- ì„¤ëª…, ì´ìœ , í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- ì˜¬ë°”ë¥¸ ì˜ˆ: 0.75
- ì˜ëª»ëœ ì˜ˆ: "ì´ ë¬¸ì¥ì€ ì´íƒˆ ì˜ë„ê°€ ìˆìŠµë‹ˆë‹¤", "0.75ì ", "ì ìˆ˜: 0.75"

ìˆ«ìë§Œ ë‹µí•´ (ì˜ˆ: 0.75):"""

            # ê°œì„ ëœ System í”„ë¡¬í”„íŠ¸
            system_prompt = """ë‹¹ì‹ ì€ ì»¤ë®¤ë‹ˆí‹°/ì„œë¹„ìŠ¤ ì´íƒˆ ì§•í›„ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸš¨ ì¤‘ìš”: ë°˜ë“œì‹œ 0.00~1.00 ì‚¬ì´ì˜ ìˆ«ìë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”!

í•µì‹¬ ì›ì¹™:
1. ëŒ€ìƒ(ì„œë¹„ìŠ¤/ì»¤ë®¤ë‹ˆí‹°) ëª…ì‹œ ì—¬ë¶€ë¥¼ ìµœìš°ì„  í™•ì¸
2. ë‹¨ìˆœ ë¶ˆë§Œê³¼ ì´íƒˆ ì˜ë„ë¥¼ ëª…í™•íˆ êµ¬ë¶„
3. ë¬¸ë§¥ê³¼ ì–´ì¡°ë¥¼ ì¢…í•© ê³ ë ¤
4. 0.75 ì´ìƒì€ ëª…í™•í•œ ì´íƒˆ ì˜ë„ë§Œ ë¶€ì—¬
5. ë°˜ë“œì‹œ ìˆ«ìë§Œ ë‹µë³€ (ì˜ˆ: 0.75)

ê³¼ëŒ€í‰ê°€ ë°©ì§€:
- ëŒ€ìƒ ë¶ˆëª…í™• ì‹œ ë³´ìˆ˜ì  í‰ê°€
- ìš´ì˜ ë¶ˆë§Œ â‰  ì´íƒˆ ìœ„í—˜
- ì¼ë°˜ì  ê°ì • í‘œí˜„ì— ë‚®ì€ ì ìˆ˜ ë¶€ì—¬

ì¶œë ¥ ì˜ˆì‹œ: 0.75 (ìˆ«ìë§Œ!)"""

            # OpenAI API í˜¸ì¶œ
            client = openai.OpenAI(api_key=OPENAI_API_KEY)
            
            response = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=20,  # â­ 10â†’20ìœ¼ë¡œ ì¦ê°€ (ìˆ«ì ì‘ë‹µì— ì¶©ë¶„)
                temperature=0,  # â­ 0ìœ¼ë¡œ ì„¤ì • (ì¼ê´€ëœ ê²°ê³¼)
                seed=42  # â­ ê³ ì •ëœ seedë¡œ ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´
            )
            
            # ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ
            response_text = response.choices[0].message.content.strip()
            
            # ìˆ«ìë§Œ ì¶”ì¶œ (ì†Œìˆ˜ì  í¬í•¨)
            import re
            score_match = re.search(r'(\d+\.?\d*)', response_text)
            
            if score_match:
                score = float(score_match.group(1))
                # 0.0~1.0 ë²”ìœ„ë¡œ ì •ê·œí™”
                score = max(0.0, min(1.0, score))
                
                # â­ ìºì‹œì— ì €ì¥ (ë‹¤ìŒë²ˆì— ê°™ì€ ë¬¸ì¥ì€ API í˜¸ì¶œ ì—†ì´ ë°˜í™˜)
                self._analysis_cache[cache_key] = score
                
                print(f"[DEBUG] LLM ë¶„ì„ ê²°ê³¼ - ë¬¸ì¥: '{sentence[:30]}...' -> ì ìˆ˜: {score:.3f}", flush=True)
                return score
            else:
                # âš ï¸ LLM íŒŒì‹± ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©
                print(f"[WARN] LLM ì‘ë‹µì—ì„œ ì ìˆ˜ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {response_text[:100]}...", flush=True)
                keyword_score, _, keyword_reasons = self._calculate_risk_score(sentence)
                
                # í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ fallback ì ìˆ˜ ìƒì„±
                # í‚¤ì›Œë“œ ì ìˆ˜ê°€ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì¤‘ê°„ê°’(0.5) ì‚¬ìš©
                fallback_score = max(0.5, keyword_score) if keyword_score > 0 else 0.5
                fallback_score = max(0.0, min(1.0, fallback_score))
                
                print(f"[WARN] Fallback: í‚¤ì›Œë“œ ì ìˆ˜ {keyword_score:.3f} -> ì‚¬ìš© ì ìˆ˜ {fallback_score:.3f}", flush=True)
                print(f"[WARN] í‚¤ì›Œë“œ ìš”ì¸: {keyword_reasons[:3]}", flush=True)  # ìƒìœ„ 3ê°œë§Œ
                
                # ìºì‹œì— fallback ì ìˆ˜ ì €ì¥
                self._analysis_cache[cache_key] = fallback_score
                
                return fallback_score
                
        except openai.RateLimitError:
            print("[ERROR] OpenAI API ìš”ì²­ í•œë„ ì´ˆê³¼. ê¸°ë³¸ê°’ 0.0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return 0.0
        except openai.AuthenticationError:
            print("[ERROR] OpenAI API ì¸ì¦ ì‹¤íŒ¨. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return 0.0
        except Exception as e:
            print(f"[ERROR] LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ê°’ 0.0ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return 0.0
    
    def get_risk_summary(self, scored_sentences: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ë¶„ì„ëœ ë¬¸ì¥ë“¤ì˜ ìœ„í—˜ë„ ìš”ì•½ ì •ë³´ ìƒì„±
        
        Args:
            scored_sentences (List[Dict]): ìœ„í—˜ ì ìˆ˜ê°€ ê³„ì‚°ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ìš”ì•½ ì •ë³´
                - total_sentences: ì´ ë¬¸ì¥ ìˆ˜
                - average_risk_score: í‰ê·  ìœ„í—˜ ì ìˆ˜
                - high_risk_count: ê³ ìœ„í—˜ ë¬¸ì¥ ìˆ˜ (>= THRESHOLD)
                - medium_risk_count: ì¤‘ìœ„í—˜ ë¬¸ì¥ ìˆ˜
                - low_risk_count: ì €ìœ„í—˜ ë¬¸ì¥ ìˆ˜
                - high_risk_threshold: ê³ ìœ„í—˜ íŒë‹¨ ì„ê³„ê°’
                - top_risk_sentences: ê°€ì¥ ìœ„í—˜í•œ ë¬¸ì¥ë“¤ (ìƒìœ„ 3ê°œ)
        """
        if not scored_sentences:
            return {
                'total_sentences': 0,
                'average_risk_score': 0.0,
                'high_risk_count': 0,
                'medium_risk_count': 0,
                'low_risk_count': 0,
                'high_risk_threshold': THRESHOLD,
                'top_risk_sentences': []
            }
            
        total_sentences = len(scored_sentences)
        total_score = sum(s.get('risk_score', 0.0) for s in scored_sentences)
        average_score = total_score / total_sentences if total_sentences > 0 else 0.0
        
        # ìœ„í—˜ ë ˆë²¨ë³„ ì¹´ìš´íŠ¸ (THRESHOLD ê¸°ì¤€ ì ìš©)
        high_risk_count = sum(1 for s in scored_sentences if s.get('risk_score', 0.0) >= THRESHOLD)
        medium_risk_count = sum(1 for s in scored_sentences 
                               if 0.4 <= s.get('risk_score', 0.0) < THRESHOLD)
        low_risk_count = sum(1 for s in scored_sentences if s.get('risk_score', 0.0) < 0.4)
        
        # ê°€ì¥ ìœ„í—˜í•œ ë¬¸ì¥ë“¤ (ìƒìœ„ 3ê°œ)
        sorted_sentences = sorted(
            scored_sentences, 
            key=lambda x: x.get('risk_score', 0.0), 
            reverse=True
        )
        top_risk_sentences = sorted_sentences[:3]
        
        return {
            'total_sentences': total_sentences,
            'average_risk_score': round(average_score, 3),
            'high_risk_count': high_risk_count,
            'medium_risk_count': medium_risk_count,
            'low_risk_count': low_risk_count,
            'high_risk_threshold': THRESHOLD,
            'top_risk_sentences': [
                {
                    'sentence': s.get('sentence', ''),
                    'risk_score': s.get('risk_score', 0.0),
                    'risk_factors': s.get('risk_factors', []),
                    'is_high_risk': s.get('is_high_risk', False)
                }
                for s in top_risk_sentences
            ]
        }
    
    def get_high_risk_sentences(self, scored_sentences: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜
        
        Args:
            scored_sentences (List[Dict]): ì ìˆ˜ê°€ ê³„ì‚°ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict[str, Any]]: ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ (risk_score >= THRESHOLD)
        """
        return [
            sentence for sentence in scored_sentences 
            if sentence.get('risk_score', 0.0) >= THRESHOLD
        ]


# í¸ì˜ë¥¼ ìœ„í•œ í•¨ìˆ˜í˜• ì¸í„°í˜ì´ìŠ¤
def score_sentences(
    sentences: List[Dict[str, Any]], 
    store_high_risk: bool = False
) -> Dict[str, Any]:
    """
    ë¬¸ì¥ë“¤ì— ìœ„í—˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í¸ì˜ í•¨ìˆ˜
    
    Args:
        sentences (List[Dict]): ë¬¸ì¥ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        store_high_risk (bool): ê³ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„° DBì— ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
    Returns:
        Dict[str, Any]: ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            - all_scored: ìœ„í—˜ ì ìˆ˜ê°€ ì¶”ê°€ëœ ëª¨ë“  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            - high_risk_candidates: ì„ê³„ê°’ì„ ë„˜ì€ ê³ ìœ„í—˜ ë¬¸ì¥ë“¤ ë¦¬ìŠ¤íŠ¸
    """
    scorer = RiskScorer()
    return scorer.score_sentences(sentences, store_high_risk)


def get_high_risk_threshold() -> float:
    """
    ê³ ìœ„í—˜ íŒë‹¨ ì„ê³„ê°’ ë°˜í™˜
    
    Returns:
        float: ê³ ìœ„í—˜ ì„ê³„ê°’
    """
    return THRESHOLD