"""
í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¤ë¦¬ íŒë‹¨ ì‹œìŠ¤í…œ
ê¸°ì¡´ BERT ëª¨ë¸ + OpenAI LLM ê²°í•©
"""
import os
import json
import re
import threading
from typing import Dict, List, Optional
from collections import Counter
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
from ethics.ethics_predict import EthicsPredictor
from ethics.ethics_vector_db import get_client, search_similar_cases, upsert_confirmed_case, build_chunk_id
from ethics.ethics_embedding import get_embedding, get_embeddings_batch
from ethics.ethics_text_splitter import split_to_sentences

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class HybridEthicsAnalyzer:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¹„ìœ¤ë¦¬ ë¶„ì„ê¸° (BERT ëª¨ë¸ + LLM + ê·œì¹™ ê¸°ë°˜)"""
    
    # ìš•ì„¤ í‚¤ì›Œë“œ ì •ì˜
    PROFANITY_KEYWORDS = {
        'severe': [  # ì‹¬í•œ ìš•ì„¤ (ê° +25ì )
            # ê¸°ë³¸ ìš•ì„¤
            'ì”¨ë°œ', 'ì‹œë°œ', 'ã……ã…‚', 'ã…†ã…‚', 'ë³‘ì‹ ', 'ã…‚ã……', 'ê°œìƒˆë¼', 'ê°œì‰', 'ê°œìƒ‰',
            'ì¢†', 'ì¢ƒ', 'ã…ˆê°™', 'ì§€ë„', 'ã…ˆã„¹', 'ì—¿ë¨¹', 'êº¼ì ¸', 'ì£½ì–´', 'ì£½ì„ë˜',
            'ë¯¸ì¹œë†ˆ', 'ë¯¸ì¹œë…„', 'ë˜ë¼ì´', 'ì‹¸ê°€ì§€', 'ì“°ë ˆê¸°ê°™ì€', 'ì°Œì§ˆ', 'ê°œë¼ì§€',
            'ë¸…ì‹ ', 'ë³‘ì‰°', 'ì‹œë°”', 'ì”¹', 'ê°œê°™ì€', 'ê°œì†Œë¦¬', 'ìƒˆë¼',
            # ì¶”ê°€ ìš•ì„¤
            'ì”¹ìƒˆë¼', 'ì”¹ë…„', 'ì”¹ë†ˆ', 'ê°œë…„', 'ê°œë†ˆ', 'ê°œìì‹', 'ê°œìƒˆ', 'ê°œì“°ë ˆê¸°',
            'ë¯¸ì¹œìƒˆë¼', 'ë¯¸ì¹œìì‹', 'ë¯¸ì¹œê²ƒ', 'ë¯¸ì¹œX', 'ëŒì•˜', 'ëŒì•„ë²„',
            'ì¢†ê¹Œ', 'ì¢ƒê¹Œ', 'ë‹¥ì³', 'ë‹¥ì¹˜ì„¸ìš”', 'êº¼ì§€ì„¸ìš”', 'ì£½ì–´ë²„ë ¤', 'ë’¤ì ¸', 'ë’¤ì§ˆ',
            'ì—¿ì´ë‚˜', 'ì—¿ë“œì…”', 'ê°œë¹¡', 'ë¹¡ì¹œ', 'ë¹¡ì³', 'ì¢†ë°¥', 'ì¡ë†ˆ', 'ì¡ë…„',
            'ë§í• ', 'ë§í• ë†ˆ', 'ê°œë§', 'ì§€ë„í•˜ë„¤', 'ì§€ë„ë§', 'ì§œì ¸', 'ì§œì¦ë‚¨',
            'ì”¨íŒ”', 'sibal', 'sival', 'fuck', 'shit', 'bitch', 'asshole',
            'ì• ë¯¸', 'ì• ë¹„', 'ëŠê¸ˆ', 'ëŠê°œë¹„', 'ê°œë“œë¦½', 'ê°œì›ƒ', 'ê²Œìƒˆ',
            'í˜¸ë¡œ', 'í˜¸ë¡œìì‹', 'í˜¸ë¡œìƒˆë¼', 'ì°½ë†ˆ', 'ì°½ë…€', 'ì…', 'ì…ë…„',
            'ë³‘ë§›', 'ë³‘í¬', 'ê¼´ê°’', 'ê¼´ì¢‹', 'ê°œë…', 'ê¸‰ì‹ì¶©', 'í‹€ë”±', 'í•œë‚¨ì¶©',
            'ê¹€ì¹˜ë…€', 'ë§˜ì¶©', 'í‹€ë‹ˆë”±ë”±', 'ê¸‰ì‹', 'ê¸‰ì‚½', 'ë“±ì‹ ', 'ë©í……êµ¬ë¦¬',
            'ëª…ì²­', 'ã…ã…Š', 'ê°œì°¨ë°˜', 'ê°œíŒ', 'ê°œì§€ë„', 'ì—¼ë³‘', 'ì”¨ë¶€ë„', 'ì”¨ë¶€ëŸ´',
            'ì¢†ê°™ë„¤', 'ì¢†ë°¥', 'ê°œìª½', 'ê°œì†Œë¦¬', 'ê°œë“œë¦½', 'ê°œì†Œ', 'ã„±ã……ã„²', 'ê°œë§‰ì¥', 
            'ì¢Œë¹¨'
        ],
        'moderate': [  # ì¤‘ê°„ ìˆ˜ìœ„ ìš•ì„¤/ë¹„ë°© (ê° +15ì )
            # ê¸°ë³¸ ë¹„ë°©
            'ë°”ë³´', 'ë©ì²­', 'ë©ì²­ì´', 'í•œì‹¬', 'í•œì‹¬í•œ', 'ëª»ë‚¬', 'ëª»ë‚œ',
            'ì§œì¦', 'ì§œì¦ë‚˜', 'ê¼´ë¶ˆê²¬', 'ê¼´ì‚¬ë‚©', 'ì§€ê²¨', 'ì§€ê¸‹ì§€ê¸‹',
            'ì—­ê²¹', 'ì—­ê²¨ìš´', 'ì§•ê·¸ëŸ½', 'ì¶”ì•…í•œ', 'ë”ëŸ½', 'í›„ì§„',
            'ì“°ë ˆê¸°', 'ìª½íŒ”', 'ìª½íŒ”ë ¤', 'ì°½í”¼', 'ë¶€ë„ëŸ½', 'ì² ë©´í”¼', 'ë»”ë»”',
            'ì–´ì´ì—†', 'í™©ë‹¹', 'ë§¥ë¹ ', 'í•œì‹¬í•˜ë‹¤', 'ì €ì§ˆ', 'ì €ê¸‰', 'ìˆ˜ì¤€ë‚®',
            'ë‹¥ì³', 'ì…ë‹¥', 'ì… ë‹¥ì³', 'ì¡°ìš©íˆ í•´',
            # ì¶”ê°€ ë¹„ë°©
            'ë¬´ì‹', 'ë¬´ì‹í•œ', 'ëª¨ìë¼', 'ëª¨ìë€', 'ë©ì²­í•œ', 'ë‹µì—†', 'ë‹µì´ì—†',
            'ê¼´ë³´ê¸°ì‹«', 'ë³´ê¸°ì‹«', 'ê±°ìŠ¬ë ¤', 'ê±°ìŠ¬ë¦¬', 'ë¯¸ê°œ', 'ë¯¸ê°œí•œ',
            'ìˆ˜ì¤€', 'ìˆ˜ì¤€ì´í•˜', 'ìˆ˜ì¤€ë¯¸ë‹¬', 'ìµœì•…', 'ìµœì•…ì˜', 'í˜•í¸ì—†',
            'í•œì‹¬ìŠ¤ëŸ½', 'ë¶€ì¡±', 'ë¶€ì¡±í•œ', 'ëª¨ìëŒ', 'ë¬¸ì œìˆ', 'ë¬¸ì œë§',
            'ì •ì‹ ì—†', 'ì •ì‹ ì°¨ë ¤', 'ìƒê°ì—†', 'ìƒê°ì´ì—†', 'ë‡Œì—†', 'ë‡Œê°€ì—†',
            'ë¬´ëŠ¥', 'ë¬´ëŠ¥í•œ', 'ë¬´ëŠ¥ë ¥', 'ì“¸ëª¨ì—†', 'ì“¸ë°ì—†', 'ê°€ì¹˜ì—†',
            'ìª½íŒ”ë¦°', 'ë§ì‹ ', 'ë§ì‹ ë‹¹', 'ì²´ë©´', 'ì—¼ì¹˜ì—†', 'ì—¼ì¹˜',
            'ë¹„ì—´', 'ë¹„ì—´í•œ', 'ì¹˜ì‚¬', 'ì¹˜ì‚¬í•œ', 'ì°Œì§ˆ', 'ì°Œì§ˆì´', 'ë£¨ì €',
            'íŒ¨ë°°ì', 'ë‚™ì˜¤ì', 'ì°ë”°', 'ì™•ë”°', 'ì•„ì‹¸', 'ì¸ì‹¸ëª»', 'í—ˆì ‘',
            'í—ˆì ‘í•œ', 'êµ¬ì œë¶ˆ', 'êµ¬ì œë¶ˆëŠ¥', 'í¬ë§ì—†', 'ê°€ë§ì—†', 'ì•ˆìŠµ',
            'ì•ˆíƒ€ê¹Œ', 'ë¶ˆìŒ', 'ì¸¡ì€', 'ê°€ì—¾', 'ë¶ˆí–‰', 'ë¹„ì°¸',
            'ìš°ìŠ¤ì›Œ', 'ìš°ìŠ¤ìš´', 'ì›ƒê¸°', 'ì›ƒê¸´', 'ì½”ë¯¸ë””', 'ê°œê·¸', 'ê°œê·¸ë§¨',
            'ì• ìƒˆë¼', 'ì• ì†¡ì´', 'ì• ê¸°', 'ê¼¬ë§ˆ', 'ì¤‘ë”©', 'ì´ˆë”©', 'ìœ ì¹˜',
            'ìœ ì¹˜í•œ', 'ìœ ì¹˜í•´', 'ì–´ë¦¬ì„', 'ì–´ë¦¬ì„ì€', 'ìš°ë§¤', 'ìš°ë§¤í•œ',
            'ì²œë°•', 'ì²œë°•í•œ', 'ì €ì†', 'ì €ì†í•œ', 'ì €ê¸‰ìŠ¤ëŸ½', 'ì¡°ì¡', 'ì¡°ì¡í•œ',
            'í˜•í¸ì—†ëŠ”', 'ë³¼í’ˆì—†', 'ì‹œì‹œí•œ', 'ë”°ë¶„í•œ', 'ì§€ë£¨í•œ', 'ì¬ë¯¸ì—†',
            'ë§¹í•˜', 'ë‘”í•˜', 'ë‘”ê°', 'ëŠë¦¬', 'êµ¼ëœ¨', 'êµ¼ëœ¬', 'ë‹µë‹µ',
            'ë¬´ì•ˆ', 'ë¬´ì•ˆí•œ', 'ë¬´ë¡€', 'ë¬´ë¡€í•œ', 'ë²„ë¦‡ì—†', 'ì‹¸ê°€ì§€ì—†', 'ì˜ˆì˜ì—†',
            'ë’¤ì§„ë‹¤', 'ë’¤ì§ˆë˜', "ë’¤ì§€ê³ "
        ],
        'patterns': [  # ìš•ì„¤ íŒ¨í„´
            r'[ã„±-ã…]+[ã……ã…†][ã…‚ã…ƒ][ã„±-ã…]*',
            r'[ã„±-ã…]*[ã…‚ã…ƒ][ã……ã…†][ã„±-ã…]*',
            r'[ã„±-ã…]+[ã…ˆã…‰][ã„¹ã„´][ã„±-ã…]*',
            r'[ì‹œì”¨][1l|!iI\*@#ë°œíŒ”ë¹¨]',
            r'ê°œ\s*[ìƒˆì‰ìƒ‰ì„¹]+',
            r'[ì¢†ì¢ƒ][ê°™ê°”]',
            r'[ëŠëŠ¬ë‹ˆ]ê¸ˆ\s*ë§ˆ',
            r'[ã…„]{2,}',
            r'[ã……ã…†]{2,}[ã…‚ã…ƒ]',
            r'[ë³‘ë¸…ë¹™][ì‹ ì‰°]',
            r'[ê°œ][\*\-_\s]*[ìƒˆì‰]',
            r'[ì”¨ì‹œ][8\*@#ë°œë¹¨íŒ”]',
            r'[ì£½ì¥­][ì–´ì–´]',
            r'[ì§€ã…ˆ][ë„ã„¹]',
            r'ë¯¸[ì¹œã…Š][ë†ˆë…„]',
            r'[ì—¿ì—‡][ë¨¹ë¨¹]',
            r'[êº¼êº¼][ì ¸ì§€]',
            r'[ë‹¥ë‹¥][ì³ì³]',
            r'[ê°œ][ê°™ê°‡]',
            r'ì”¹[\s]*[ìƒˆë…„ë†ˆ]',
        ]
    }
    
    # ìŠ¤íŒ¸ í‚¤ì›Œë“œ ì •ì˜
    SPAM_KEYWORDS = {
        'high': ['ëŒ€ì¶œ', 'ë‹¹ì²¨', 'ë¬´ë£Œ', 'ê³µì§œ', 'í˜„ê¸ˆ', 'ì ë¦½', 'í´ë¦­', 'ì ‘ì†', 
                 'ì„ ì°©ìˆœ', 'í•œì •', 'ì´ë²¤íŠ¸', 'íŠ¹ê°€', 'ì„¸ì¼', 'í• ì¸', 'ì¿ í°',
                 'ë¶€ì—…', 'ì¬íƒ', 'íˆ¬ì', 'ìˆ˜ìµ', 'ë„ë°•', 'ì¹´ì§€ë…¸', 'ì„±ì¸',
                 'í™˜ê¸‰', 'ì§€ê¸‰', 'ì¦‰ì‹œ', 'ê¸´ê¸‰', 'ë§ˆê°', 'ì¶•í•˜', 'ë‹¹ì²¨',
                 'ì²´í—˜', 'ë³´ì¡°ì œ', 'ë¹„ë²•', 'ìë™', 'ê²°ì œ', 'ì·¨ì†Œ', 'êµ­ì„¸ì²­',
                 'ì •ë¶€ì§€ì›', 'ì €ì‹ ìš©', 'ê³„ì¢Œ', 'ì…ë ¥', 'ë§í¬', 'í™•ì¸', 'ë°©ë¬¸'],
        'medium': ['ê´‘ê³ ', 'í™ë³´', 'íŒë§¤', 'êµ¬ë§¤', 'ê°€ì…', 'íšŒì›', 'ë“±ë¡',
                  'ì°¸ì—¬', 'ì‹ ì²­', 'ë¬¸ì˜', 'ì•ˆë‚´', 'ì œê³µ', 'ê³µê°œ', 'ê°•ì˜',
                  'íƒë°°', 'ë°°ì†¡', 'ì§€ì—°'],
        'patterns': [
            r'http[s]?://[^\s]+',
            r'bit\.ly/[^\s]+',
            r'\w+\.(kr|com|net|co\.kr|info)/\w+',
            r'\d{3}-\d{3,4}-\d{4}',
            r'\d{2,3}-\d{3,4}-\d{4}',
            r'080-\d{3,4}-\d{4}',
            r'ì¹´í†¡.*[Ii][Dd]',
            r'[A-Z]{3,}',
            r'\[ê´‘ê³ \]',
            r'\[Webë°œì‹ \]',
            r'â–¶|ğŸ‘‰|â©|â¡',
            r'â˜…|â˜†|ğŸ”¥|ğŸ’°|ğŸ‰|ğŸŠ',
            r'\d{1,3}%\s*í• ì¸',
            r'\d{1,3}ë§Œì›',
        ]
    }
    
    def __init__(self, 
                 model_path='ethics/models/binary_classifier.pth',
                 config_path='ethics/models/config.json',
                 api_key: Optional[str] = None,
                 model_name: Optional[str] = None):
        """
        Args:
            model_path: BERT ëª¨ë¸ ê²½ë¡œ
            config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
            api_key: OpenAI API í‚¤ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            model_name: OpenAI ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        # BERT ëª¨ë¸ ì´ˆê¸°í™”
        print("[INFO] BERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.bert_predictor = EthicsPredictor(model_path, config_path)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.model_name = model_name or 'gpt-4.1-nano'
        
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        self.client = OpenAI(api_key=self.api_key)
        print(f"[INFO] LLM ëª¨ë¸ ì—°ê²° ì™„ë£Œ: {self.model_name}")
        
        # RAG ê¸°ëŠ¥ ì´ˆê¸°í™” (ì„ íƒì , ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
        try:
            self.vector_client = get_client()
            print("[INFO] RAG ë²¡í„°DB ì—°ê²° ì™„ë£Œ")
            self.rag_enabled = True
        except Exception as e:
            print(f"[WARN] RAG ë²¡í„°DB ì—°ê²° ì‹¤íŒ¨: {e}. RAG ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            self.rag_enabled = False
            self.vector_client = None
    
    def _calculate_profanity_boost(self, text: str) -> Dict:
        """ìš•ì„¤ ê°ì§€ ë° ì ìˆ˜ ë¶€ìŠ¤íŠ¸ ê³„ì‚°"""
        boost_score = 0.0
        profanity_count = 0
        detected_profanities = []
        
        # 1. ì‹¬í•œ ìš•ì„¤ ì²´í¬ (ê° +25ì )
        for keyword in self.PROFANITY_KEYWORDS['severe']:
            if keyword in text:
                boost_score += 25
                profanity_count += 1
                detected_profanities.append(keyword)
        
        # 2. ì¤‘ê°„ ìˆ˜ìœ„ ìš•ì„¤/ë¹„ë°© ì²´í¬ (ê° +15ì )
        for keyword in self.PROFANITY_KEYWORDS['moderate']:
            if keyword in text:
                boost_score += 15
                profanity_count += 1
                detected_profanities.append(keyword)
        
        # 3. ìš•ì„¤ íŒ¨í„´ ë§¤ì¹­
        for pattern in self.PROFANITY_KEYWORDS['patterns']:
            matches = re.findall(pattern, text)
            if matches:
                pattern_count = min(len(matches), 3)
                boost_score += pattern_count * 20
                profanity_count += pattern_count
        
        # 4. ìš•ì„¤ ë°˜ë³µ ê°ì§€
        if profanity_count > 3:
            boost_score += 10
        
        # ìµœëŒ€ ë¶€ìŠ¤íŠ¸ëŠ” 50ì ìœ¼ë¡œ ì œí•œ
        boost_score = min(boost_score, 50.0)
        
        # ì‹¬ê°ë„ íŒë‹¨
        if boost_score >= 40:
            severity = 'severe'
        elif boost_score >= 20:
            severity = 'moderate'
        elif boost_score > 0:
            severity = 'mild'
        else:
            severity = 'none'
        
        return {
            'boost_score': boost_score,
            'profanity_detected': profanity_count > 0,
            'profanity_count': profanity_count,
            'severity': severity
        }
    
    def _calculate_rule_based_spam_score(self, text: str) -> float:
        """ê·œì¹™ ê¸°ë°˜ ìŠ¤íŒ¸ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        text_lower = text.lower()
        
        # 1. ê³ ìœ„í—˜ í‚¤ì›Œë“œ ì²´í¬ (ê° +20ì )
        for keyword in self.SPAM_KEYWORDS['high']:
            if keyword in text_lower:
                score += 20
        
        # 2. ì¤‘ìœ„í—˜ í‚¤ì›Œë“œ ì²´í¬ (ê° +5ì )
        for keyword in self.SPAM_KEYWORDS['medium']:
            if keyword in text_lower:
                score += 5
        
        # 3. íŒ¨í„´ ë§¤ì¹­ ì²´í¬
        pattern_match_count = 0
        for pattern in self.SPAM_KEYWORDS['patterns']:
            if re.search(pattern, text):
                pattern_match_count += 1
        
        if pattern_match_count >= 3:
            score += 40
        elif pattern_match_count >= 2:
            score += 30
        elif pattern_match_count >= 1:
            score += 20
        
        # 4. íŠ¹ìˆ˜ë¬¸ì/ì´ëª¨í‹°ì½˜ ë¹„ìœ¨ ì²´í¬
        special_chars = len(re.findall(r'[!@#$%^&*()_+=\[\]{}|\\:;"\'<>,.?/~`ğŸ‰ğŸŠğŸ”¥ğŸ’°ğŸ’¯]', text))
        if len(text) > 0:
            special_ratio = special_chars / len(text)
            if special_ratio > 0.15:
                score += 15
        
        # 5. ëŒ€ë¬¸ì ë¹„ìœ¨ ì²´í¬
        uppercase_count = sum(1 for c in text if c.isupper() and c.isalpha())
        alpha_count = sum(1 for c in text if c.isalpha())
        if alpha_count > 0:
            uppercase_ratio = uppercase_count / alpha_count
            if uppercase_ratio > 0.5:
                score += 10
        
        # 6. ë¬¸ì¥/êµ¬ë¬¸ ë°˜ë³µ ê°ì§€ (100ì ì´ìƒ)
        if len(text) >= 100:
            max_repeat = 0
            
            # ë°©ë²• 1: ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì²´í¬
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            if len(lines) >= 3:
                normalized_lines = [line.lower().replace(' ', '') for line in lines if len(line) > 5]
                if normalized_lines:
                    line_counts = Counter(normalized_lines)
                    max_repeat = max(line_counts.values())
            
            # ë°©ë²• 2: ë‹¨ì–´/êµ¬ë¬¸ ë‹¨ìœ„ ë°˜ë³µ ì²´í¬ (ê³µë°±ì´ë‚˜ êµ¬ë‘ì ìœ¼ë¡œ ë¶„í• )
            words = re.split(r'[\s,.!?;]+', text.lower())
            words = [w.strip() for w in words if len(w.strip()) > 3]
            if len(words) >= 5:
                word_counts = Counter(words)
                word_repeat = max(word_counts.values()) if word_counts else 0
                max_repeat = max(max_repeat, word_repeat)
            
            # ë°©ë²• 3: ì—°ì†ëœ ë™ì¼ íŒ¨í„´ ê°ì§€ (sliding window)
            # 5-15ì ê¸¸ì´ì˜ íŒ¨í„´ì„ ì°¾ì•„ì„œ ë°˜ë³µ ì²´í¬
            for pattern_len in [5, 10, 15]:
                if len(text) >= pattern_len * 3:
                    patterns = []
                    for i in range(0, len(text) - pattern_len + 1, pattern_len):
                        pattern = text[i:i+pattern_len].lower().replace(' ', '').strip()
                        if len(pattern) >= pattern_len * 0.8:  # ìµœì†Œ 80% ê¸¸ì´
                            patterns.append(pattern)
                    
                    if patterns:
                        pattern_counts = Counter(patterns)
                        pattern_repeat = max(pattern_counts.values())
                        max_repeat = max(max_repeat, pattern_repeat)
            
            # 5íšŒ ì´ìƒ ë°˜ë³µ ì²´í¬
            if max_repeat >= 5:
                # ê¸°ë³¸ +50ì  + ì¶”ê°€ ë°˜ë³µë§ˆë‹¤ +6ì 
                repeat_score = 50 + ((max_repeat - 5) * 6)
                
                # 15íšŒ ì´ìƒ ê·¹ì‹¬í•œ ë°˜ë³µ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
                if max_repeat >= 15:
                    repeat_score += 20
                
                score += min(repeat_score, 100)  # ìµœëŒ€ 100ì 
        
        # 7. ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ìŠ¤íŒ¸ ê°€ëŠ¥ì„± ë‚®ìŒ
        if len(text) < 20 and score < 20:
            score *= 0.5
        
        return min(score, 100.0)
        
    def _analyze_with_llm(self, text: str) -> Dict:
        """LLMì„ ì‚¬ìš©í•œ ë¹„ìœ¤ë¦¬ ë° ìŠ¤íŒ¸ ë¶„ì„"""
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¤ë¦¬ì„±ê³¼ ìŠ¤íŒ¸ ì—¬ë¶€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:
{{
    "immoral_score": 0-100 ì‚¬ì´ì˜ ìˆ«ì (0=ì™„ì „ ìœ¤ë¦¬ì , 100=ë§¤ìš° ë¹„ìœ¤ë¦¬ì ),
    "spam_score": 0-100 ì‚¬ì´ì˜ ìˆ«ì (ìŠ¤íŒ¸ í™•ì‹¤ì„±: 100=ëª…ë°±íˆ ìŠ¤íŒ¸, 50=ì• ë§¤í•¨, 0=ëª…ë°±íˆ ì •ìƒ),
    "confidence": 0-100 ì‚¬ì´ì˜ ìˆ«ì (íŒë‹¨ì˜ í™•ì‹ ë„),
    "types": ["ìœ í˜•1", "ìœ í˜•2", ...]
}}

ë¶„ì„ ìœ í˜• ëª©ë¡:
- "ìš•ì„¤ ë° ë¹„ë°©": ë¹„ì†ì–´, ìš•ì„¤, íƒ€ì¸ì„ ë¹„ë‚œí•˜ëŠ” í‘œí˜„
- "ë„ë°° ë° ê´‘ê³ ": ìƒì—…ì  ê´‘ê³ , ìŠ¤íŒ¸, ë„ë°°ì„± ë©”ì‹œì§€
- "ì—†ìŒ": í•´ë‹¹ ìœ í˜•ì´ ì—†ëŠ” ê²½ìš°

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¤ë¦¬ì„±ê³¼ ìŠ¤íŒ¸ ì—¬ë¶€ë¥¼ ì •í™•í•˜ê²Œ íŒë‹¨í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=500
            )
            
            content = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹±
            if content.startswith('```'):
                content = content.split('```')[1]
                if content.startswith('json'):
                    content = content[4:]
                content = content.strip()
            
            result = json.loads(content)
            
            # ê°’ ê²€ì¦ ë° ì •ê·œí™”
            result['immoral_score'] = max(0, min(100, float(result.get('immoral_score', 50))))
            result['spam_score'] = max(0, min(100, float(result.get('spam_score', 0))))
            result['confidence'] = max(0, min(100, float(result.get('confidence', 50))))
            result['types'] = result.get('types', ['ì—†ìŒ'])
            
            return result
            
        except Exception as e:
            print(f"[WARN] LLM ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {
                'immoral_score': 50.0,
                'spam_score': 0.0,
                'confidence': 30.0,
                'types': ['ë¶„ì„ ì‹¤íŒ¨']
            }
    
    def _search_similar_cases(self, text: str) -> List[Dict]:
        """
        ë²¡í„°DBì—ì„œ ìœ ì‚¬í•œ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ì¼€ì´ìŠ¤ ê²€ìƒ‰
        âš¡ ë°°ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë¬¸ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬ (ì†ë„ 4-6ë°° í–¥ìƒ)
        
        Args:
            text (str): ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: ìœ ì‚¬ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if not self.rag_enabled or not self.vector_client:
            return []
        
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹
            sentences = split_to_sentences(text, min_length=10)
            
            if not sentences:
                return []
            
            # âš¡ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ëª¨ë“  ë¬¸ì¥ ì²˜ë¦¬)
            embeddings = get_embeddings_batch(sentences)
            
            # ê° ì„ë² ë”©ë³„ë¡œ ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰
            all_similar_cases = []
            seen_ids = set()  # ì¤‘ë³µ ì œê±°ìš©
            
            for embedding in embeddings:
                # ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ (ì‹ ë¢°ë„ 80 ì´ìƒë§Œ)
                similar_cases = search_similar_cases(
                    client=self.vector_client,
                    embedding=embedding,
                    top_k=3,  # ë¬¸ì¥ë‹¹ ìµœëŒ€ 3ê°œ
                    min_score=0.5,
                    min_confidence=80.0,
                    prefer_confirmed=True
                )
                
                # ì¤‘ë³µ ì œê±° ë° ì¶”ê°€
                for case in similar_cases:
                    case_id = case.get('id')
                    if case_id and case_id not in seen_ids:
                        seen_ids.add(case_id)
                        all_similar_cases.append(case)
            
            # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            all_similar_cases.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
            return all_similar_cases[:5]
            
        except Exception as e:
            print(f"[WARN] ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _adjust_scores_with_similarity(
        self,
        base_immoral_score: float,
        base_spam_score: float,
        similar_cases: List[Dict]
    ) -> Dict[str, float]:
        """
        ìœ ì‚¬ ì¼€ì´ìŠ¤ë“¤ì˜ ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ë³´ì • ì ìˆ˜ ê³„ì‚°
        
        Args:
            base_immoral_score: ê¸°ì¡´ ë¹„ìœ¤ë¦¬ ì ìˆ˜
            base_spam_score: ê¸°ì¡´ ìŠ¤íŒ¸ ì ìˆ˜
            similar_cases: ìœ ì‚¬ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Dict: ë³´ì • ì ìˆ˜ ë° ë©”íƒ€ë°ì´í„°
        """
        if not similar_cases:
            return {
                'adjusted_immoral_score': base_immoral_score,
                'adjusted_spam_score': base_spam_score,
                'confidence_boost': 0.0,
                'similar_case_count': 0,
                'max_similarity': 0.0
            }
        
        # ê´€ë¦¬ì í™•ì¸ëœ ì¼€ì´ìŠ¤ ìš°ì„  ì‚¬ìš©
        confirmed_cases = [c for c in similar_cases if c.get('confirmed', False)]
        confirmed_count = len(confirmed_cases)  # ì‹¤ì œ í™•ì • ì¼€ì´ìŠ¤ ìˆ˜ ì €ì¥
        
        if not confirmed_cases:
            # í™•ì¸ëœ ì¼€ì´ìŠ¤ê°€ ì—†ìœ¼ë©´ í™•ì¸ë˜ì§€ ì•Šì€ ì¼€ì´ìŠ¤ ì‚¬ìš© (ê°€ì¤‘ì¹˜ ë‚®ì¶¤)
            confirmed_cases = similar_cases
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight_immoral = 0.0
        weighted_sum_immoral = 0.0
        
        total_weight_spam = 0.0
        weighted_sum_spam = 0.0
        
        for case in confirmed_cases:
            similarity = case.get('score', 0.0)
            metadata = case.get('metadata', {})
            
            # ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (ì œê³± ì‚¬ìš©)
            weight = similarity ** 2
            
            # ë¹„ìœ¤ë¦¬ ì ìˆ˜ ê°€ì¤‘ í•©
            immoral_score = float(metadata.get('immoral_score', 0.0))
            weighted_sum_immoral += immoral_score * weight
            total_weight_immoral += weight
            
            # ìŠ¤íŒ¸ ì ìˆ˜ ê°€ì¤‘ í•©
            spam_score = float(metadata.get('spam_score', 0.0))
            weighted_sum_spam += spam_score * weight
            total_weight_spam += weight
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        adjusted_immoral = (
            weighted_sum_immoral / total_weight_immoral 
            if total_weight_immoral > 0 else base_immoral_score
        )
        adjusted_spam = (
            weighted_sum_spam / total_weight_spam 
            if total_weight_spam > 0 else base_spam_score
        )
        
        # ì‹ ë¢°ë„ ì¦ê°€ëŸ‰ ê³„ì‚° (í™•ì •ëœ ì¼€ì´ìŠ¤ ê¸°ì¤€)
        confirmed_max_similarity = max([c.get('score', 0.0) for c in confirmed_cases]) if confirmed_cases else 0.0
        case_count_factor = min(len(confirmed_cases) / 3.0, 1.0)  # ìµœëŒ€ 3ê°œ ê¸°ì¤€
        confidence_boost = confirmed_max_similarity * case_count_factor * 0.2  # ìµœëŒ€ 20% ì¦ê°€
        
        # ì „ì²´ ìœ ì‚¬ ì¼€ì´ìŠ¤ì—ì„œ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚° (í™”ë©´ í‘œì‹œìš©)
        overall_max_similarity = max([c.get('score', 0.0) for c in similar_cases]) if similar_cases else 0.0
        
        return {
            'adjusted_immoral_score': adjusted_immoral,
            'adjusted_spam_score': adjusted_spam,
            'confidence_boost': confidence_boost,
            'similar_case_count': len(confirmed_cases),
            'confirmed_case_count': confirmed_count,  # ì‹¤ì œ í™•ì • ì¼€ì´ìŠ¤ ìˆ˜
            'max_similarity': overall_max_similarity  # ì „ì²´ ì¼€ì´ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½
        }
    
    def _combine_scores(
        self,
        base_immoral_score: float,
        base_spam_score: float,
        adjusted_scores: Dict[str, float]
    ) -> Dict[str, float]:
        """
        ê¸°ì¡´ ì ìˆ˜ì™€ ë³´ì • ì ìˆ˜ë¥¼ ê²°í•©
        
        Args:
            base_immoral_score: ê¸°ì¡´ ë¹„ìœ¤ë¦¬ ì ìˆ˜
            base_spam_score: ê¸°ì¡´ ìŠ¤íŒ¸ ì ìˆ˜
            adjusted_scores: ë³´ì • ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        
        Returns:
            Dict: ìµœì¢… ì ìˆ˜ ë° ë©”íƒ€ë°ì´í„°
        """
        similar_count = adjusted_scores.get('similar_case_count', 0)
        confirmed_count = adjusted_scores.get('confirmed_case_count', 0)
        max_similarity = adjusted_scores.get('max_similarity', 0.0)
        
        # ë³´ì • ì ìˆ˜ ë¹„ì¤‘ ê²°ì •
        if max_similarity >= 0.8:
            if confirmed_count >= 1:
                # í™•ì • ì¼€ì´ìŠ¤ 1ê°œ ì´ìƒ & ìœ ì‚¬ë„ 80% ì´ìƒ â†’ ìµœê³  ê°€ì¤‘ì¹˜
                adjustment_weight = 0.6
            elif similar_count >= 2:
                # ì¼ë°˜ ì¼€ì´ìŠ¤ 2ê°œ ì´ìƒ & ìœ ì‚¬ë„ 80% ì´ìƒ â†’ ë†’ì€ ê°€ì¤‘ì¹˜
                adjustment_weight = 0.5
            elif similar_count >= 1:
                # ì¼ë°˜ ì¼€ì´ìŠ¤ 1ê°œ ì´ìƒ & ìœ ì‚¬ë„ 80% ì´ìƒ â†’ ì¤‘ê°„ ê°€ì¤‘ì¹˜
                adjustment_weight = 0.3
            else:
                adjustment_weight = 0.1
        elif max_similarity >= 0.7:
            if confirmed_count >= 1:
                # í™•ì • ì¼€ì´ìŠ¤ 1ê°œ ì´ìƒ & ìœ ì‚¬ë„ 70~80% â†’ ì¤‘ê°„ ê°€ì¤‘ì¹˜
                adjustment_weight = 0.4
            elif similar_count >= 1:
                # ì¼ë°˜ ì¼€ì´ìŠ¤ 1ê°œ ì´ìƒ & ìœ ì‚¬ë„ 70~80% â†’ ë‚®ì€ ê°€ì¤‘ì¹˜
                adjustment_weight = 0.2
            else:
                adjustment_weight = 0.1
        else:
            # ê·¸ ì™¸ â†’ ìµœì†Œ ê°€ì¤‘ì¹˜
            adjustment_weight = 0.1
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        final_immoral = (
            base_immoral_score * (1 - adjustment_weight) +
            adjusted_scores['adjusted_immoral_score'] * adjustment_weight
        )
        
        final_spam = (
            base_spam_score * (1 - adjustment_weight) +
            adjusted_scores['adjusted_spam_score'] * adjustment_weight
        )
        
        return {
            'final_immoral_score': min(100.0, final_immoral),
            'final_spam_score': min(100.0, final_spam),
            'adjustment_applied': adjustment_weight > 0.1,
            'adjustment_weight': adjustment_weight
        }
    
    def _auto_save_high_confidence_case(
        self,
        text: str,
        immoral_score: float,
        spam_score: float,
        confidence: float,
        spam_confidence: float,
        post_id: str = "",
        user_id: str = ""
    ) -> None:
        """
        ì‹ ë¢°ë„ 80 ì´ìƒì¸ ì¼€ì´ìŠ¤ë¥¼ ë²¡í„°DBì— ìë™ ì €ì¥ (ë™ê¸° ë²„ì „)
        
        Args:
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            immoral_score: ë¹„ìœ¤ë¦¬ ì ìˆ˜
            spam_score: ìŠ¤íŒ¸ ì ìˆ˜
            confidence: ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„
            spam_confidence: ìŠ¤íŒ¸ ì‹ ë¢°ë„
            post_id: ê²Œì‹œë¬¼ ID (ì„ íƒ)
            user_id: ì‚¬ìš©ì ID (ì„ íƒ)
        """
        if not self.rag_enabled or not self.vector_client:
            return
        
        # ì‹ ë¢°ë„ 80 ì´ìƒì¸ ê²½ìš°ë§Œ ì €ì¥
        if confidence < 80.0 and spam_confidence < 80.0:
            return
        
        try:
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì²­í‚¹
            sentences = split_to_sentences(text, min_length=10)
            
            if not sentences:
                return
            
            # âš¡ ë°°ì¹˜ ì„ë² ë”© ìƒì„± (í•œ ë²ˆì˜ API í˜¸ì¶œ)
            embeddings = get_embeddings_batch(sentences)
            
            # ê° ë¬¸ì¥ë³„ë¡œ ì €ì¥
            for sentence, embedding in zip(sentences, embeddings):
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    "sentence": sentence,
                    "immoral_score": immoral_score,
                    "spam_score": spam_score,
                    "immoral_confidence": confidence,
                    "spam_confidence": spam_confidence,
                    "confidence": max(confidence, spam_confidence),  # ë†’ì€ ì‹ ë¢°ë„ ì‚¬ìš©
                    "confirmed": False,  # ê´€ë¦¬ì í™•ì¸ ì „
                    "post_id": post_id,
                    "user_id": user_id,
                    "created_at": datetime.now().isoformat(),
                    "feedback_type": "auto_saved"
                }
                
                # ë²¡í„°DBì— ì €ì¥
                upsert_confirmed_case(
                    client=self.vector_client,
                    embedding=embedding,
                    metadata=metadata
                )
            
            print(f"[INFO] ê³ ì‹ ë¢°ë„ ì¼€ì´ìŠ¤ ìë™ ì €ì¥ ì™„ë£Œ: {len(sentences)}ê°œ ë¬¸ì¥")
            
        except Exception as e:
            print(f"[WARN] ìë™ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _auto_save_high_confidence_case_async(
        self,
        text: str,
        immoral_score: float,
        spam_score: float,
        confidence: float,
        spam_confidence: float,
        post_id: str = "",
        user_id: str = ""
    ) -> None:
        """
        ì‹ ë¢°ë„ 80 ì´ìƒì¸ ì¼€ì´ìŠ¤ë¥¼ ë²¡í„°DBì— ë¹„ë™ê¸°ë¡œ ìë™ ì €ì¥
        âš¡ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ì–´ ì‚¬ìš©ì ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• (1~5ì´ˆ ê°œì„ )
        
        Args:
            text: ì €ì¥í•  í…ìŠ¤íŠ¸
            immoral_score: ë¹„ìœ¤ë¦¬ ì ìˆ˜
            spam_score: ìŠ¤íŒ¸ ì ìˆ˜
            confidence: ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„
            spam_confidence: ìŠ¤íŒ¸ ì‹ ë¢°ë„
            post_id: ê²Œì‹œë¬¼ ID (ì„ íƒ)
            user_id: ì‚¬ìš©ì ID (ì„ íƒ)
        """
        def save_task():
            """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  ì €ì¥ ì‘ì—…"""
            self._auto_save_high_confidence_case(
                text=text,
                immoral_score=immoral_score,
                spam_score=spam_score,
                confidence=confidence,
                spam_confidence=spam_confidence,
                post_id=post_id,
                user_id=user_id
            )
        
        # ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰ (ë©”ì¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ ìë™ ì¢…ë£Œ)
        thread = threading.Thread(target=save_task, daemon=True)
        thread.start()
        print(f"[INFO] ë²¡í„°DB ì €ì¥ ë°±ê·¸ë¼ìš´ë“œ ì‹œì‘ (ë¹„ë™ê¸°)")
    
    def analyze(self, text: str) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ìˆ˜í–‰ (ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ LLM ë¯¸ì‚¬ìš©)"""
        # 1. BERT ëª¨ë¸ ë¶„ì„
        bert_result = self.bert_predictor.predict(text)
        bert_score = bert_result['probabilities']['ë¹„ìœ¤ë¦¬ì '] * 100
        bert_confidence = bert_result['confidence'] * 100
        
        result = {
            'text': text,
            'bert_score': bert_score,
            'bert_confidence': bert_confidence,
        }
        
        # 2. ê·œì¹™ ê¸°ë°˜ ìŠ¤íŒ¸ ì ìˆ˜ ê³„ì‚° (LLM ì—†ì´ë„ ê°€ëŠ¥)
        rule_spam_score = self._calculate_rule_based_spam_score(text)
        
        # 3. ìš•ì„¤ ê°ì§€ ë° ë¶€ìŠ¤íŠ¸ ê³„ì‚° (LLM ì—†ì´ë„ ê°€ëŠ¥)
        profanity_info = self._calculate_profanity_boost(text)
        profanity_boost = profanity_info['boost_score']
        
        # 4. RAG ê¸°ë°˜ ì¦‰ì‹œ ì°¨ë‹¨ ì²´í¬ (LLM ë¶„ì„ ì „ì— ë¨¼ì € í™•ì¸)
        similar_cases = []
        rag_case_summaries = []
        
        adjustment_applied = False
        similar_cases_count = 0
        max_similarity = 0.0
        adjustment_weight = 0.0
        auto_blocked = False
        auto_block_reason = None

        if self.rag_enabled:
            try:
                # ìœ ì‚¬ ì¼€ì´ìŠ¤ ê²€ìƒ‰
                similar_cases = self._search_similar_cases(text)
                
                if similar_cases:
                    # ì¦‰ì‹œ ì°¨ë‹¨ ì¡°ê±´ ì²´í¬: ìœ ì‚¬ë„ 95% ì´ìƒ, ì ìˆ˜ 90 ì´ìƒì¸ ê´€ë¦¬ì í™•ì • ì‚¬ë¡€
                    for case in similar_cases:
                        metadata = case.get('metadata', {})
                        similarity = case.get('score', 0.0) * 100  # 0-1 ë²”ìœ„ë¥¼ 0-100ìœ¼ë¡œ ë³€í™˜
                        confidence = float(metadata.get('confidence', 0.0))
                        immoral_confidence = float(metadata.get('immoral_confidence', confidence))
                        spam_confidence_val = float(metadata.get('spam_confidence', confidence))
                        confirmed = bool(metadata.get('confirmed', False))
                        immoral_score = float(metadata.get('immoral_score', 0.0))
                        spam_score = float(metadata.get('spam_score', 0.0))
                        
                        # ê´€ë¦¬ì í™•ì • ì‚¬ë¡€ì´ê³ , ìœ ì‚¬ë„ 90% ì´ìƒ, ì ìˆ˜ 90 ì´ìƒì¸ ê²½ìš°
                        if confirmed and similarity >= 90.0:
                            # ë¹„ìœ¤ë¦¬ í™•ì • ì‚¬ë¡€
                            if immoral_score >= 90 and immoral_confidence >= 80:
                                auto_blocked = True
                                auto_block_reason = 'immoral'
                                
                                # LLM ë¶„ì„ ì—†ì´ ì¦‰ì‹œ ì°¨ë‹¨ ê²°ê³¼ ë°˜í™˜
                                print(f"[INFO] ì¦‰ì‹œ ì°¨ë‹¨ (LLM ë¯¸ì‚¬ìš©): ë¹„ìœ¤ë¦¬ í™•ì • ì‚¬ë¡€ì™€ ìœ ì‚¬ë„ {similarity:.1f}%, ì ìˆ˜ {immoral_score:.1f}, ì‹ ë¢°ë„ {immoral_confidence:.1f}")
                                
                                # ì¦‰ì‹œ ì°¨ë‹¨ ì¼€ì´ìŠ¤ëŠ” ë²¡í„°DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì´ë¯¸ ìœ ì‚¬í•œ í™•ì • ì‚¬ë¡€ê°€ ì¡´ì¬)
                                
                                result.update({
                                    'base_score': bert_score,
                                    'final_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ë¹„ìœ¤ë¦¬ ì ìˆ˜ null (BERT ë‹¨ë… ì •í™•ë„ ë‚®ìŒ)
                                    'final_confidence': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„ null
                                    'spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì ìˆ˜ null
                                    'spam_confidence': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì‹ ë¢°ë„ null
                                    'base_spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì ìˆ˜ null
                                    'rule_spam_score': rule_spam_score,
                                    'profanity_detected': profanity_info['profanity_detected'],
                                    'profanity_count': profanity_info['profanity_count'],
                                    'profanity_severity': profanity_info['severity'],
                                    'profanity_boost': profanity_boost,
                                    'types': metadata.get('types', ['ìš•ì„¤ ë° ë¹„ë°©']),  # ìœ ì‚¬ ì‚¬ë¡€ì˜ íƒ€ì… ì‚¬ìš©
                                    'weights': {
                                        'bert': 1.0,
                                        'llm': 0.0  # LLM ì‚¬ìš© ì•ˆí•¨
                                    },
                                    'rag_enabled': self.rag_enabled,
                                    'similar_cases_count': len(similar_cases),
                                    'max_similarity': similarity / 100.0,  # 0-1 ë²”ìœ„ë¡œ ë³€í™˜
                                    'adjustment_applied': False,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ RAG ë³´ì • ë¯¸ì ìš©
                                    'adjustment_weight': 0.0,
                                    'auto_blocked': True,
                                    'auto_block_reason': auto_block_reason,
                                    'adjusted_immoral_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ ë³´ì • ì ìˆ˜ ì—†ìŒ
                                    'adjusted_spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ ë³´ì • ì ìˆ˜ ì—†ìŒ
                                    'rag_similar_cases': [{
                                        'sentence': case.get('document', ''),
                                        'similarity': similarity,
                                        'immoral_score': immoral_score,
                                        'spam_score': spam_score,
                                        'confidence': confidence,
                                        'confirmed': True,
                                        'feedback_type': 'admin_confirmed',
                                        'created_at': metadata.get('created_at', '')
                                    }]
                                })
                                return result
                            
                            # ìŠ¤íŒ¸ í™•ì • ì‚¬ë¡€
                            elif spam_score >= 90 and spam_confidence_val >= 80:
                                auto_blocked = True
                                auto_block_reason = 'spam'
                                
                                # LLM ë¶„ì„ ì—†ì´ ì¦‰ì‹œ ì°¨ë‹¨ ê²°ê³¼ ë°˜í™˜
                                print(f"[INFO] ì¦‰ì‹œ ì°¨ë‹¨ (LLM ë¯¸ì‚¬ìš©): ìŠ¤íŒ¸ í™•ì • ì‚¬ë¡€ì™€ ìœ ì‚¬ë„ {similarity:.1f}%, ì ìˆ˜ {spam_score:.1f}, ì‹ ë¢°ë„ {spam_confidence_val:.1f}")
                                
                                # ì¦‰ì‹œ ì°¨ë‹¨ ì¼€ì´ìŠ¤ëŠ” ë²¡í„°DBì— ì €ì¥í•˜ì§€ ì•ŠìŒ (ì´ë¯¸ ìœ ì‚¬í•œ í™•ì • ì‚¬ë¡€ê°€ ì¡´ì¬)
                                
                                result.update({
                                    'base_score': bert_score,
                                    'final_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ë¹„ìœ¤ë¦¬ ì ìˆ˜ null (BERT ë‹¨ë… ì •í™•ë„ ë‚®ìŒ)
                                    'final_confidence': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„ null
                                    'spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì ìˆ˜ null
                                    'spam_confidence': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì‹ ë¢°ë„ null
                                    'base_spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨: ìŠ¤íŒ¸ ì ìˆ˜ null
                                    'rule_spam_score': rule_spam_score,
                                    'profanity_detected': profanity_info['profanity_detected'],
                                    'profanity_count': profanity_info['profanity_count'],
                                    'profanity_severity': profanity_info['severity'],
                                    'profanity_boost': profanity_boost,
                                    'types': metadata.get('types', ['ë„ë°° ë° ê´‘ê³ ']),  # ìœ ì‚¬ ì‚¬ë¡€ì˜ íƒ€ì… ì‚¬ìš©
                                    'weights': {
                                        'bert': 1.0,
                                        'llm': 0.0  # LLM ì‚¬ìš© ì•ˆí•¨
                                    },
                                    'rag_enabled': self.rag_enabled,
                                    'similar_cases_count': len(similar_cases),
                                    'max_similarity': similarity / 100.0,  # 0-1 ë²”ìœ„ë¡œ ë³€í™˜
                                    'adjustment_applied': False,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ RAG ë³´ì • ë¯¸ì ìš©
                                    'adjustment_weight': 0.0,
                                    'auto_blocked': True,
                                    'auto_block_reason': auto_block_reason,
                                    'adjusted_immoral_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ ë³´ì • ì ìˆ˜ ì—†ìŒ
                                    'adjusted_spam_score': None,  # ì¦‰ì‹œ ì°¨ë‹¨ì€ ë³´ì • ì ìˆ˜ ì—†ìŒ
                                    'rag_similar_cases': [{
                                        'sentence': case.get('document', ''),
                                        'similarity': similarity,
                                        'immoral_score': immoral_score,
                                        'spam_score': spam_score,
                                        'confidence': confidence,
                                        'confirmed': True,
                                        'feedback_type': 'admin_confirmed',
                                        'created_at': metadata.get('created_at', '')
                                    }]
                                })
                                return result
                    
                    # ìœ ì‚¬ ì‚¬ë¡€ ìš”ì•½ ìƒì„± (ìµœëŒ€ 5ê°œ)
                    for case in similar_cases[:5]:
                        metadata = case.get('metadata', {})
                        rag_case_summaries.append({
                            'sentence': case.get('document', ''),
                            'similarity': case.get('score', 0.0) * 100,  # 0-100 ë²”ìœ„ë¡œ ë³€í™˜
                            'immoral_score': float(metadata.get('immoral_score', 0.0)),
                            'spam_score': float(metadata.get('spam_score', 0.0)),
                            'confidence': float(metadata.get('confidence', 0.0)),
                            'confirmed': bool(metadata.get('confirmed', False)),
                            'feedback_type': metadata.get('feedback_type', ''),
                            'created_at': metadata.get('created_at', '')
                        })

                    # ì ìˆ˜ ë³´ì • ê³„ì‚°
                    adjusted_scores = self._adjust_scores_with_similarity(
                        base_immoral_score=base_final_score,
                        base_spam_score=base_final_spam_score,
                        similar_cases=similar_cases
                    )
                    
                    # ì ìˆ˜ ê²°í•©
                    combined_scores = self._combine_scores(
                        base_immoral_score=base_final_score,
                        base_spam_score=base_final_spam_score,
                        adjusted_scores=adjusted_scores
                    )
                    
                    adjusted_immoral_score = combined_scores['final_immoral_score']
                    adjusted_spam_score = combined_scores['final_spam_score']
                    adjustment_applied = combined_scores['adjustment_applied']
                    similar_cases_count = adjusted_scores['similar_case_count']
                    max_similarity = adjusted_scores['max_similarity']
                    adjustment_weight = combined_scores.get('adjustment_weight', 0.0)

                    # ì‹ ë¢°ë„ ë¶€ìŠ¤íŠ¸ ì ìš©
                    if adjustment_applied:
                        final_confidence = min(100.0, final_confidence + adjusted_scores['confidence_boost'])
                        spam_confidence = min(100.0, spam_confidence + adjusted_scores['confidence_boost'] * 0.5)
            except Exception as e:
                print(f"[WARN] RAG ë³´ì • ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ì¦‰ì‹œ ì°¨ë‹¨ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ LLM ë¶„ì„ ìˆ˜í–‰
        if not auto_blocked:
            print(f"[INFO] LLM ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            
            # LLM ë¶„ì„
            llm_result = self._analyze_with_llm(text)
            llm_score = llm_result['immoral_score']
            llm_confidence = llm_result['confidence']
            llm_spam_score = llm_result['spam_score']
            
            # ìŠ¤íŒ¸ ì ìˆ˜ ê²°í•©
        if rule_spam_score >= 80:
            final_spam_score = (llm_spam_score * 0.3) + (rule_spam_score * 0.7)
        else:
            final_spam_score = (llm_spam_score * 0.6) + (rule_spam_score * 0.4)
        
            # ìŠ¤íŒ¸ ì‹ ë¢°ë„ ê³„ì‚°
        if rule_spam_score > 60:
                rule_confidence = 95.0
        elif rule_spam_score > 30:
                rule_confidence = 85.0
        else:
                rule_confidence = 70.0
        
        spam_confidence = (llm_confidence * 0.6) + (rule_confidence * 0.4)
        
        result.update({
            'llm_score': llm_score,
            'llm_confidence': llm_confidence,
            'llm_spam_score': llm_spam_score,
            'rule_spam_score': rule_spam_score,
            'spam_score': final_spam_score,
            'spam_confidence': spam_confidence,
            'types': llm_result['types'],
            'profanity_detected': profanity_info['profanity_detected'],
            'profanity_count': profanity_info['profanity_count'],
            'profanity_severity': profanity_info['severity'],
            'profanity_boost': profanity_boost
        })
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        bert_weight = bert_confidence
        llm_weight = llm_confidence
        total_weight = bert_weight + llm_weight
        
        if total_weight > 0:
            bert_weight_norm = bert_weight / total_weight
            llm_weight_norm = llm_weight / total_weight
        else:
            bert_weight_norm = 0.5
            llm_weight_norm = 0.5
        
        # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê¸°ë³¸ ë¹„ìœ¤ë¦¬ ì ìˆ˜ ê³„ì‚°
        base_score = (bert_score * bert_weight_norm) + (llm_score * llm_weight_norm)
        final_confidence = (bert_confidence * bert_weight_norm) + (llm_confidence * llm_weight_norm)
        
        # ìš•ì„¤ ë¶€ìŠ¤íŠ¸ ì ìš©
        base_final_score = min(base_score + profanity_boost, 100.0)
        base_final_spam_score = final_spam_score
        
        # RAG ë³´ì •ì´ ìˆì—ˆë‹¤ë©´ ì¬ê³„ì‚°
        if self.rag_enabled and similar_cases and not auto_blocked:
            try:
                # ì ìˆ˜ ë³´ì • ê³„ì‚°
                adjusted_scores = self._adjust_scores_with_similarity(
                    base_immoral_score=base_final_score,
                    base_spam_score=base_final_spam_score,
                    similar_cases=similar_cases
                )
                
                # ì ìˆ˜ ê²°í•©
                combined_scores = self._combine_scores(
                    base_immoral_score=base_final_score,
                    base_spam_score=base_final_spam_score,
                    adjusted_scores=adjusted_scores
                )
                
                adjusted_immoral_score = combined_scores['final_immoral_score']
                adjusted_spam_score = combined_scores['final_spam_score']
                adjustment_applied = combined_scores['adjustment_applied']
                similar_cases_count = adjusted_scores['similar_case_count']
                max_similarity = adjusted_scores['max_similarity']
                adjustment_weight = combined_scores.get('adjustment_weight', 0.0)
                
                # ì‹ ë¢°ë„ ë¶€ìŠ¤íŠ¸ ì ìš©
                if adjustment_applied:
                    final_confidence = min(100.0, final_confidence + adjusted_scores['confidence_boost'])
                    spam_confidence = min(100.0, spam_confidence + adjusted_scores['confidence_boost'] * 0.5)
            except Exception as e:
                print(f"[WARN] RAG ì ìˆ˜ ë³´ì • ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ìµœì¢… ì ìˆ˜ ê²°ì •
        final_score = adjusted_immoral_score if adjustment_applied else base_final_score
        final_spam_score_result = adjusted_spam_score if adjustment_applied else base_final_spam_score
        
        # âš¡ ê³ ì‹ ë¢°ë„ ì¼€ì´ìŠ¤ ìë™ ì €ì¥ (ì‹ ë¢°ë„ 80 ì´ìƒ) - ë¹„ë™ê¸°ë¡œ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
        if final_confidence >= 80.0 or spam_confidence >= 80.0:
            try:
                self._auto_save_high_confidence_case_async(
                    text=text,
                    immoral_score=final_score,
                    spam_score=final_spam_score_result,
                    confidence=final_confidence,
                    spam_confidence=spam_confidence
                )
            except Exception as e:
                print(f"[WARN] ê³ ì‹ ë¢°ë„ ì¼€ì´ìŠ¤ ìë™ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        result.update({
            'base_score': base_score,
            'final_score': final_score,
            'final_confidence': final_confidence,
            'spam_score': final_spam_score_result,
            'spam_confidence': spam_confidence,
            'base_spam_score': base_final_spam_score,  # RAG ë³´ì • ì „ ìŠ¤íŒ¸ ì ìˆ˜ ì¶”ê°€
            'weights': {
                'bert': bert_weight_norm,
                'llm': llm_weight_norm
            },
            'rag_enabled': self.rag_enabled,
            'similar_cases_count': similar_cases_count,
            'max_similarity': max_similarity,
            'adjustment_applied': adjustment_applied,
            'adjustment_weight': adjustment_weight if adjustment_applied else 0.0,
            'adjusted_immoral_score': adjusted_immoral_score if adjustment_applied else None,
            'adjusted_spam_score': adjusted_spam_score if adjustment_applied else None,
                'rag_similar_cases': rag_case_summaries,
                'auto_blocked': False
        })
        
        return result
    
    def analyze_batch(self, texts: List[str]) -> List[Dict]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì¼ê´„ ë¶„ì„ (LLM í•„ìˆ˜ ì‚¬ìš©)"""
        results = []
        for text in texts:
            results.append(self.analyze(text))
        return results

