"""
ğŸ¯ Mock API ì—”ë“œí¬ì¸íŠ¸
ì‹œë‹ˆì–´ì˜ ì„¤ëª…:
- ì‹¤ì œ ë°±ì—”ë“œ ì™„ì„± ì „ê¹Œì§€ ì‚¬ìš©í•  ê°€ì§œ ë°ì´í„°
- í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ì‹œ ìœ ìš©
- ë‚˜ì¤‘ì— ì‹¤ì œ DBë¡œ êµì²´
"""

import logging
import os

from fastapi import APIRouter, Query, HTTPException, Request
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from collections import Counter
import random
import time
import httpx
router = APIRouter(tags=["api"])
logger = logging.getLogger(__name__)

# Ethics Analyzer ì „ì—­ ë³€ìˆ˜ (main.pyì—ì„œ ì´ˆê¸°í™”ë¨)
ethics_analyzer = None

# ============================================
# ğŸ“Š ë°ì´í„° ëª¨ë¸ (Pydantic)
# ============================================

class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    id: int
    title: str
    content: str
    author: str
    date: str
    category: str

class BounceMetrics(BaseModel):
    """ì´íƒˆë¥  ë©”íŠ¸ë¦­"""
    avg_bounce_rate: float
    total_visitors: int
    bounced_visitors: int
    period: str

class TrendItem(BaseModel):
    """íŠ¸ë Œë“œ ì•„ì´í…œ"""
    keyword: str
    mentions: int
    change: float
    category: str

class ReportCategory(BaseModel):
    """ì‹ ê³  ì¹´í…Œê³ ë¦¬"""
    name: str
    count: int
    status: str
    avg_processing_time: str

class EthicsScoreRequest(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ìš”ì²­"""
    text: str

class EthicsScoreResponse(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ì‘ë‹µ"""
    ethics_score: float
    detected_expressions: List[dict]
    recommendations: List[dict]

# ============================================
# ğŸ” ê²€ìƒ‰ API
# ============================================

@router.get("/search")
async def search(q: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ")):
    """
    ìì—°ì–´ ê²€ìƒ‰ API
    
    **ì‹œë‹ˆì–´ì˜ íŒ:**
    - Query(...) : í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    - Query(None) : ì„ íƒì  íŒŒë¼ë¯¸í„°
    """
    
    if not q:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # Mock ë°ì´í„° ìƒì„±
    results = [
        {
            "id": i,
            "title": f"{q}ì— ê´€í•œ ê²Œì‹œê¸€ {i+1}",
            "content": f"ì´ê²ƒì€ '{q}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìƒ˜í”Œ ê²Œì‹œê¸€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë©ë‹ˆë‹¤.",
            "author": f"ì‚¬ìš©ì{random.randint(1, 100)}",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "category": random.choice(["ììœ ê²Œì‹œíŒ", "ì§ˆë¬¸", "ì •ë³´", "í† ë¡ "])
        }
        for i in range(5)
    ]
    
    return {
        "query": q,
        "total": len(results),
        "results": results
    }

# ============================================
# ğŸ“Š ì´íƒˆë¥  ë©”íŠ¸ë¦­ API
# ============================================

@router.get("/metrics/bounce")
async def get_bounce_metrics():
    """
    ë°©ë¬¸ê° ì´íƒˆë¥  ë°ì´í„°
    
    **Mock ë°ì´í„°:**
    ì‹¤ì œë¡œëŠ” Google Analyticsë‚˜ ìì²´ ë¶„ì„ ì‹œìŠ¤í…œì—ì„œ ê°€ì ¸ì˜´
    """
    
    return {
        "metrics": {
            "avg_bounce_rate": 42.5,
            "total_visitors": 15234,
            "bounced_visitors": 6474,
            "period": "2025-01-01 ~ 2025-01-31"
        },
        "details": [
            {
                "date": f"2025-01-{i+1:02d}",
                "visitors": random.randint(300, 800),
                "bounced": random.randint(100, 400),
                "bounce_rate": random.uniform(30, 60)
            }
            for i in range(7)
        ]
    }

# ============================================
# ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ API (ì‹¤ì œ ë°ì´í„°)
# ============================================

@router.get("/trends")
async def get_trends(limit: int = Query(100, ge=1, le=1000)):
    
    # â­ í‚¤ì›Œë“œ ì •ê·œí™” ë§¤í•‘ (ìì—°ì–´ â†’ í‚¤ì›Œë“œ)
    KEYWORD_NORMALIZATION = {
        "ê²€ìƒ‰í–ˆìŒ": "ê²€ìƒ‰",
        "ê²€ìƒ‰í•˜ê¸°": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì¤‘": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì–´": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì–´ë“¤": "ê²€ìƒ‰",
        "ì•ˆë…•í•˜ì„¸ìš”": "ì¸ì‚¬",
        "ì•ˆë…•": "ì¸ì‚¬",
        "ã…ã…‡": "ì¸ì‚¬",
    }
    
    def normalize_keyword(word: str) -> str:
        """í‚¤ì›Œë“œ ì •ê·œí™”"""
        word = word.strip()
        return KEYWORD_NORMALIZATION.get(word, word)
    
    try:
        # â­ Mock ë°ì´í„° ê°•ì œ ì‚¬ìš© (410ê°œ í’ì„±í•œ ë°ì´í„°!)
        raise Exception("Force using mock data with 410 keywords")
        
        print(f"\n[DEBUG] Calling dad.dothome.co.kr API with limit={limit}")
        async with httpx.AsyncClient(timeout=10.0) as client:
            # âœ… 1. ì‹¤ì œ ì¸ê¸° ê²€ìƒ‰ì–´ API í˜¸ì¶œ (standalone ë²„ì „)
            url = "https://dad.dothome.co.kr/adm/popular_api_standalone.php"
            print(f"[DEBUG] URL: {url}")
            
            response = await client.get(url, params={"limit": limit})
            print(f"[DEBUG] Response status: {response.status_code}")
            print(f"[DEBUG] Response content-type: {response.headers.get('content-type')}")
            
            response.raise_for_status()
            
            data = response.json()
            print(f"[DEBUG] JSON parsed successfully")
            print(f"[DEBUG] success={data.get('success')}, data_count={len(data.get('data', []))}")
            
            if not data.get("success", False):
                raise Exception("API returned error")
            
            # âœ… 2. ê²Œì‹œê¸€/ëŒ“ê¸€ í†µê³„ API í˜¸ì¶œ
            stats_url = "https://dad.dothome.co.kr/adm/board_stats_api.php"
            print(f"[DEBUG] Fetching board stats from: {stats_url}")
            
            stats_response = await client.get(stats_url)
            stats_data = stats_response.json()
            
            total_posts = 0
            total_comments = 0
            
            if stats_data.get("success"):
                total_posts = stats_data["data"]["total_posts"]
                total_comments = stats_data["data"]["total_comments"]
                print(f"[DEBUG] Board stats: posts={total_posts}, comments={total_comments}")
                
                # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
                if "debug" in stats_data:
                    print(f"[DEBUG] Tables found: {stats_data['debug'].get('tables_found', [])}")
            else:
                print(f"[DEBUG] Board stats API failed, using defaults")
            
            # ë°ì´í„° ë³€í™˜
            api_data = data.get("data", [])
            print(f"[DEBUG] Converting {len(api_data)} items to keywords")
            
            # â­ í‚¤ì›Œë“œ ì •ê·œí™” + ë¹ˆë„ ì§‘ê³„
            word_counts = Counter()
            date_word_counts = {}  # ë‚ ì§œë³„ í‚¤ì›Œë“œ ë¹ˆë„
            
            for item in api_data:
                word = item.get("word", "").strip()
                date = item.get("date", "")
                
                if word:
                    # í‚¤ì›Œë“œ ì •ê·œí™”
                    normalized_word = normalize_keyword(word)
                    word_counts[normalized_word] += 1
                    
                    # ë‚ ì§œë³„ ì§‘ê³„
                    if date not in date_word_counts:
                        date_word_counts[date] = Counter()
                    date_word_counts[date][normalized_word] += 1
            
            # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í‚¤ì›Œë“œ ìƒì„±
            keywords = [
                {
                    "word": word,
                    "count": count
                }
                for word, count in word_counts.most_common()
            ]
            
            print(f"[DEBUG] Top 5 keywords (after normalization): {keywords[:5]}")
            
            # â­ ì¦ê°ë¥  ê³„ì‚° (ë‚ ì§œë³„ ë¹„êµ)
            dates = sorted(date_word_counts.keys())
            trends = []
            
            for kw in keywords[:10]:
                word = kw["word"]
                
                # ìµœê·¼ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œì˜ ê²€ìƒ‰ íšŸìˆ˜ ë¹„êµ
                if len(dates) >= 2:
                    recent_count = date_word_counts[dates[-1]].get(word, 0)
                    previous_count = date_word_counts[dates[-2]].get(word, 0)
                    
                    if previous_count > 0:
                        change = ((recent_count - previous_count) / previous_count) * 100
                    else:
                        change = 100.0 if recent_count > 0 else 0.0
                else:
                    change = 0.0
                
                # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                if change > 50:
                    category = "ê¸‰ìƒìŠ¹"
                elif change > 0:
                    category = "ìƒìŠ¹"
                elif change < -50:
                    category = "ê¸‰ê°"
                elif change < 0:
                    category = "í•˜ë½"
                else:
                    category = "ìœ ì§€"
                
                trends.append({
                    "keyword": word,
                    "mentions": kw["count"],
                    "change": round(change, 1),
                    "category": category
                })
            
            # â­ íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„± (ë‚ ì§œë³„ ê²€ìƒ‰ íšŸìˆ˜)
            timeline = []
            for date in sorted(dates):
                total_count = sum(date_word_counts[date].values())
                timeline.append({
                    "date": date,
                    "count": total_count
                })
            
            # â­ ì‹¤ì œ í†µê³„ ê³„ì‚°
            total_searches = sum(word_counts.values())
            unique_keywords = len(keywords)
            
            return {
                "summary": {
                    "total_posts": total_posts,             # â­ ì‹¤ì œ ê²Œì‹œê¸€ ìˆ˜
                    "total_comments": total_comments,        # â­ ì‹¤ì œ ëŒ“ê¸€ ìˆ˜
                    "total_searches": total_searches,        # ì´ ê²€ìƒ‰ íšŸìˆ˜
                    "unique_keywords": unique_keywords,      # ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜
                    "total_trends": len(keywords),
                    "new_trends": len([t for t in trends if t["change"] > 50]),
                    "rising_trends": len([t for t in trends if t["change"] > 0])
                },
                "keywords": keywords,
                "trends": trends,
                "timeline": timeline,  # â­ íƒ€ì„ë¼ì¸ ë°ì´í„° ì¶”ê°€!
                "source": "dad.dothome.co.kr",
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        # â­ ì—ëŸ¬ ë°œìƒ ì‹œ í˜„ì‹¤ì ì¸ Mock ë°ì´í„° ë°˜í™˜ (ë¡œê·¸ì¸ ë¬¸ì œ ëŒ€ì‘)
        print(f"[INFO] Using mock data due to: {e}")
        
        # í˜„ì‹¤ì ì¸ í•œêµ­ì–´ í‚¤ì›Œë“œ Mock ë°ì´í„° (400ê°œ ì´ìƒ!)
        mock_keywords_pool = [
            # ê¸°ìˆ /IT (60ê°œ)
            "ì¸ê³µì§€ëŠ¥", "ChatGPT", "ë¸”ë¡ì²´ì¸", "ë©”íƒ€ë²„ìŠ¤", "NFT",
            "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë”¥ëŸ¬ë‹", "ë¨¸ì‹ ëŸ¬ë‹",
            "ììœ¨ì£¼í–‰", "ì „ê¸°ì°¨", "í…ŒìŠ¬ë¼", "ì‚¼ì„±ì „ì", "ë°˜ë„ì²´",
            "5G", "6G", "IoT", "ìŠ¤ë§ˆíŠ¸í™ˆ", "ì›¨ì–´ëŸ¬ë¸”",
            "ë¡œë´‡", "ë“œë¡ ", "AR", "VR", "XR",
            "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤", "ì¿ ë²„ë„¤í‹°ìŠ¤", "ë„ì»¤", "ê¹ƒí—ˆë¸Œ", "ì˜¤í”ˆì†ŒìŠ¤",
            "Python", "JavaScript", "React", "Vue", "TypeScript",
            "AWS", "Azure", "GCP", "DevOps", "CI/CD",
            "Node.js", "Django", "Flask", "Spring", "FastAPI",
            "MongoDB", "PostgreSQL", "MySQL", "Redis", "Elasticsearch",
            "API", "REST", "GraphQL", "gRPC", "WebSocket",
            "Linux", "Ubuntu", "CentOS", "Windows", "macOS",
            
            # ê²½ì œ/ê¸ˆìœµ (50ê°œ)
            "ì£¼ì‹", "ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "ë¦¬í”Œ", "ë¶€ë™ì‚°",
            "ê¸ˆë¦¬", "í™˜ìœ¨", "ë‹¬ëŸ¬", "ì›í™”", "ì—”í™”",
            "ì¦ì‹œ", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë‚˜ìŠ¤ë‹¥", "ë‹¤ìš°ì¡´ìŠ¤",
            "S&P500", "ì±„ê¶Œ", "í€ë“œ", "ETF", "ë¦¬ì¸ ",
            "ë°°ë‹¹", "ì¬í…Œí¬", "íˆ¬ì", "ì €ì¶•", "ëŒ€ì¶œ",
            "ì‹ ìš©ì¹´ë“œ", "ì²´í¬ì¹´ë“œ", "í˜„ê¸ˆ", "ëª¨ë°”ì¼ë±…í‚¹", "í•€í…Œí¬",
            "ê°„í¸ê²°ì œ", "ì¹´ì¹´ì˜¤í˜ì´", "ë„¤ì´ë²„í˜ì´", "í† ìŠ¤", "í˜ì´ì½”",
            "ë±…í¬ìƒëŸ¬ë“œ", "ì²­ì•½", "ë¶„ì–‘", "ë§¤ë§¤", "ì„ëŒ€",
            "ì„¸ê¸ˆ", "ì ˆì„¸", "ì†Œë“ì„¸", "ë²•ì¸ì„¸", "ë¶€ê°€ì„¸",
            "ì—°ê¸ˆ", "ë³´í—˜", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "CMA",
            "ISA", "IRP", "í‡´ì§ì—°ê¸ˆ", "401k", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ",
            
            # ì—°ì˜ˆ/ë¬¸í™” (60ê°œ)
            "K-POP", "BTS", "ë¸”ë™í•‘í¬", "ë‰´ì§„ìŠ¤", "ì•„ì´ë¸Œ",
            "ë¥´ì„¸ë¼í•Œ", "ì—ìŠ¤íŒŒ", "íŠ¸ì™€ì´ìŠ¤", "ì„¸ë¸í‹´", "ì—”ì‹œí‹°",
            "ì•„ì´ë“¤", "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ", "ì—”í•˜ì´í”ˆ", "ìˆì§€", "ì¼€í”ŒëŸ¬",
            "IU", "ì„ì˜ì›…", "íƒœì—°", "ì•„ì´ìœ ", "ì§€ë“œë˜ê³¤",
            "ì˜í™”", "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "ì›¹íˆ°", "ë§Œí™”",
            "ë„·í”Œë¦­ìŠ¤", "ë””ì¦ˆë‹ˆí”ŒëŸ¬ìŠ¤", "í‹°ë¹™", "ì›¨ì´ë¸Œ", "ì™“ì± ",
            "ìœ íŠœë¸Œ", "í‹±í†¡", "ì¸ìŠ¤íƒ€ê·¸ë¨", "í˜ì´ìŠ¤ë¶", "íŠ¸ìœ„í„°",
            "ì‡¼ì¸ ", "ë¦´ìŠ¤", "ìŠ¤í† ë¦¬", "ë¼ì´ë¸Œ", "ìŠ¤íŠ¸ë¦¬ë°",
            "ì½˜ì„œíŠ¸", "ë®¤ì§€ì»¬", "ì „ì‹œíšŒ", "í˜ìŠ¤í‹°ë²Œ", "ê³µì—°",
            "OST", "ìŒì›", "ì°¨íŠ¸", "ë©œë¡ ", "ì§€ë‹ˆ",
            "ë²…ìŠ¤", "ë°”ì´ë¸Œ", "í”Œë¡œ", "ìŠ¤í¬í‹°íŒŒì´", "ì• í”Œë®¤ì§",
            "ì•„ì¹´ë°ë¯¸", "ì¹¸ì˜í™”ì œ", "ê¸ˆì¢…ì˜í™”ì œ", "ë°±ìƒì˜ˆìˆ ëŒ€ìƒ", "ê³¨ë“ ê¸€ë¡œë¸Œ",
            
            # ìŠ¤í¬ì¸  (40ê°œ)
            "ì¶•êµ¬", "ì•¼êµ¬", "ë°°êµ¬", "ë†êµ¬", "í…Œë‹ˆìŠ¤",
            "ê³¨í”„", "ìˆ˜ì˜", "ìœ¡ìƒ", "ë°°ë“œë¯¼í„´", "íƒêµ¬",
            "eìŠ¤í¬ì¸ ", "LOL", "ì˜¤ë²„ì›Œì¹˜", "ë°°ê·¸", "í”¼íŒŒ",
            "ë°œë¡œë€íŠ¸", "ë¡¤", "LCK", "LPL", "ì›”ë“œì»µ",
            "ì†í¥ë¯¼", "í™©í¬ì°¬", "ì´ê°•ì¸", "ê¹€ë¯¼ì¬", "ì¡°ê·œì„±",
            "ë©”ì‹œ", "í˜¸ë‚ ë‘", "ìŒë°”í˜", "í™€ë€ë“œ", "ë„¤ì´ë§ˆë¥´",
            "í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸", "ë¼ë¦¬ê°€", "ë¶„ë°ìŠ¤ë¦¬ê°€", "ì„¸ë¦¬ì—A", "Kë¦¬ê·¸",
            "KBO", "MLB", "NPB", "ì˜¬ë¦¼í”½", "ì•„ì‹œì•ˆê²Œì„",
            
            # ê±´ê°•/ì˜ë£Œ (40ê°œ)
            "ì½”ë¡œë‚˜", "ë°±ì‹ ", "ê±´ê°•", "ë‹¤ì´ì–´íŠ¸", "ìš´ë™",
            "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "í—¬ìŠ¤", "PT", "í™ˆíŠ¸",
            "ë¹„íƒ€ë¯¼", "ì˜ì–‘ì œ", "ë‹¨ë°±ì§ˆ", "í”„ë¡œí‹´", "ë³´ì¶©ì œ",
            "ë³‘ì›", "ì˜ì‚¬", "ê°„í˜¸ì‚¬", "ì•½êµ­", "í•œì˜ì›",
            "ì •ì‹ ê±´ê°•", "ìš°ìš¸ì¦", "ë¶ˆì•ˆ", "ê³µí™©", "ìŠ¤íŠ¸ë ˆìŠ¤",
            "ìˆ˜ë©´", "ë¶ˆë©´ì¦", "ëª…ìƒ", "ë§ˆìŒì±™ê¹€", "íë§",
            "ë‹¤ì´ì–´íŠ¸ì‹ë‹¨", "í—¬ìŠ¤ì¥", "í”¼íŠ¸ë‹ˆìŠ¤", "í¬ë¡œìŠ¤í•", "ìŠ¤í”¼ë‹",
            "ìŠ¤íŠ¸ë ˆì¹­", "ê·¼ë ¥ìš´ë™", "ìœ ì‚°ì†Œ", "ë¬´ì‚°ì†Œ", "ì¬í™œ",
            
            # ìŒì‹/ì—¬í–‰ (50ê°œ)
            "ë§›ì§‘", "ì¹´í˜", "ë””ì €íŠ¸", "ë² ì´ì»¤ë¦¬", "ë¸ŒëŸ°ì¹˜",
            "ë ˆìŠ¤í† ë‘", "ë·”í˜", "ì¼ì‹", "ì¤‘ì‹", "í•œì‹",
            "ì–‘ì‹", "ë¶„ì‹", "ì¹˜í‚¨", "í”¼ì", "í–„ë²„ê±°",
            "ì¡±ë°œ", "ë³´ìŒˆ", "ì‚¼ê²¹ì‚´", "ê³±ì°½", "íšŒ",
            "ì´ˆë°¥", "ë¼ë©˜", "ìš°ë™", "ëˆê°€ìŠ¤", "ì¹´ë ˆ",
            "ì§œì¥ë©´", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡", "ë§ˆë¼íƒ•", "í› ê¶ˆ",
            "ì»¤í”¼", "ì°¨", "ë°€í¬í‹°", "ìŠ¤ë¬´ë””", "ì—ì´ë“œ",
            "ì—¬í–‰", "ì œì£¼ë„", "ë¶€ì‚°", "ê°•ë¦‰", "ê²½ì£¼",
            "ì „ì£¼", "ì—¬ìˆ˜", "ì†ì´ˆ", "ì¸ì²œ", "ìˆ˜ì›",
            "í•´ì™¸ì—¬í–‰", "ì¼ë³¸", "ëŒ€ë§Œ", "íƒœêµ­", "ë² íŠ¸ë‚¨",
            "ìœ ëŸ½", "ë¯¸êµ­", "í˜¸ì£¼", "í˜¸í…”", "ë¦¬ì¡°íŠ¸",
            
            # ITê¸°ê¸°/ê°€ì „ (40ê°œ)
            "ì•„ì´í°", "ê°¤ëŸ­ì‹œ", "ë§¥ë¶", "ì•„ì´íŒ¨ë“œ", "ê°¤ëŸ­ì‹œíƒ­",
            "ë…¸íŠ¸ë¶", "ë°ìŠ¤í¬í†±", "ê²Œì´ë°PC", "ë§ˆìš°ìŠ¤", "í‚¤ë³´ë“œ",
            "ëª¨ë‹ˆí„°", "TV", "ëƒ‰ì¥ê³ ", "ì„¸íƒê¸°", "ê±´ì¡°ê¸°",
            "ì—ì–´ì»¨", "ê³µê¸°ì²­ì •ê¸°", "ì²­ì†Œê¸°", "ë¡œë´‡ì²­ì†Œê¸°", "ì‹ê¸°ì„¸ì²™ê¸°",
            "ì „ìë ˆì¸ì§€", "ì˜¤ë¸", "ì—ì–´í”„ë¼ì´ì–´", "ë¯¹ì„œê¸°", "ì»¤í”¼ë¨¸ì‹ ",
            "ìŠ¤ë§ˆíŠ¸ì›Œì¹˜", "ê°¤ëŸ­ì‹œì›Œì¹˜", "ì• í”Œì›Œì¹˜", "ì—ì–´íŒŸ", "ê°¤ëŸ­ì‹œë²„ì¦ˆ",
            "ì´ì–´í°", "í—¤ë“œí°", "ìŠ¤í”¼ì»¤", "ì‚¬ìš´ë“œë°”", "ë¹”í”„ë¡œì í„°",
            "ì¹´ë©”ë¼", "DSLR", "ë¯¸ëŸ¬ë¦¬ìŠ¤", "ì•¡ì…˜ìº ", "ë“œë¡ ì¹´ë©”ë¼",
            
            # íŒ¨ì…˜/ë·°í‹° (40ê°œ)
            "íŒ¨ì…˜", "ë·°í‹°", "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´", "ë©”ì´í¬ì—…",
            "ë¦½ìŠ¤í‹±", "íŒŒìš´ë°ì´ì…˜", "ì¿ ì…˜", "ì„ í¬ë¦¼", "ì„¸ëŸ¼",
            "í† ë„ˆ", "ì—ì„¼ìŠ¤", "ì•°í”Œ", "í¬ë¦¼", "ë§ˆìŠ¤í¬íŒ©",
            "í´ë Œì§•", "í¼í´ë Œì§•", "í´ë Œì§•ì˜¤ì¼", "ë¦¬ë¬´ë²„", "ë¯¸ìŠ¤íŠ¸",
            "ë‚˜ì´í‚¤", "ì•„ë””ë‹¤ìŠ¤", "í‘¸ë§ˆ", "ë‰´ë°œë€ìŠ¤", "ì»¨ë²„ìŠ¤",
            "ëª…í’ˆ", "êµ¬ì°Œ", "ìƒ¤ë„¬", "ë£¨ì´ë¹„í†µ", "ì—ë¥´ë©”ìŠ¤",
            "í”„ë¼ë‹¤", "ë²„ë²„ë¦¬", "ë°œë Œì‹œì•„ê°€", "ìƒë¡œë‘", "ë””ì˜¬",
            "ì‹ ë°œ", "ìš´ë™í™”", "ìŠ¤ë‹ˆì»¤ì¦ˆ", "êµ¬ë‘", "ìƒŒë“¤",
            
            # ìƒí™œ/ì£¼ê±° (30ê°œ)
            "ì•„íŒŒíŠ¸", "ì˜¤í”¼ìŠ¤í…”", "ë¹Œë¼", "ì›ë£¸", "íˆ¬ë£¸",
            "ì „ì„¸", "ì›”ì„¸", "ë§¤ë§¤", "ì²­ì•½", "ë¶„ì–‘",
            "ì¸í…Œë¦¬ì–´", "ë¦¬ëª¨ë¸ë§", "ê°€êµ¬", "ì´ì¼€ì•„", "í•œìƒ˜",
            "ì¹¨ëŒ€", "ì†ŒíŒŒ", "ì±…ìƒ", "ì˜ì", "ìˆ˜ë‚©",
            "ì¡°ëª…", "ì»¤íŠ¼", "ëŸ¬ê·¸", "ì¿ ì…˜", "ì´ë¶ˆ",
            "ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "í™©ì‚¬", "íƒœí’", "í­ì—¼",
            
            # êµìœ¡/ì·¨ì—… (40ê°œ)
            "ì·¨ì—…", "ì´ì§", "ì—°ë´‰", "ë©´ì ‘", "ìì†Œì„œ",
            "ì´ë ¥ì„œ", "í¬íŠ¸í´ë¦¬ì˜¤", "ê²½ë ¥", "ì¸í„´", "ì‹ ì…",
            "ìŠ¤íƒ€íŠ¸ì—…", "ëŒ€ê¸°ì—…", "ì¤‘ê²¬ê¸°ì—…", "ì™¸êµ­ê³„", "ê³µê¸°ì—…",
            "ê³µë¬´ì›", "êµì‚¬", "ê°„í˜¸ì‚¬", "ì˜ì‚¬", "ë³€í˜¸ì‚¬",
            "ìê²©ì¦", "í† ìµ", "í† í”Œ", "ì˜¤í”½", "JPT",
            "HSK", "ì½”ë”©í…ŒìŠ¤íŠ¸", "ì•Œê³ ë¦¬ì¦˜", "SQL", "ì—‘ì…€",
            "íŒŒì›Œí¬ì¸íŠ¸", "ì›Œë“œ", "í•œê¸€", "í”„ë ˆì  í…Œì´ì…˜", "ì˜ì–´íšŒí™”",
            "í•™ì›", "ê³¼ì™¸", "ì¸ê°•", "ê°•ì˜", "êµìœ¡"
        ]
        
        # ëœë¤í•˜ê²Œ limitê°œ ì„ íƒí•˜ê³  ì‹¤ì œê°™ì€ ë¹ˆë„ ë¶€ì—¬
        selected_keywords = random.sample(
            mock_keywords_pool, 
            min(limit, len(mock_keywords_pool))
        )
        
        # ì‹¤ì œê°™ì€ ê²€ìƒ‰ ë¹ˆë„ ìƒì„± (10~300íšŒë¡œ í¬ê²Œ í™•ëŒ€!)
        keywords = [
            {
                "word": kw,
                "count": random.randint(5, 1000)  # ë¹ˆë„ ë²”ìœ„ ëŒ€í­ í™•ëŒ€
            }
            for kw in selected_keywords
        ]
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
        keywords.sort(key=lambda x: x["count"], reverse=True)
        
        # ìƒìœ„ 10ê°œë¡œ íŠ¸ë Œë“œ ìƒì„±
        trends = [
            {
                "keyword": kw["word"],
                "mentions": kw["count"],  # â­ countì™€ ë™ì¼í•˜ê²Œ!
                "change": random.uniform(-30, 50),
                "category": random.choice(["ì¸ê¸°", "íŠ¸ë Œë“œ", "ì´ìŠˆ", "ê¸‰ìƒìŠ¹", "í™”ì œ"])
            }
            for kw in keywords[:10]
        ]
        
        # íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„± (ìµœê·¼ 30ì¼)
        timeline = []
        for i in range(30):
            date = (datetime.now() - timedelta(days=29-i)).strftime("%Y-%m-%d")
            # ì¼ì˜ ìë¦¬ê°€ 0ì´ë‚˜ 5ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜
            base_count = random.randint(50, 500)
            if base_count % 10 == 0 or base_count % 10 == 5:
                base_count += random.choice([1, 2, 3, 4, 6, 7, 8, 9])
            timeline.append({
                "date": date,
                "count": base_count
            })
        
        # í†µê³„ ìˆ˜ì¹˜ ìƒì„± (ì¼ì˜ ìë¦¬ê°€ 0ì´ë‚˜ 5ê°€ ì•„ë‹ˆë„ë¡)
        def make_natural_number(min_val, max_val):
            """ì¼ì˜ ìë¦¬ê°€ 0ì´ë‚˜ 5ê°€ ì•„ë‹Œ ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ ìƒì„±"""
            num = random.randint(min_val, max_val)
            last_digit = num % 10
            if last_digit == 0 or last_digit == 5:
                # 1, 2, 3, 4, 6, 7, 8, 9 ì¤‘ í•˜ë‚˜ë¡œ ì¡°ì •
                adjustment = random.choice([1, 2, 3, 4, 6, 7, 8, 9])
                num = (num // 10) * 10 + adjustment
            return num
        
        return {
            "summary": {
                "total_posts": make_natural_number(5000, 15000),
                "total_comments": make_natural_number(10000, 50000),
                "total_searches": sum(kw["count"] for kw in keywords),
                "unique_keywords": len(keywords),
                "total_trends": len(keywords),
                "new_trends": len([t for t in trends if t["change"] > 20]),
                "rising_trends": len([t for t in trends if t["change"] > 0])
            },
            "keywords": keywords,
            "trends": trends,
            "timeline": timeline,
            "source": "mock_data",
            "note": "API ì¸ì¦ ë¬¸ì œë¡œ Mock ë°ì´í„° ì‚¬ìš© ì¤‘",
            "timestamp": datetime.now().isoformat()
        }

# ============================================
# ğŸš¨ ì‹ ê³ ê¸€ ë¶„ë¥˜ API
# ============================================

@router.get("/reports/moderation")
async def get_reports():
    """ì‹ ê³ ê¸€ í†µê³„ ë°ì´í„°"""
    
    categories = [
        ("ìŠ¤íŒ¸/ê´‘ê³ ", "pending"),
        ("ìš•ì„¤/ë¹„ë°©", "resolved"),
        ("ìŒë€ë¬¼", "resolved"),
        ("ê°œì¸ì •ë³´ ë…¸ì¶œ", "pending"),
        ("ì €ì‘ê¶Œ ì¹¨í•´", "rejected"),
        ("ê¸°íƒ€", "pending")
    ]
    
    total = sum(random.randint(10, 100) for _ in categories)
    
    return {
        "stats": {
            "total": total,
            "pending": random.randint(20, 50),
            "resolved": random.randint(30, 60),
            "rejected": random.randint(5, 15)
        },
        "categories": [
            {
                "name": name,
                "count": random.randint(10, 100),
                "status": status,
                "avg_processing_time": f"{random.randint(1, 48)}ì‹œê°„"
            }
            for name, status in categories
        ]
    }

# ============================================
# âš ï¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ API
# ============================================

@router.post("/moderation/ethics-score")
async def analyze_ethics_score(request: EthicsScoreRequest):

    text = request.text.strip()
    
    if not text:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ Mock ë¶„ì„
    ethics_keywords = ["ë°”ë³´", "ë©ì²­", "ì“°ë ˆê¸°", "ì£½ì–´", "êº¼ì ¸"]
    detected = []
    
    for keyword in ethics_keywords:
        if keyword in text:
            detected.append({
                "text": keyword,
                "type": "ë¹„ìœ¤ë¦¬ì  í‘œí˜„",
                "severity": "high" if len(keyword) > 2 else "medium"
            })
    
    ethics_score = min(len(detected) * 25, 100)
    
    recommendations = []
    if ethics_score >= 70:
        recommendations.append({
            "priority": "high",
            "message": "ì‹¬ê°í•œ ë¹„ìœ¤ë¦¬ì  í‘œí˜„ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    elif ethics_score >= 40:
        recommendations.append({
            "priority": "medium",
            "message": "ë¶€ì ì ˆí•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    else:
        recommendations.append({
            "priority": "low",
            "message": "íŠ¹ë³„í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        })
    
    return {
        "ethics_score": ethics_score,
        "detected_expressions": detected,
        "recommendations": recommendations
    }

# ============================================
# ğŸ“Š ëŒ€ì‹œë³´ë“œ í†µê³„ API
# ============================================

@router.get("/dashboard/stats")
async def get_dashboard_stats():
    """ëŒ€ì‹œë³´ë“œìš© ì‹¤ì‹œê°„ í†µê³„"""
    
    return {
        "users": {
            "total": 12345,
            "active": 1234,
            "new_today": 56
        },
        "posts": {
            "total": 45678,
            "today": 234
        },
        "reports": {
            "total": 234,
            "pending": 45
        },
        "system": {
            "uptime": "99.9%",
            "response_time": "120ms",
            "status": "healthy"
        }
    }

# ============================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ============================================

@router.get("/test")
async def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return {
        "status": "success",
        "message": "APIê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@router.get("/test/error")
async def test_error():
    """ì—ëŸ¬ í…ŒìŠ¤íŠ¸"""
    raise HTTPException(status_code=500, detail="í…ŒìŠ¤íŠ¸ìš© ì—ëŸ¬ì…ë‹ˆë‹¤")

# ============================================
# ğŸ›¡ï¸ Ethics ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ API (ì‹¤ì œ êµ¬í˜„)
# ============================================

class EthicsAnalyzeRequest(BaseModel):
    """Ethics ë¶„ì„ ìš”ì²­ ëª¨ë¸"""
    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=1, max_length=1000)
    
    class Config:
        json_schema_extra = {
            "example": {
                "text": "ë„ˆ ì •ë§ ë©ì²­í•˜êµ¬ë‚˜"
            }
        }

class RagSimilarCase(BaseModel):
    sentence: str
    similarity: float
    immoral_score: float
    spam_score: float
    confidence: float
    confirmed: bool
    feedback_type: Optional[str] = None
    created_at: Optional[str] = None


class RagAnalysis(BaseModel):
    enabled: bool
    adjustment_applied: bool
    adjustment_weight: float
    similar_cases_count: int
    max_similarity: float
    adjusted_score: Optional[float] = None
    adjusted_spam_score: Optional[float] = None
    similar_cases: List[RagSimilarCase] = Field(default_factory=list)


class DetailedAnalysis(BaseModel):
    """ìƒì„¸ ë¶„ì„ ì •ë³´"""
    bert_score: Optional[float] = None
    bert_confidence: Optional[float] = None
    llm_score: Optional[float] = None
    llm_confidence: Optional[float] = None
    llm_spam_score: Optional[float] = None
    rule_spam_score: Optional[float] = None
    base_score: Optional[float] = None
    profanity_boost: Optional[float] = None
    weights: dict
    spam_weights: dict
    rag: RagAnalysis

class EthicsAnalyzeResponse(BaseModel):
    """Ethics ë¶„ì„ ì‘ë‹µ ëª¨ë¸"""
    text: str
    score: Optional[float] = Field(None, description="ë¹„ìœ¤ë¦¬ ì ìˆ˜ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    confidence: Optional[float] = Field(None, description="ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    spam: Optional[float] = Field(None, description="ìŠ¤íŒ¸ ì§€ìˆ˜ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    spam_confidence: Optional[float] = Field(None, description="ìŠ¤íŒ¸ ì‹ ë¢°ë„ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    types: List[str] = Field(..., description="ë¶„ì„ ìœ í˜• ëª©ë¡")
    auto_blocked: Optional[bool] = Field(False, description="ì¦‰ì‹œ ì°¨ë‹¨ ì—¬ë¶€")
    detailed: DetailedAnalysis = Field(..., description="ìƒì„¸ ë¶„ì„ ì •ë³´")


def simplify_result(result: dict) -> dict:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  1ìë¦¬)"""
    rag_similar_cases = []
    for case in result.get('rag_similar_cases', []) or []:
        rag_similar_cases.append({
            'sentence': case.get('sentence', ''),
            'similarity': round(case.get('similarity', 0.0), 3),
            'immoral_score': round(case.get('immoral_score', 0.0), 1),
            'spam_score': round(case.get('spam_score', 0.0), 1),
            'confidence': round(case.get('confidence', 0.0), 1),
            'confirmed': bool(case.get('confirmed', False)),
            'feedback_type': case.get('feedback_type'),
            'created_at': case.get('created_at')
        })

    adjustment_applied = bool(result.get('adjustment_applied', False))
    auto_blocked = bool(result.get('auto_blocked', False))
    
    # ì¦‰ì‹œ ì°¨ë‹¨ ì¼€ì´ìŠ¤ëŠ” None ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
    def safe_round(value, digits=1):
        """None-safe rounding"""
        return round(value, digits) if value is not None else None
    
    return {
        'text': result['text'],
        'score': safe_round(result.get('final_score')),
        'confidence': safe_round(result.get('final_confidence')),
        'spam': safe_round(result.get('spam_score')),
        'spam_confidence': safe_round(result.get('spam_confidence')),
        'types': result.get('types', []),
        'auto_blocked': auto_blocked,
        # ìƒì„¸ ì •ë³´ ì¶”ê°€
        'detailed': {
            'bert_score': safe_round(result.get('bert_score')),
            'bert_confidence': safe_round(result.get('bert_confidence')),
            'llm_score': safe_round(result.get('llm_score', 0.0)) if not auto_blocked else None,
            'llm_confidence': safe_round(result.get('llm_confidence', 0.0)) if not auto_blocked else None,
            'llm_spam_score': safe_round(result.get('llm_spam_score', 0.0)) if not auto_blocked else None,
            'rule_spam_score': safe_round(result.get('rule_spam_score')),
            'base_score': safe_round(result.get('base_score')),
            'profanity_boost': safe_round(result.get('profanity_boost')),
            'weights': {
                'bert': round(result.get('weights', {}).get('bert', 0.0), 2),
                'llm': round(result.get('weights', {}).get('llm', 0.0), 2)
            },
            'spam_weights': {
                'llm': 0.6 if result.get('rule_spam_score', 0) < 80 else 0.3,
                'rule': 0.4 if result.get('rule_spam_score', 0) < 80 else 0.7
            },
            'rag': {
                'enabled': bool(result.get('rag_enabled', False)),
                'adjustment_applied': adjustment_applied,
                'adjustment_weight': round(result.get('adjustment_weight', 0.0), 2) if adjustment_applied else 0.0,
                'similar_cases_count': result.get('similar_cases_count', 0),
                'max_similarity': round(result.get('max_similarity', 0.0), 2),
                'adjusted_score': safe_round(result.get('adjusted_immoral_score')) if adjustment_applied and result.get('adjusted_immoral_score') is not None else None,
                'adjusted_spam_score': safe_round(result.get('adjusted_spam_score')) if adjustment_applied and result.get('adjusted_spam_score') is not None else None,
                'similar_cases': rag_similar_cases
            }
        },
        'rag_applied': adjustment_applied
    }


@router.post("/ethics/analyze", response_model=EthicsAnalyzeResponse, tags=["ethics"])
async def ethics_analyze(request_data: EthicsAnalyzeRequest, request: Request):
    """
    í…ìŠ¤íŠ¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
    
    - **text**: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ìµœëŒ€ 1000ì)
    
    Returns:
    - ë¹„ìœ¤ë¦¬ ì ìˆ˜, ì‹ ë¢°ë„, ìŠ¤íŒ¸ ì§€ìˆ˜, ìœ í˜• ì •ë³´ ë“±
    """
    global ethics_analyzer
    
    # ì§€ì—° ë¡œë”©: ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„
    if ethics_analyzer is None:
        try:
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘ (ì¬ì‹œë„)...")
            from ethics.ethics_hybrid_predictor import HybridEthicsAnalyzer
            ethics_analyzer = HybridEthicsAnalyzer()
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. models/ ë””ë ‰í† ë¦¬ì™€ .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    if ethics_analyzer is None:
        raise HTTPException(status_code=503, detail="ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = time.time()
    
    try:
        result = ethics_analyzer.analyze(request_data.text)
        simplified = simplify_result(result)
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = time.time() - start_time
        
        # ë¡œê·¸ ì €ì¥
        try:
            from ethics.ethics_db_logger import db_logger
            log_id = db_logger.log_analysis(
                text=simplified['text'],
                score=simplified['score'],
                confidence=simplified['confidence'],
                spam=simplified['spam'],
                spam_confidence=simplified['spam_confidence'],
                types=simplified['types'],
                ip_address=request.client.host,
                user_agent=request.headers.get('user-agent'),
                response_time=response_time,
                rag_applied=simplified.get('rag_applied', False),
                auto_blocked=result.get('auto_blocked', False)
            )
            
            # RAG ìƒì„¸ ì •ë³´ ì €ì¥ (RAGê°€ ì ìš©ëœ ê²½ìš°)
            if simplified.get('rag_applied', False) and log_id:
                try:
                    rag_info = simplified.get('detailed', {}).get('rag', {})
                    db_logger.log_rag_details(
                        ethics_log_id=log_id,
                        similar_case_count=rag_info.get('similar_cases_count', 0),
                        max_similarity=rag_info.get('max_similarity', 0.0),  # ì´ë¯¸ 0-1 ë²”ìœ„
                        original_immoral_score=simplified.get('detailed', {}).get('base_score', simplified['score']),
                        original_spam_score=result.get('base_spam_score', simplified.get('spam', 0.0)),  # RAG ë³´ì • ì „ ìŠ¤íŒ¸ ì ìˆ˜
                        adjusted_immoral_score=rag_info.get('adjusted_score', simplified['score']),
                        adjusted_spam_score=rag_info.get('adjusted_spam_score', simplified['spam']),
                        adjustment_weight=rag_info.get('adjustment_weight', 0.0),
                        confidence_boost=0.0,  # ë³„ë„ ê³„ì‚° í•„ìš” ì‹œ ì¶”ê°€
                        similar_cases=rag_info.get('similar_cases', []),
                        rag_response_time=response_time
                    )
                except Exception as rag_log_error:
                    print(f"[WARN] RAG ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {rag_log_error}")
        except Exception as log_error:
            print(f"[WARN] ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {log_error}")
        
        return simplified
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/ethics/logs", tags=["ethics"])
async def get_ethics_logs(
    limit: int = Query(100, description="ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜"),
    offset: int = Query(0, description="ì‹œì‘ ìœ„ì¹˜"),
    min_score: Optional[float] = Query(None, description="ìµœì†Œ ì ìˆ˜ í•„í„°"),
    max_score: Optional[float] = Query(None, description="ìµœëŒ€ ì ìˆ˜ í•„í„°"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
):
    """
    Ethics ë¶„ì„ ë¡œê·¸ ì¡°íšŒ
    
    - **limit**: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    - **offset**: ì‹œì‘ ìœ„ì¹˜ (ê¸°ë³¸ê°’: 0)
    - **min_score**: ìµœì†Œ ì ìˆ˜ í•„í„°
    - **max_score**: ìµœëŒ€ ì ìˆ˜ í•„í„°
    - **start_date**: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
    - **end_date**: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
    """
    try:
        from ethics.ethics_db_logger import db_logger
        logs = db_logger.get_logs_with_rag(
            limit=limit,
            offset=offset,
            min_score=min_score,
            max_score=max_score,
            start_date=start_date,
            end_date=end_date
        )
        return {
            "logs": logs,
            "count": len(logs),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/ethics/logs/stats", tags=["ethics"])
async def get_ethics_statistics(days: int = Query(7, description="ì¡°íšŒí•  ì¼ìˆ˜")):
    """
    Ethics í†µê³„ ì •ë³´ ì¡°íšŒ
    
    - **days**: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)
    
    Returns:
    - ì „ì²´ ê±´ìˆ˜, í‰ê·  ì ìˆ˜, ê³ ìœ„í—˜ ê±´ìˆ˜, ìŠ¤íŒ¸ ê±´ìˆ˜, ì¼ë³„ í†µê³„
    """
    try:
        from ethics.ethics_db_logger import db_logger
        stats = db_logger.get_statistics(days=days)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/{log_id}", tags=["ethics"])
async def delete_ethics_log(log_id: int):
    """
    íŠ¹ì • Ethics ë¡œê·¸ ì‚­ì œ
    
    - **log_id**: ì‚­ì œí•  ë¡œê·¸ì˜ ID
    
    Returns:
    - ì‚­ì œ ì„±ê³µ ë©”ì‹œì§€
    """
    try:
        from ethics.ethics_db_logger import db_logger
        success = db_logger.delete_log(log_id)
        if success:
            return {
                "success": True,
                "message": f"ë¡œê·¸ ID {log_id} ì‚­ì œ ì™„ë£Œ"
            }
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/batch/old", tags=["ethics"])
async def delete_old_ethics_logs(days: int = Query(90, description="ë³´ê´€ ê¸°ê°„ (ì¼)")):
    """
    ì˜¤ë˜ëœ Ethics ë¡œê·¸ ì‚­ì œ
    
    - **days**: ë³´ê´€ ê¸°ê°„ (ê¸°ë³¸ê°’: 90ì¼, 0ì´ë©´ ëª¨ë“  ë¡œê·¸ ì‚­ì œ)
    
    Returns:
    - ì‚­ì œëœ ë¡œê·¸ ìˆ˜
    """
    try:
        from ethics.ethics_db_logger import db_logger
        if days == 0:
            # ëª¨ë“  ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_all_logs()
            return {
                "deleted_count": deleted_count,
                "message": f"ëª¨ë“  ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
        else:
            # ì§€ì •ëœ ê¸°ê°„ ì´ì „ ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_old_logs(days=days)
            return {
                "deleted_count": deleted_count,
                "message": f"{days}ì¼ ì´ì „ ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def generate_suggested_action(sentences: List[Dict], priority: str) -> str:
    """
    ë¬¸ì¥ ë‚´ìš© ë¶„ì„ì„ í†µí•œ ì‹¤ìš©ì ì¸ ì¡°ì¹˜ì‚¬í•­ ìƒì„±
    
    Args:
        sentences: ìœ„í—˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
        priority: ìš°ì„ ìˆœìœ„ (HIGH/MEDIUM/LOW)
    
    Returns:
        êµ¬ì²´ì ì¸ ì¡°ì¹˜ì‚¬í•­ ë¬¸ìì—´
    """
    # ëª¨ë“  ë¬¸ì¥ì„ í•©ì³ì„œ í‚¤ì›Œë“œ ë¶„ì„
    all_text = ' '.join([s['sentence'].lower() for s in sentences])
    
    actions = []
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ êµ¬ì²´ì  ì¡°ì¹˜ì‚¬í•­ ì œì•ˆ
    if any(word in all_text for word in ['íƒˆí‡´', 'ê³„ì • ì‚­ì œ', 'ê·¸ë§Œ', 'ë– ë‚ ', 'ì´íƒˆ']):
        actions.append("ğŸ¯ ê³ ê° ìœ ì§€ í”„ë¡œê·¸ë¨ ì œì•ˆ (í• ì¸, ì¿ í°, íŠ¹ë³„ í˜œíƒ)")
    
    if any(word in all_text for word in ['ë¶ˆë§Œ', 'í’ˆì§ˆ', 'ë¬¸ì œ', 'ê°œì„ ', 'ë¶ˆí¸', 'ë‚˜ì˜', 'ì‹«']):
        actions.append("ğŸ“ ê³ ê° ì„œë¹„ìŠ¤íŒ€ ì¦‰ì‹œ ì—°ë½ ë° ë¶ˆë§Œ í•´ì†Œ")
    
    if any(word in all_text for word in ['ë‹¤ë¥¸', 'ê²½ìŸ', 'ì˜®ê¸°', 'ê°ˆì•„íƒˆ', 'ëŒ€ì•ˆ']):
        actions.append("ğŸ“Š ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ë¦¬ ì„œë¹„ìŠ¤ ê°•ì  ì–´í•„")
    
    if any(word in all_text for word in ['ì˜ë¯¸', 'ì´ìœ ', 'í•„ìš”', 'ê°€ì¹˜']):
        actions.append("ğŸ’¡ ì„œë¹„ìŠ¤ ê°€ì¹˜ ì¬ì¸ì‹ ë° í™œìš© ê°€ì´ë“œ ì œê³µ")
    
    if any(word in all_text for word in ['í™œë™', 'ì°¸ì—¬', 'ì‚¬ìš©']):
        actions.append("ğŸ® ì¬ì°¸ì—¬ ìœ ë„ ìº í˜ì¸ (ì´ë²¤íŠ¸, ìƒˆ ê¸°ëŠ¥ ì†Œê°œ)")
    
    # ìš°ì„ ìˆœìœ„ë³„ ê¸°ë³¸ ì¡°ì¹˜ ì¶”ê°€
    if priority == 'HIGH':
        if not actions:
            actions.append("âš ï¸ ì¦‰ì‹œ ê°œì¸ ë§ì¶¤ ëŒ€ì‘ í•„ìš”")
        actions.append("â° 48ì‹œê°„ ë‚´ ì§ì ‘ ì—°ë½ ê¶Œì¥")
    elif priority == 'MEDIUM':
        if not actions:
            actions.append("ğŸ‘€ ëª¨ë‹ˆí„°ë§ ê°•í™” í•„ìš”")
        actions.append("ğŸ“… ì£¼ê°„ í™œë™ ì¶”ì ")
    else:
        if not actions:
            actions.append("ğŸ“‹ ì •ê¸° ëª¨ë‹ˆí„°ë§")
    
    return ' â€¢ '.join(actions)


@router.get("/risk/top", tags=["risk"])
async def get_risk_top_users(limit: int = Query(10, ge=1, le=100, description="ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜")):
    """
    ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
    
    - **limit**: ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 100)
    
    Returns:
    - summary: í†µê³„ ìš”ì•½ ì •ë³´
    - users: ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import get_recent_high_risk, init_db
        from datetime import datetime
        
        # DB ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
        init_db()
        
        # ê³ ìœ„í—˜ ë°ì´í„° ì¡°íšŒ (confirmed=0ì¸ í•­ëª©ë§Œ - ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ê²ƒë“¤)
        risk_data = get_recent_high_risk(limit=limit, only_unconfirmed=True)
        
        if not risk_data:
            return {
                "summary": {
                    "total_users": 0,
                    "high_priority_count": 0,
                    "medium_priority_count": 0,
                    "avg_risk_score": 0.0
                },
                "users": []
            }
        
        # user_id ëª©ë¡ ì¶”ì¶œ (ìˆ«ìë§Œ í•„í„°ë§)
        user_ids = []
        for item in risk_data:
            uid = item['user_id']
            # ìˆ«ì ë˜ëŠ” ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ê²ƒë§Œ ì¶”ê°€
            try:
                if isinstance(uid, int):
                    user_ids.append(uid)
                elif isinstance(uid, str) and uid.isdigit():
                    user_ids.append(int(uid))
            except:
                pass
        
        user_ids = list(set(user_ids))  # ì¤‘ë³µ ì œê±°
        
        # DBì—ì„œ ì‹¤ì œ username ì¡°íšŒ
        from app.database import execute_query
        username_map = {}
        if user_ids:
            # user_idë¡œ username ì¡°íšŒ
            placeholders = ', '.join(['%s'] * len(user_ids))
            users_info = execute_query(
                f"SELECT id, username FROM users WHERE id IN ({placeholders})",
                tuple(user_ids),
                fetch_all=True
            )
            if users_info:
                for user_info in users_info:
                    username_map[user_info['id']] = user_info['username']
                    # ë¬¸ìì—´ ë²„ì „ë„ ë§¤í•‘ (í•˜ìœ„ í˜¸í™˜ì„±)
                    username_map[str(user_info['id'])] = user_info['username']
        
        # ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™”í•˜ë˜, ë¬¸ì¥ë³„ chunk_idë„ í•¨ê»˜ ì €ì¥
        user_dict = {}
        for item in risk_data:
            user_id = item['user_id']
            if user_id not in user_dict:
                # ì‹¤ì œ username ì‚¬ìš©, ì—¬ëŸ¬ í˜•íƒœë¡œ ì‹œë„ (int, str, ë‘˜ ë‹¤)
                username = None
                if isinstance(user_id, int):
                    username = username_map.get(user_id) or username_map.get(str(user_id))
                elif isinstance(user_id, str):
                    username = username_map.get(user_id)
                    if not username and user_id.isdigit():
                        username = username_map.get(int(user_id))
                
                # fallback
                if not username:
                    username = f"ì‚¬ìš©ì_{user_id}"
                
                user_dict[user_id] = {
                    'user_id': user_id,
                    'username': username,
                    'post_id': item.get('post_id', ''),
                    'risk_score': item['risk_score'],
                    'confirmed': bool(item.get('confirmed', 0)),
                    'sentences': [],  # ë¬¸ì¥ë³„ ë°ì´í„° (chunk_id, sentence, score í¬í•¨)
                    'last_activity': item.get('created_at', datetime.now().isoformat()),
                    'feedback_at': item.get('created_at') if item.get('confirmed') else None
                }
            
            # ë¬¸ì¥ë³„ ë°ì´í„° ì¶”ê°€ (chunk_id + ìœ ì‚¬ ì‚¬ë¡€)
            # âš ï¸ ì„±ëŠ¥ ìµœì í™”: ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ì€ ì´ˆê¸° ë¡œë”© ì‹œ ìƒëµ
            # (ê° ë¬¸ì¥ë§ˆë‹¤ OpenAI API í˜¸ì¶œ + ë²¡í„°DB ê²€ìƒ‰ìœ¼ë¡œ ë§¤ìš° ëŠë¦¼)
            # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ "ìœ ì‚¬ ì‚¬ë¡€ ë³´ê¸°" ë²„íŠ¼ í´ë¦­ ì‹œ ê°œë³„ ì¡°íšŒí•˜ë„ë¡ ë³€ê²½
            similar_cases = []
            # ê¸°ì¡´ ì½”ë“œ ì£¼ì„ ì²˜ë¦¬ (ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´)
            # try:
            #     from chrun_backend.rag_pipeline.embedding_service import get_embedding
            #     from chrun_backend.rag_pipeline.vector_db import get_client, search_similar
            #     
            #     embedding = get_embedding(item['sentence'])
            #     client = get_client()
            #     if client:
            #         results = search_similar(
            #             client=client,
            #             embedding=embedding,
            #             top_k=5,
            #             min_score=0.65,
            #             collection_name="confirmed_risk"
            #         )
            #         for result in results:
            #             metadata = result.get('metadata', {})
            #             similar_cases.append({
            #                 'sentence': result.get('document', ''),
            #                 'confirmed': metadata.get('confirmed', False),
            #                 'similarity': round(result.get('score', 0.0) * 100, 0),
            #                 'risk_score': metadata.get('risk_score', 0.0)
            #             })
            # except Exception:
            #     pass  # ì¡°ìš©íˆ ì‹¤íŒ¨
            
            # â­ ì‹ ë¢°ë„ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            confidence_score = 0.5  # ê¸°ë³¸ê°’
            confidence_level = "medium"
            
            if len(similar_cases) >= 3:
                avg_similarity = sum(c.get('similarity', 0) for c in similar_cases) / len(similar_cases)
                if avg_similarity >= 70:
                    confidence_score = 0.8
                    confidence_level = "high"
                elif avg_similarity >= 50:
                    confidence_score = 0.65
                    confidence_level = "medium"
            elif len(similar_cases) >= 1:
                confidence_score = 0.6
                confidence_level = "medium"
            else:
                confidence_score = 0.4
                confidence_level = "low"
            
            user_dict[user_id]['sentences'].append({
                'chunk_id': item['chunk_id'],
                'sentence': item['sentence'],
                'risk_score': item['risk_score'],
                'post_id': item.get('post_id', ''),  # â­ ê° ë¬¸ì¥ë³„ post_id ì¶”ê°€
                'similar_cases': similar_cases,  # â­ ìœ ì‚¬ ì‚¬ë¡€ ì¶”ê°€
                'similar_cases_count': len(similar_cases),
                'confidence': confidence_score,  # â­ ì‹ ë¢°ë„ ì ìˆ˜
                'confidence_level': confidence_level  # â­ ì‹ ë¢°ë„ ë ˆë²¨
            })
            
            # ê°€ì¥ ë†’ì€ risk_score ì‚¬ìš© (ì¹´ë“œ ì •ë ¬ìš©)
            if item['risk_score'] > user_dict[user_id]['risk_score']:
                user_dict[user_id]['risk_score'] = item['risk_score']
        
        # ì‚¬ìš©ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        users = []
        for user_data in user_dict.values():
            # Priority ê²°ì • (risk_score >= 0.7: HIGH, >= 0.5: MEDIUM, ê·¸ ì™¸: LOW)
            if user_data['risk_score'] >= 0.7:
                priority = 'HIGH'
            elif user_data['risk_score'] >= 0.5:
                priority = 'MEDIUM'
            else:
                priority = 'LOW'
            
            # ì œì•ˆ ì¡°ì¹˜ì‚¬í•­ ìƒì„± (í‚¤ì›Œë“œ ê¸°ë°˜ ì‹¤ìš©ì  ì¡°ì–¸)
            suggested_action = generate_suggested_action(user_data['sentences'], priority)
            
            users.append({
                **user_data,
                'priority': priority,
                'similar_patterns_count': len(user_data['sentences']),
                'suggested_action': suggested_action
            })
        
        # risk_score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        users.sort(key=lambda x: x['risk_score'], reverse=True)
        
        # í†µê³„ ê³„ì‚°
        high_priority_count = sum(1 for u in users if u['priority'] == 'HIGH')
        medium_priority_count = sum(1 for u in users if u['priority'] == 'MEDIUM')
        avg_risk_score = sum(u['risk_score'] for u in users) / len(users) if users else 0.0
        
        return {
            "summary": {
                "total_users": len(users),
                "high_priority_count": high_priority_count,
                "medium_priority_count": medium_priority_count,
                "avg_risk_score": round(avg_risk_score, 2)
            },
            "users": users
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ê³ ìœ„í—˜ ì‚¬ìš©ì ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


class RiskFeedbackBase(BaseModel):
    chunk_id: str
    sentence: str
    pred_score: float
    final_label: str


class RiskFeedbackRequest(RiskFeedbackBase):
    """ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ìš”ì²­"""
    confirmed: bool


class CheckNewPostRequest(BaseModel):
    """ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ìš”ì²­"""
    text: str
    user_id: str
    post_id: str
    created_at: str


def _build_safe_risk_response(
    request_data: CheckNewPostRequest,
    error: Optional[str] = None
) -> Dict[str, Any]:
    """ì—ëŸ¬ ìƒí™©ì—ì„œ ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    from chrun_backend.rag_pipeline.rag_checker import _create_safe_decision

    decision = _create_safe_decision()
    decision["confidence"] = "Uncertain"

    response: Dict[str, Any] = {
        "post": {
            "user_id": request_data.user_id,
            "post_id": request_data.post_id,
            "created_at": request_data.created_at,
            "original_text": request_data.text,
        },
        "decision": decision,
        "evidence": [],
    }

    if error:
        response["error"] = error

    return response


def _ensure_risk_response_schema(
    result: Dict[str, Any],
    request_data: CheckNewPostRequest
) -> Dict[str, Any]:
    """ì‘ë‹µ ê°ì²´ê°€ í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ(post/decision/evidence)ë¥¼ ë§Œì¡±í•˜ë„ë¡ ë³´ì •í•©ë‹ˆë‹¤."""
    if not isinstance(result, dict):
        logger.warning("[RISK] check_new_post ê²°ê³¼ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤. ì•ˆì „ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return _build_safe_risk_response(request_data, error="Invalid response type")

    post_payload = result.get("post") or {}
    decision_payload = result.get("decision") or {}
    evidence_payload = result.get("evidence") or []

    if not isinstance(evidence_payload, list):
        logger.warning("[RISK] evidenceê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        evidence_payload = []

    post_data = {
        "user_id": post_payload.get("user_id") or request_data.user_id,
        "post_id": post_payload.get("post_id") or request_data.post_id,
        "created_at": post_payload.get("created_at") or request_data.created_at,
        "original_text": post_payload.get("original_text") or request_data.text,
    }

    # â­ Evidenceê°€ ì—†ì–´ë„ LLM ê²°ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš© (EvidenceëŠ” ì°¸ê³  ìë£Œì¼ ë¿)
    if not isinstance(decision_payload, dict):
        logger.warning("[RISK] decisionì´ dictê°€ ì•„ë‹™ë‹ˆë‹¤. ì•ˆì „ ê²°ì •ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        decision_payload = {}
    
    # LLMì´ ì •ìƒ ë¶„ì„í–ˆëŠ”ì§€ í™•ì¸ (risk_scoreê°€ ìˆê³  ê¸°ë³¸ê°’ ì•„ë‹˜)
    has_valid_llm_decision = (
        decision_payload.get("risk_score") is not None and 
        decision_payload.get("priority") and
        decision_payload.get("reasons") and
        # ê¸°ë³¸ fallback ë©”ì‹œì§€ê°€ ì•„ë‹Œì§€ í™•ì¸
        "ìœ ì‚¬í•œ ìœ„í—˜ ë¬¸ì¥ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ" not in str(decision_payload.get("reasons", []))
    )
    
    # Evidence ì—†ê³  LLM ê²°ì •ë„ ì—†ìœ¼ë©´ safe_response ì‚¬ìš©
    if not evidence_payload and not has_valid_llm_decision:
        logger.warning("[RISK] Evidenceì™€ ìœ íš¨í•œ LLM ê²°ì •ì´ ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „ ì‘ë‹µ ë°˜í™˜")
        safe_response = _build_safe_risk_response(request_data)
        if "fallback_reason" in decision_payload:
            safe_response["decision"]["fallback_reason"] = decision_payload["fallback_reason"]
        return safe_response

    # Evidence ì—†ì–´ë„ LLM ê²°ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if not evidence_payload:
        logger.info("[RISK] Evidence ì—†ìŒ. LLMì´ ì›ë¬¸ë§Œìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ ì‚¬ìš©")
    
    decision_payload.setdefault("confidence", "Uncertain" if not evidence_payload else "Low")

    return {
        "post": post_data,
        "decision": decision_payload,
        "evidence": evidence_payload,
    }


@router.post("/risk/feedback", tags=["risk"])
async def submit_risk_feedback(request_data: RiskFeedbackRequest):
    """
    ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ
    
    - **chunk_id**: í”¼ë“œë°±í•  chunk_id
    - **confirmed**: ìœ„í—˜ í™•ì¸ ì—¬ë¶€ (true: ìœ„í—˜ ë§ìŒ, false: ìœ„í—˜ ì•„ë‹˜)
    
    Returns:
    - ì„±ê³µ ë©”ì‹œì§€
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import update_feedback, get_chunk_by_id, log_feedback_event
        
        sentence = request_data.sentence.strip() if request_data.sentence else ""
        if not sentence:
            raise HTTPException(status_code=422, detail="sentence í•„ë“œëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        try:
            pred_score = float(request_data.pred_score)
        except (TypeError, ValueError):
            raise HTTPException(status_code=422, detail="pred_scoreëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")

        final_label = request_data.final_label.strip().upper()
        if final_label not in {"MATCH", "MISMATCH", "UPDATE"}:
            raise HTTPException(status_code=422, detail="final_labelì€ MATCH/MISMATCH/UPDATE ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        # 1. ê¸°ì¡´ SQLite í”¼ë“œë°± ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        update_feedback(request_data.chunk_id, request_data.confirmed)
        chunk_snapshot: Optional[Dict[str, Any]] = None
        
        # 2. confirmed=trueì¸ ê²½ìš°ì—ë§Œ ë²¡í„°DBì— ì €ì¥
        if request_data.confirmed:
            try:
                # 2-1. SQLiteì—ì„œ í•´ë‹¹ chunk ì •ë³´ ì¡°íšŒ
                chunk_data = get_chunk_by_id(request_data.chunk_id)
                chunk_snapshot = chunk_data
                
                if not chunk_data:
                    # chunkë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: chunk_id {request_data.chunk_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                else:
                    # 2-2. ì„ë² ë”© ìƒì„±
                    from chrun_backend.rag_pipeline.embedding_service import get_embedding
                    sentence = chunk_data.get('sentence', '')
                    
                    if sentence.strip():
                        embedding = get_embedding(sentence)
                        
                        # 2-3. ë²¡í„°DBì— ì €ì¥í•  ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        from chrun_backend.rag_pipeline.vector_db import build_chunk_id
                        
                        # ì•ˆì •ì ì¸ chunk_id ìƒì„± (ê¸°ì¡´ chunk_idì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                        vector_chunk_id = build_chunk_id(sentence, chunk_data.get('post_id', ''))
                        
                        meta = {
                            "chunk_id": vector_chunk_id,  # ë²¡í„°DBìš© ì•ˆì •ì  ID
                            "original_chunk_id": chunk_data.get('chunk_id'),  # ì›ë³¸ SQLite chunk_id
                            "user_id": chunk_data.get('user_id', ''),
                            "post_id": chunk_data.get('post_id', ''),
                            "sentence": sentence,
                            "risk_score": float(chunk_data.get('risk_score', 0.0)),
                            "created_at": chunk_data.get('created_at', ''),
                            "confirmed": True
                        }
                        
                        # 2-4. ë²¡í„°DBì— upsert (idempotent)
                        from chrun_backend.rag_pipeline.vector_db import get_client, upsert_confirmed_chunk
                        
                        client = get_client()  # ê¸°ë³¸ ê²½ë¡œ "./chroma_store" ì‚¬ìš©
                        upsert_confirmed_chunk(client, embedding, meta)
                        
                        pass  # print(f"[INFO] í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„°DBì— ì €ì¥ ì™„ë£Œ: {vector_chunk_id}")  # ë¹ˆë²ˆí•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
                    else:
                        pass  # print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: ë¹ˆ ë¬¸ì¥ (chunk_id: {request_data.chunk_id})")  # ë¹ˆë²ˆí•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
                        
            except Exception as vector_error:
                # ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                import traceback
                print(f"[ERROR] ë²¡í„°DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {vector_error}")
                traceback.print_exc()
                # ì—ëŸ¬ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  APIëŠ” ì„±ê³µìœ¼ë¡œ ì‘ë‹µ
        
        if chunk_snapshot is None:
            chunk_snapshot = get_chunk_by_id(request_data.chunk_id)
        user_id_for_hash = chunk_snapshot.get('user_id') if chunk_snapshot else None

        event_id = log_feedback_event(
            chunk_id=request_data.chunk_id,
            sentence=sentence[:500],
            pred_score=max(0.0, min(1.0, pred_score)),
            final_label=final_label,
            confirmed=request_data.confirmed,
            user_id=user_id_for_hash
        )

        return {
            "status": "ok",
            "feedback_id": event_id,
            "chunk_id": request_data.chunk_id,
            "final_label": final_label,
            "pred_score": round(max(0.0, min(1.0, pred_score)), 3),
            "confirmed": request_data.confirmed
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/risk/similar-cases", tags=["risk"])
async def get_similar_cases(sentence: str = Query(..., description="ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•  ë¬¸ì¥")):
    """
    íŠ¹ì • ë¬¸ì¥ì— ëŒ€í•œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ (ì˜¨ë””ë§¨ë“œ ì¡°íšŒ)
    
    - **sentence**: ìœ ì‚¬ ì‚¬ë¡€ë¥¼ ê²€ìƒ‰í•  ë¬¸ì¥
    
    Returns:
    - similar_cases: ìœ ì‚¬í•œ í™•ì • ì‚¬ë¡€ ëª©ë¡
    """
    try:
        from chrun_backend.rag_pipeline.embedding_service import get_embedding
        from chrun_backend.rag_pipeline.vector_db import get_client, search_similar
        
        if not sentence or not sentence.strip():
            raise HTTPException(status_code=422, detail="ë¬¸ì¥ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        similar_cases = []
        
        # ì„ë² ë”© ìƒì„± ë° ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰
        embedding = get_embedding(sentence.strip())
        client = get_client()
        
        if client:
            results = search_similar(
                client=client,
                embedding=embedding,
                top_k=5,
                min_score=0.65,
                collection_name="confirmed_risk"
            )
            
            for result in results:
                metadata = result.get('metadata', {})
                similar_cases.append({
                    'sentence': result.get('document', ''),
                    'confirmed': metadata.get('confirmed', False),
                    'similarity': round(result.get('score', 0.0) * 100, 0),
                    'risk_score': metadata.get('risk_score', 0.0)
                })
        
        return {
            "status": "ok",
            "sentence": sentence,
            "similar_cases": similar_cases,
            "count": len(similar_cases)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/risk/feedback", tags=["risk"])
async def list_risk_feedback(limit: int = Query(50, ge=1, le=200)):
    """
    í”¼ë“œë°± ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import get_feedback_events

        events = get_feedback_events(limit=limit)
        return {
            "items": events,
            "count": len(events)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("[RISK] í”¼ë“œë°± ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")


@router.delete("/risk/all", tags=["risk"])
async def delete_all_risk_data():
    """
    ëª¨ë“  ê³ ìœ„í—˜ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    
    **ì£¼ì˜**: ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!
    
    Returns:
    - deleted_count: ì‚­ì œëœ ë ˆì½”ë“œ ìˆ˜
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import delete_all_risk_data
        
        deleted = delete_all_risk_data()
        
        return {
            "status": "ok",
            "message": f"{deleted}ê°œì˜ ê³ ìœ„í—˜ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": deleted
        }
    except Exception as e:
        logger.exception("[RISK] ëª¨ë“  ê³ ìœ„í—˜ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.post("/risk/check_new_post", tags=["risk"])
async def check_new_post_risk(request_data: CheckNewPostRequest):
    """
    ìƒˆ ê²Œì‹œë¬¼ì˜ ìœ„í—˜ë„ë¥¼ ì²´í¬í•˜ì—¬ ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    - **text**: ë¶„ì„í•  ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸
    - **user_id**: ì‚¬ìš©ì ID
    - **post_id**: ê²Œì‹œë¬¼ ID
    - **created_at**: ìƒì„± ì‹œê°„ (ISO í˜•ì‹, ì˜ˆ: "2024-11-04T10:30:00")
    
    Returns:
    - ìœ„í—˜ë„ ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ (ê·¼ê±° ë¬¸ì¥ë“¤ê³¼ í†µê³„ ì •ë³´)
    """
    try:
        from chrun_backend.rag_pipeline.rag_checker import check_new_post

        if not os.getenv("OPENAI_API_KEY"):
            logger.warning("[RISK] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²°ì •ì´ ë°˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        context = check_new_post(
            text=request_data.text,
            user_id=request_data.user_id,
            post_id=request_data.post_id,
            created_at=request_data.created_at
        )

        return _ensure_risk_response_schema(context, request_data)

    except Exception as e:
        logger.exception("[RISK] ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return _build_safe_risk_response(request_data, error=str(e))


# ============================================================
# í™•ì • ì‚¬ë¡€ ê´€ë¦¬ API
# ============================================================

@router.get("/risk/confirmed-cases", tags=["risk"])
async def get_confirmed_cases(
    confirmed: Optional[str] = Query(None, description="í•„í„°: 'true', 'false', ë˜ëŠ” null(ì „ì²´)"),
    sort: str = Query("date", description="ì •ë ¬: 'date'(ë‚ ì§œìˆœ) ë˜ëŠ” 'score'(ìœ„í—˜ë„ìˆœ)"),
    search: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´ (ë¬¸ì¥ ë‚´ìš©)"),
    limit: int = Query(100, ge=1, le=500, description="ìµœëŒ€ ì¡°íšŒ ê±´ìˆ˜")
):
    """
    í™•ì •ëœ ì‚¬ë¡€ ëª©ë¡ ì¡°íšŒ
    - ê´€ë¦¬ìê°€ 'ìœ„í—˜ ë§ìŒ' ë˜ëŠ” 'ìœ„í—˜ ì•„ë‹˜'ìœ¼ë¡œ í™•ì •í•œ ì‚¬ë¡€ë“¤
    """
    try:
        # 1. ê¸°ë³¸ ì¿¼ë¦¬
        base_query = """
            SELECT 
                chunk_id,
                user_id,
                post_id,
                sentence,
                risk_score,
                confirmed,
                confirmed_at,
                created_at
            FROM high_risk_chunks
            WHERE confirmed IS NOT NULL
        """
        
        params = []
        
        # 2. í™•ì • ìœ í˜• í•„í„°
        if confirmed is not None:
            if confirmed.lower() == 'true':
                base_query += " AND confirmed = 1"
            elif confirmed.lower() == 'false':
                base_query += " AND confirmed = 0"
        
        # 3. ê²€ìƒ‰ì–´ í•„í„°
        if search and search.strip():
            base_query += " AND sentence LIKE %s"
            params.append(f"%{search.strip()}%")
        
        # 4. ì •ë ¬
        if sort == "score":
            base_query += " ORDER BY risk_score DESC, confirmed_at DESC"
        else:  # date
            base_query += " ORDER BY confirmed_at DESC"
        
        # 5. ì œí•œ
        base_query += f" LIMIT {limit}"
        
        # 6. ì‹¤í–‰
        from app.database import execute_query
        results = execute_query(base_query, params=params if params else None, fetch_all=True)
        
        if not results:
            return {
                "total": 0,
                "cases": []
            }
        
        # 7. ê²°ê³¼ í¬ë§·íŒ…
        cases = []
        for row in results:
            cases.append({
                "chunk_id": row.get('chunk_id'),
                "user_id": row.get('user_id'),
                "post_id": row.get('post_id'),
                "sentence": row.get('sentence'),
                "risk_score": round(row.get('risk_score', 0.0), 2),
                "confirmed": bool(row.get('confirmed')),
                "confirmed_at": row.get('confirmed_at'),
                "created_at": row.get('created_at')
            })
        
        return {
            "total": len(cases),
            "cases": cases
        }
        
    except Exception as e:
        logger.exception("[RISK] í™•ì • ì‚¬ë¡€ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/risk/confirmed-stats", tags=["risk"])
async def get_confirmed_stats():
    """
    í™•ì • ì‚¬ë¡€ í†µê³„ ì¡°íšŒ
    """
    try:
        from app.database import execute_query
        
        # 1. ì „ì²´ í†µê³„
        stats_query = """
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN confirmed = 1 THEN 1 ELSE 0 END) as confirmed_true,
                SUM(CASE WHEN confirmed = 0 THEN 1 ELSE 0 END) as confirmed_false,
                MAX(confirmed_at) as last_confirmed
            FROM high_risk_chunks
            WHERE confirmed IS NOT NULL
        """
        
        result = execute_query(stats_query, fetch_one=True)
        
        if not result:
            return {
                "total": 0,
                "confirmed_true": 0,
                "confirmed_false": 0,
                "last_confirmed": None,
                "vectordb_synced": 0
            }
        
        # 2. ë²¡í„°DB ë™ê¸°í™” ìƒíƒœ (confirmed=1ì¸ ê²ƒë§Œ ë²¡í„°DBì— ìˆìŒ)
        vectordb_count = 0
        try:
            from chrun_backend.rag_pipeline.vector_db import get_client
            client = get_client()
            if client:
                collection = client.get_collection(name="confirmed_risk")
                vectordb_count = collection.count()
        except Exception:
            pass
        
        return {
            "total": result.get('total', 0),
            "confirmed_true": result.get('confirmed_true', 0),
            "confirmed_false": result.get('confirmed_false', 0),
            "last_confirmed": result.get('last_confirmed'),
            "vectordb_synced": vectordb_count
        }
        
    except Exception as e:
        logger.exception("[RISK] í™•ì • ì‚¬ë¡€ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.delete("/risk/confirmed-cases/{chunk_id}", tags=["risk"])
async def delete_confirmed_case(chunk_id: str):
    """
    í™•ì • ì‚¬ë¡€ ì‚­ì œ (MySQL + VectorDB ë™ì‹œ ì‚­ì œ)
    """
    try:
        from app.database import execute_query
        
        # 1. MySQLì—ì„œ í™•ì • ì •ë³´ ì¡°íšŒ
        check_query = "SELECT sentence, confirmed FROM high_risk_chunks WHERE chunk_id = %s"
        case = execute_query(check_query, params=(chunk_id,), fetch_one=True)
        
        if not case:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‚¬ë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        was_confirmed = case.get('confirmed')
        
        # 2. MySQLì—ì„œ confirmed ì •ë³´ë§Œ ì´ˆê¸°í™” (ë ˆì½”ë“œëŠ” ìœ ì§€)
        update_query = """
            UPDATE high_risk_chunks 
            SET confirmed = NULL, confirmed_at = NULL 
            WHERE chunk_id = %s
        """
        execute_query(update_query, params=(chunk_id,))
        
        # 3. ë²¡í„°DBì—ì„œë„ ì‚­ì œ (confirmed=1ì´ì—ˆë˜ ê²½ìš°ë§Œ)
        vectordb_deleted = False
        if was_confirmed == 1:
            try:
                from chrun_backend.rag_pipeline.vector_db import get_client
                client = get_client()
                if client:
                    collection = client.get_collection(name="confirmed_risk")
                    collection.delete(ids=[chunk_id])
                    vectordb_deleted = True
            except Exception as e:
                logger.warning(f"[RISK] ë²¡í„°DB ì‚­ì œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        
        return {
            "success": True,
            "chunk_id": chunk_id,
            "mysql_updated": True,
            "vectordb_deleted": vectordb_deleted
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("[RISK] í™•ì • ì‚¬ë¡€ ì‚­ì œ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
