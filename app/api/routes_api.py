"""
ğŸ¯ Mock API ì—”ë“œí¬ì¸íŠ¸
ì‹œë‹ˆì–´ì˜ ì„¤ëª…:
- ì‹¤ì œ ë°±ì—”ë“œ ì™„ì„± ì „ê¹Œì§€ ì‚¬ìš©í•  ê°€ì§œ ë°ì´í„°
- í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ì‹œ ìœ ìš©
- ë‚˜ì¤‘ì— ì‹¤ì œ DBë¡œ êµì²´
"""

from fastapi import APIRouter, Query, HTTPException, Request
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime, timedelta
from collections import Counter
import random
import time
import httpx

router = APIRouter(tags=["api"])

# Ethics Analyzer ì „ì—­ ë³€ìˆ˜ (main.pyì—ì„œ ì´ˆê¸°í™”ë¨)
ethics_analyzer = None

# ============================================
# ğŸ“Š ë°ì´í„° ëª¨ë¸ (Pydantic)
# ============================================

class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    id: int
    title: str
    content: str
    author: str
    date: str
    category: str

class BounceMetrics(BaseModel):
    """ì´íƒˆë¥  ë©”íŠ¸ë¦­"""
    avg_bounce_rate: float
    total_visitors: int
    bounced_visitors: int
    period: str

class TrendItem(BaseModel):
    """íŠ¸ë Œë“œ ì•„ì´í…œ"""
    keyword: str
    mentions: int
    change: float
    category: str

class ReportCategory(BaseModel):
    """ì‹ ê³  ì¹´í…Œê³ ë¦¬"""
    name: str
    count: int
    status: str
    avg_processing_time: str

class EthicsScoreRequest(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ìš”ì²­"""
    text: str

class EthicsScoreResponse(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ì‘ë‹µ"""
    ethics_score: float
    detected_expressions: List[dict]
    recommendations: List[dict]

# ============================================
# ğŸ” ê²€ìƒ‰ API
# ============================================

@router.get("/search")
async def search(q: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ")):
    """
    ìì—°ì–´ ê²€ìƒ‰ API
    
    **ì‹œë‹ˆì–´ì˜ íŒ:**
    - Query(...) : í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    - Query(None) : ì„ íƒì  íŒŒë¼ë¯¸í„°
    """
    
    if not q:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # Mock ë°ì´í„° ìƒì„±
    results = [
        {
            "id": i,
            "title": f"{q}ì— ê´€í•œ ê²Œì‹œê¸€ {i+1}",
            "content": f"ì´ê²ƒì€ '{q}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìƒ˜í”Œ ê²Œì‹œê¸€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë©ë‹ˆë‹¤.",
            "author": f"ì‚¬ìš©ì{random.randint(1, 100)}",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "category": random.choice(["ììœ ê²Œì‹œíŒ", "ì§ˆë¬¸", "ì •ë³´", "í† ë¡ "])
        }
        for i in range(5)
    ]
    
    return {
        "query": q,
        "total": len(results),
        "results": results
    }

# ============================================
# ğŸ“Š ì´íƒˆë¥  ë©”íŠ¸ë¦­ API
# ============================================

@router.get("/metrics/bounce")
async def get_bounce_metrics():
    """
    ë°©ë¬¸ê° ì´íƒˆë¥  ë°ì´í„°
    
    **Mock ë°ì´í„°:**
    ì‹¤ì œë¡œëŠ” Google Analyticsë‚˜ ìì²´ ë¶„ì„ ì‹œìŠ¤í…œì—ì„œ ê°€ì ¸ì˜´
    """
    
    return {
        "metrics": {
            "avg_bounce_rate": 42.5,
            "total_visitors": 15234,
            "bounced_visitors": 6474,
            "period": "2025-01-01 ~ 2025-01-31"
        },
        "details": [
            {
                "date": f"2025-01-{i+1:02d}",
                "visitors": random.randint(300, 800),
                "bounced": random.randint(100, 400),
                "bounce_rate": random.uniform(30, 60)
            }
            for i in range(7)
        ]
    }

# ============================================
# ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ API (ì‹¤ì œ ë°ì´í„°)
# ============================================

@router.get("/trends")
async def get_trends(limit: int = Query(100, ge=1, le=1000)):
    """
    ì‹¤ì œ íŠ¸ë Œë“œ ë°ì´í„° ë°˜í™˜ (dad.dothome.co.kr API ì—°ë™)
    
    **ì‹œë‹ˆì–´ì˜ ì„¤ëª…:**
    - ì™¸ë¶€ APIì—ì„œ ì‹¤ì œ ì¸ê¸° ê²€ìƒ‰ì–´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    - í‚¤ì›Œë“œ ì •ê·œí™” (ê²€ìƒ‰í–ˆìŒâ†’ê²€ìƒ‰, ê²€ìƒ‰ì–´â†’ê²€ìƒ‰)
    - ë‚ ì§œë³„ íƒ€ì„ë¼ì¸ ìƒì„±
    - ì‹¤ì œ ì¦ê°ë¥  ê³„ì‚°
    """
    
    # â­ í‚¤ì›Œë“œ ì •ê·œí™” ë§¤í•‘ (ìì—°ì–´ â†’ í‚¤ì›Œë“œ)
    KEYWORD_NORMALIZATION = {
        "ê²€ìƒ‰í–ˆìŒ": "ê²€ìƒ‰",
        "ê²€ìƒ‰í•˜ê¸°": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì¤‘": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì–´": "ê²€ìƒ‰",
        "ê²€ìƒ‰ì–´ë“¤": "ê²€ìƒ‰",
        "ì•ˆë…•í•˜ì„¸ìš”": "ì¸ì‚¬",
        "ì•ˆë…•": "ì¸ì‚¬",
        "ã…ã…‡": "ì¸ì‚¬",
    }
    
    def normalize_keyword(word: str) -> str:
        """í‚¤ì›Œë“œ ì •ê·œí™”"""
        word = word.strip()
        return KEYWORD_NORMALIZATION.get(word, word)
    
    try:
        print(f"\n[DEBUG] Calling dad.dothome.co.kr API with limit={limit}")
        async with httpx.AsyncClient(timeout=10.0) as client:
            # âœ… 1. ì‹¤ì œ ì¸ê¸° ê²€ìƒ‰ì–´ API í˜¸ì¶œ (standalone ë²„ì „)
            url = "https://dad.dothome.co.kr/adm/popular_api_standalone.php"
            print(f"[DEBUG] URL: {url}")
            
            response = await client.get(url, params={"limit": limit})
            print(f"[DEBUG] Response status: {response.status_code}")
            print(f"[DEBUG] Response content-type: {response.headers.get('content-type')}")
            
            response.raise_for_status()
            
            data = response.json()
            print(f"[DEBUG] JSON parsed successfully")
            print(f"[DEBUG] success={data.get('success')}, data_count={len(data.get('data', []))}")
            
            if not data.get("success", False):
                raise Exception("API returned error")
            
            # âœ… 2. ê²Œì‹œê¸€/ëŒ“ê¸€ í†µê³„ API í˜¸ì¶œ
            stats_url = "https://dad.dothome.co.kr/adm/board_stats_api.php"
            print(f"[DEBUG] Fetching board stats from: {stats_url}")
            
            stats_response = await client.get(stats_url)
            stats_data = stats_response.json()
            
            total_posts = 0
            total_comments = 0
            
            if stats_data.get("success"):
                total_posts = stats_data["data"]["total_posts"]
                total_comments = stats_data["data"]["total_comments"]
                print(f"[DEBUG] Board stats: posts={total_posts}, comments={total_comments}")
                
                # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥
                if "debug" in stats_data:
                    print(f"[DEBUG] Tables found: {stats_data['debug'].get('tables_found', [])}")
            else:
                print(f"[DEBUG] Board stats API failed, using defaults")
            
            # ë°ì´í„° ë³€í™˜
            api_data = data.get("data", [])
            print(f"[DEBUG] Converting {len(api_data)} items to keywords")
            
            # â­ í‚¤ì›Œë“œ ì •ê·œí™” + ë¹ˆë„ ì§‘ê³„
            word_counts = Counter()
            date_word_counts = {}  # ë‚ ì§œë³„ í‚¤ì›Œë“œ ë¹ˆë„
            
            for item in api_data:
                word = item.get("word", "").strip()
                date = item.get("date", "")
                
                if word:
                    # í‚¤ì›Œë“œ ì •ê·œí™”
                    normalized_word = normalize_keyword(word)
                    word_counts[normalized_word] += 1
                    
                    # ë‚ ì§œë³„ ì§‘ê³„
                    if date not in date_word_counts:
                        date_word_counts[date] = Counter()
                    date_word_counts[date][normalized_word] += 1
            
            # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í‚¤ì›Œë“œ ìƒì„±
            keywords = [
                {
                    "word": word,
                    "count": count
                }
                for word, count in word_counts.most_common()
            ]
            
            print(f"[DEBUG] Top 5 keywords (after normalization): {keywords[:5]}")
            
            # â­ ì¦ê°ë¥  ê³„ì‚° (ë‚ ì§œë³„ ë¹„êµ)
            dates = sorted(date_word_counts.keys())
            trends = []
            
            for kw in keywords[:10]:
                word = kw["word"]
                
                # ìµœê·¼ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œì˜ ê²€ìƒ‰ íšŸìˆ˜ ë¹„êµ
                if len(dates) >= 2:
                    recent_count = date_word_counts[dates[-1]].get(word, 0)
                    previous_count = date_word_counts[dates[-2]].get(word, 0)
                    
                    if previous_count > 0:
                        change = ((recent_count - previous_count) / previous_count) * 100
                    else:
                        change = 100.0 if recent_count > 0 else 0.0
                else:
                    change = 0.0
                
                # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
                if change > 50:
                    category = "ê¸‰ìƒìŠ¹"
                elif change > 0:
                    category = "ìƒìŠ¹"
                elif change < -50:
                    category = "ê¸‰ê°"
                elif change < 0:
                    category = "í•˜ë½"
                else:
                    category = "ìœ ì§€"
                
                trends.append({
                    "keyword": word,
                    "mentions": kw["count"],
                    "change": round(change, 1),
                    "category": category
                })
            
            # â­ íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„± (ë‚ ì§œë³„ ê²€ìƒ‰ íšŸìˆ˜)
            timeline = []
            for date in sorted(dates):
                total_count = sum(date_word_counts[date].values())
                timeline.append({
                    "date": date,
                    "count": total_count
                })
            
            # â­ ì‹¤ì œ í†µê³„ ê³„ì‚°
            total_searches = sum(word_counts.values())
            unique_keywords = len(keywords)
            
            return {
                "summary": {
                    "total_posts": total_posts,             # â­ ì‹¤ì œ ê²Œì‹œê¸€ ìˆ˜
                    "total_comments": total_comments,        # â­ ì‹¤ì œ ëŒ“ê¸€ ìˆ˜
                    "total_searches": total_searches,        # ì´ ê²€ìƒ‰ íšŸìˆ˜
                    "unique_keywords": unique_keywords,      # ê³ ìœ  í‚¤ì›Œë“œ ìˆ˜
                    "total_trends": len(keywords),
                    "new_trends": len([t for t in trends if t["change"] > 50]),
                    "rising_trends": len([t for t in trends if t["change"] > 0])
                },
                "keywords": keywords,
                "trends": trends,
                "timeline": timeline,  # â­ íƒ€ì„ë¼ì¸ ë°ì´í„° ì¶”ê°€!
                "source": "dad.dothome.co.kr",
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        # â­ ì—ëŸ¬ ë°œìƒ ì‹œ í˜„ì‹¤ì ì¸ Mock ë°ì´í„° ë°˜í™˜ (ë¡œê·¸ì¸ ë¬¸ì œ ëŒ€ì‘)
        print(f"[INFO] Using mock data due to: {e}")
        
        # í˜„ì‹¤ì ì¸ í•œêµ­ì–´ í‚¤ì›Œë“œ Mock ë°ì´í„°
        mock_keywords_pool = [
            "ì¸ê³µì§€ëŠ¥", "ChatGPT", "ë¸”ë¡ì²´ì¸", "ë©”íƒ€ë²„ìŠ¤", "NFT",
            "ë¹…ë°ì´í„°", "í´ë¼ìš°ë“œ", "ì‚¬ì´ë²„ë³´ì•ˆ", "ë”¥ëŸ¬ë‹", "ë¨¸ì‹ ëŸ¬ë‹",
            "ììœ¨ì£¼í–‰", "ì „ê¸°ì°¨", "í…ŒìŠ¬ë¼", "ì‚¼ì„±ì „ì", "ë°˜ë„ì²´",
            "K-POP", "BTS", "ì¶•êµ¬", "ì•¼êµ¬", "ë°°êµ¬",
            "ì£¼ì‹", "ë¹„íŠ¸ì½”ì¸", "ë¶€ë™ì‚°", "ê¸ˆë¦¬", "í™˜ìœ¨",
            "ë‚ ì”¨", "ë¯¸ì„¸ë¨¼ì§€", "ì½”ë¡œë‚˜", "ë°±ì‹ ", "ê±´ê°•",
            "ë‹¤ì´ì–´íŠ¸", "ìš´ë™", "ìš”ê°€", "í•„ë¼í…ŒìŠ¤", "í—¬ìŠ¤",
            "ë§›ì§‘", "ì¹´í˜", "ì—¬í–‰", "ì œì£¼ë„", "ë¶€ì‚°",
            "ë„·í”Œë¦­ìŠ¤", "ìœ íŠœë¸Œ", "ì¸ìŠ¤íƒ€ê·¸ë¨", "í‹±í†¡", "í˜ì´ìŠ¤ë¶",
            "ì•„ì´í°", "ê°¤ëŸ­ì‹œ", "ê²Œì„", "LOL", "ì˜¤ë²„ì›Œì¹˜",
            "ì˜í™”", "ë“œë¼ë§ˆ", "ì˜ˆëŠ¥", "ì›¹íˆ°", "ë§Œí™”",
            "íŒ¨ì…˜", "ë·°í‹°", "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´", "ë©”ì´í¬ì—…",
            "ë¶€ë™ì‚°", "ì „ì„¸", "ì›”ì„¸", "ì•„íŒŒíŠ¸", "ì˜¤í”¼ìŠ¤í…”",
            "ì·¨ì—…", "ì´ì§", "ì—°ë´‰", "ë©´ì ‘", "ìì†Œì„œ"
        ]
        
        # ëœë¤í•˜ê²Œ limitê°œ ì„ íƒí•˜ê³  ì‹¤ì œê°™ì€ ë¹ˆë„ ë¶€ì—¬
        selected_keywords = random.sample(
            mock_keywords_pool, 
            min(limit, len(mock_keywords_pool))
        )
        
        # ì‹¤ì œê°™ì€ ê²€ìƒ‰ ë¹ˆë„ ìƒì„± (ë†’ì€ ë¹ˆë„ ~ ë‚®ì€ ë¹ˆë„)
        keywords = [
            {
                "word": kw,
                "count": random.randint(1, 15)  # í˜„ì‹¤ì ì¸ ê²€ìƒ‰ íšŸìˆ˜ (1~15íšŒ)
            }
            for kw in selected_keywords
        ]
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
        keywords.sort(key=lambda x: x["count"], reverse=True)
        
        # ìƒìœ„ 10ê°œë¡œ íŠ¸ë Œë“œ ìƒì„±
        trends = [
            {
                "keyword": kw["word"],
                "mentions": kw["count"],  # â­ countì™€ ë™ì¼í•˜ê²Œ!
                "change": random.uniform(-30, 50),
                "category": random.choice(["ì¸ê¸°", "íŠ¸ë Œë“œ", "ì´ìŠˆ", "ê¸‰ìƒìŠ¹", "í™”ì œ"])
            }
            for kw in keywords[:10]
        ]
        
        return {
            "summary": {
                "total_trends": len(keywords),
                "new_trends": len([t for t in trends if t["change"] > 20]),
                "rising_trends": len([t for t in trends if t["change"] > 0])
            },
            "keywords": keywords,
            "trends": trends,
            "source": "mock_data",
            "note": "API ì¸ì¦ ë¬¸ì œë¡œ Mock ë°ì´í„° ì‚¬ìš© ì¤‘",
            "timestamp": datetime.now().isoformat()
        }

# ============================================
# ğŸš¨ ì‹ ê³ ê¸€ ë¶„ë¥˜ API
# ============================================

@router.get("/reports/moderation")
async def get_reports():
    """ì‹ ê³ ê¸€ í†µê³„ ë°ì´í„°"""
    
    categories = [
        ("ìŠ¤íŒ¸/ê´‘ê³ ", "pending"),
        ("ìš•ì„¤/ë¹„ë°©", "resolved"),
        ("ìŒë€ë¬¼", "resolved"),
        ("ê°œì¸ì •ë³´ ë…¸ì¶œ", "pending"),
        ("ì €ì‘ê¶Œ ì¹¨í•´", "rejected"),
        ("ê¸°íƒ€", "pending")
    ]
    
    total = sum(random.randint(10, 100) for _ in categories)
    
    return {
        "stats": {
            "total": total,
            "pending": random.randint(20, 50),
            "resolved": random.randint(30, 60),
            "rejected": random.randint(5, 15)
        },
        "categories": [
            {
                "name": name,
                "count": random.randint(10, 100),
                "status": status,
                "avg_processing_time": f"{random.randint(1, 48)}ì‹œê°„"
            }
            for name, status in categories
        ]
    }

# ============================================
# âš ï¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ API
# ============================================

@router.post("/moderation/ethics-score")
async def analyze_ethics_score(request: EthicsScoreRequest):
    """
    í…ìŠ¤íŠ¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„
    
    **ì‹¤ì œë¡œëŠ”:**
    - NLP ëª¨ë¸ ì‚¬ìš©
    - AI ê¸°ë°˜ ë¶„ì„
    - ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    """
    
    text = request.text.strip()
    
    if not text:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ Mock ë¶„ì„
    ethics_keywords = ["ë°”ë³´", "ë©ì²­", "ì“°ë ˆê¸°", "ì£½ì–´", "êº¼ì ¸"]
    detected = []
    
    for keyword in ethics_keywords:
        if keyword in text:
            detected.append({
                "text": keyword,
                "type": "ë¹„ìœ¤ë¦¬ì  í‘œí˜„",
                "severity": "high" if len(keyword) > 2 else "medium"
            })
    
    ethics_score = min(len(detected) * 25, 100)
    
    recommendations = []
    if ethics_score >= 70:
        recommendations.append({
            "priority": "high",
            "message": "ì‹¬ê°í•œ ë¹„ìœ¤ë¦¬ì  í‘œí˜„ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    elif ethics_score >= 40:
        recommendations.append({
            "priority": "medium",
            "message": "ë¶€ì ì ˆí•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    else:
        recommendations.append({
            "priority": "low",
            "message": "íŠ¹ë³„í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        })
    
    return {
        "ethics_score": ethics_score,
        "detected_expressions": detected,
        "recommendations": recommendations
    }

# ============================================
# ğŸ“Š ëŒ€ì‹œë³´ë“œ í†µê³„ API
# ============================================

@router.get("/dashboard/stats")
async def get_dashboard_stats():
    """ëŒ€ì‹œë³´ë“œìš© ì‹¤ì‹œê°„ í†µê³„"""
    
    return {
        "users": {
            "total": 12345,
            "active": 1234,
            "new_today": 56
        },
        "posts": {
            "total": 45678,
            "today": 234
        },
        "reports": {
            "total": 234,
            "pending": 45
        },
        "system": {
            "uptime": "99.9%",
            "response_time": "120ms",
            "status": "healthy"
        }
    }

# ============================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ============================================

@router.get("/test")
async def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return {
        "status": "success",
        "message": "APIê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@router.get("/test/error")
async def test_error():
    """ì—ëŸ¬ í…ŒìŠ¤íŠ¸"""
    raise HTTPException(status_code=500, detail="í…ŒìŠ¤íŠ¸ìš© ì—ëŸ¬ì…ë‹ˆë‹¤")

# ============================================
# ğŸ›¡ï¸ Ethics ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ API (ì‹¤ì œ êµ¬í˜„)
# ============================================

class EthicsAnalyzeRequest(BaseModel):
    """Ethics ë¶„ì„ ìš”ì²­ ëª¨ë¸"""
    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=1, max_length=1000)
    
    class Config:
        json_schema_extra = {
            "example": {
                "text": "ë„ˆ ì •ë§ ë©ì²­í•˜êµ¬ë‚˜"
            }
        }

class DetailedAnalysis(BaseModel):
    """ìƒì„¸ ë¶„ì„ ì •ë³´"""
    bert_score: float
    bert_confidence: float
    llm_score: float
    llm_confidence: float
    llm_spam_score: float
    rule_spam_score: float
    base_score: float
    profanity_boost: float
    weights: dict
    spam_weights: dict

class EthicsAnalyzeResponse(BaseModel):
    """Ethics ë¶„ì„ ì‘ë‹µ ëª¨ë¸"""
    text: str
    score: float = Field(..., description="ë¹„ìœ¤ë¦¬ ì ìˆ˜ (0-100)")
    confidence: float = Field(..., description="ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„ (0-100)")
    spam: float = Field(..., description="ìŠ¤íŒ¸ ì§€ìˆ˜ (0-100)")
    spam_confidence: float = Field(..., description="ìŠ¤íŒ¸ ì‹ ë¢°ë„ (0-100)")
    types: List[str] = Field(..., description="ë¶„ì„ ìœ í˜• ëª©ë¡")
    detailed: DetailedAnalysis = Field(..., description="ìƒì„¸ ë¶„ì„ ì •ë³´")


def simplify_result(result: dict) -> dict:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  1ìë¦¬)"""
    return {
        'text': result['text'],
        'score': round(result['final_score'], 1),
        'confidence': round(result['final_confidence'], 1),
        'spam': round(result['spam_score'], 1),
        'spam_confidence': round(result['spam_confidence'], 1),
        'types': result['types'],
        # ìƒì„¸ ì •ë³´ ì¶”ê°€
        'detailed': {
            'bert_score': round(result['bert_score'], 1),
            'bert_confidence': round(result['bert_confidence'], 1),
            'llm_score': round(result['llm_score'], 1),
            'llm_confidence': round(result['llm_confidence'], 1),
            'llm_spam_score': round(result['llm_spam_score'], 1),
            'rule_spam_score': round(result['rule_spam_score'], 1),
            'base_score': round(result['base_score'], 1),
            'profanity_boost': round(result['profanity_boost'], 1),
            'weights': {
                'bert': round(result['weights']['bert'], 2),
                'llm': round(result['weights']['llm'], 2)
            },
            'spam_weights': {
                'llm': 0.6 if result['rule_spam_score'] < 80 else 0.3,
                'rule': 0.4 if result['rule_spam_score'] < 80 else 0.7
            }
        }
    }


@router.post("/ethics/analyze", response_model=EthicsAnalyzeResponse, tags=["ethics"])
async def ethics_analyze(request_data: EthicsAnalyzeRequest, request: Request):
    """
    í…ìŠ¤íŠ¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
    
    - **text**: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ìµœëŒ€ 1000ì)
    
    Returns:
    - ë¹„ìœ¤ë¦¬ ì ìˆ˜, ì‹ ë¢°ë„, ìŠ¤íŒ¸ ì§€ìˆ˜, ìœ í˜• ì •ë³´ ë“±
    """
    global ethics_analyzer
    
    # ì§€ì—° ë¡œë”©: ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„
    if ethics_analyzer is None:
        try:
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘ (ì¬ì‹œë„)...")
            from ethics.ethics_hybrid_predictor import HybridEthicsAnalyzer
            ethics_analyzer = HybridEthicsAnalyzer()
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. models/ ë””ë ‰í† ë¦¬ì™€ .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    if ethics_analyzer is None:
        raise HTTPException(status_code=503, detail="ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = time.time()
    
    try:
        result = ethics_analyzer.analyze(request_data.text)
        simplified = simplify_result(result)
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = time.time() - start_time
        
        # ë¡œê·¸ ì €ì¥
        try:
            from ethics.ethics_db_logger import db_logger
            db_logger.log_analysis(
                text=simplified['text'],
                score=simplified['score'],
                confidence=simplified['confidence'],
                spam=simplified['spam'],
                spam_confidence=simplified['spam_confidence'],
                types=simplified['types'],
                ip_address=request.client.host,
                user_agent=request.headers.get('user-agent'),
                response_time=response_time
            )
        except Exception as log_error:
            print(f"[WARN] ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {log_error}")
        
        return simplified
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/ethics/logs", tags=["ethics"])
async def get_ethics_logs(
    limit: int = Query(100, description="ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜"),
    offset: int = Query(0, description="ì‹œì‘ ìœ„ì¹˜"),
    min_score: Optional[float] = Query(None, description="ìµœì†Œ ì ìˆ˜ í•„í„°"),
    max_score: Optional[float] = Query(None, description="ìµœëŒ€ ì ìˆ˜ í•„í„°"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
):
    """
    Ethics ë¶„ì„ ë¡œê·¸ ì¡°íšŒ
    
    - **limit**: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    - **offset**: ì‹œì‘ ìœ„ì¹˜ (ê¸°ë³¸ê°’: 0)
    - **min_score**: ìµœì†Œ ì ìˆ˜ í•„í„°
    - **max_score**: ìµœëŒ€ ì ìˆ˜ í•„í„°
    - **start_date**: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
    - **end_date**: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
    """
    try:
        from ethics.ethics_db_logger import db_logger
        logs = db_logger.get_logs(
            limit=limit,
            offset=offset,
            min_score=min_score,
            max_score=max_score,
            start_date=start_date,
            end_date=end_date
        )
        return {
            "logs": logs,
            "count": len(logs),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/ethics/logs/stats", tags=["ethics"])
async def get_ethics_statistics(days: int = Query(7, description="ì¡°íšŒí•  ì¼ìˆ˜")):
    """
    Ethics í†µê³„ ì •ë³´ ì¡°íšŒ
    
    - **days**: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)
    
    Returns:
    - ì „ì²´ ê±´ìˆ˜, í‰ê·  ì ìˆ˜, ê³ ìœ„í—˜ ê±´ìˆ˜, ìŠ¤íŒ¸ ê±´ìˆ˜, ì¼ë³„ í†µê³„
    """
    try:
        from ethics.ethics_db_logger import db_logger
        stats = db_logger.get_statistics(days=days)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/{log_id}", tags=["ethics"])
async def delete_ethics_log(log_id: int):
    """
    íŠ¹ì • Ethics ë¡œê·¸ ì‚­ì œ
    
    - **log_id**: ì‚­ì œí•  ë¡œê·¸ì˜ ID
    
    Returns:
    - ì‚­ì œ ì„±ê³µ ë©”ì‹œì§€
    """
    try:
        from ethics.ethics_db_logger import db_logger
        success = db_logger.delete_log(log_id)
        if success:
            return {
                "success": True,
                "message": f"ë¡œê·¸ ID {log_id} ì‚­ì œ ì™„ë£Œ"
            }
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/batch/old", tags=["ethics"])
async def delete_old_ethics_logs(days: int = Query(90, description="ë³´ê´€ ê¸°ê°„ (ì¼)")):
    """
    ì˜¤ë˜ëœ Ethics ë¡œê·¸ ì‚­ì œ
    
    - **days**: ë³´ê´€ ê¸°ê°„ (ê¸°ë³¸ê°’: 90ì¼, 0ì´ë©´ ëª¨ë“  ë¡œê·¸ ì‚­ì œ)
    
    Returns:
    - ì‚­ì œëœ ë¡œê·¸ ìˆ˜
    """
    try:
        from ethics.ethics_db_logger import db_logger
        if days == 0:
            # ëª¨ë“  ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_all_logs()
            return {
                "deleted_count": deleted_count,
                "message": f"ëª¨ë“  ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
        else:
            # ì§€ì •ëœ ê¸°ê°„ ì´ì „ ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_old_logs(days=days)
            return {
                "deleted_count": deleted_count,
                "message": f"{days}ì¼ ì´ì „ ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ============================================
# RAG ê´€ë ¨ ì½”ë“œ (í…ŒìŠ¤íŠ¸ ì™„ë£Œ ì „ê¹Œì§€ ì£¼ì„ ì²˜ë¦¬)
# ============================================
# @router.get("/risk/top", tags=["risk"])
# async def get_risk_top_users(limit: int = Query(10, ge=1, le=100, description="ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜")):
#     """
#     ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
#     
#     - **limit**: ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 100)
#     
#     Returns:
#     - summary: í†µê³„ ìš”ì•½ ì •ë³´
#     - users: ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡
#     """
#     try:
#         from chrun_backend.rag_pipeline.high_risk_store import get_recent_high_risk, init_db
#         from datetime import datetime
#         
#         # DB ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
#         init_db()
#         
#         # ê³ ìœ„í—˜ ë°ì´í„° ì¡°íšŒ
#         risk_data = get_recent_high_risk(limit=limit)
#         
#         if not risk_data:
#             return {
#                 "summary": {
#                     "total_users": 0,
#                     "high_priority_count": 0,
#                     "medium_priority_count": 0,
#                     "avg_risk_score": 0.0
#                 },
#                 "users": []
#             }
#         
#         # ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ user_idì˜ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ ì‚¬ìš©ìë¡œ)
#         user_dict = {}
#         for item in risk_data:
#             user_id = item['user_id']
#             if user_id not in user_dict:
#                 user_dict[user_id] = {
#                     'chunk_id': item['chunk_id'],
#                     'user_id': user_id,
#                     'username': f"ì‚¬ìš©ì_{user_id}",
#                     'post_id': item.get('post_id', ''),
#                     'risk_score': item['risk_score'],
#                     'confirmed': bool(item.get('confirmed', 0)),
#                     'evidence_sentences': [],
#                     'last_activity': item.get('created_at', datetime.now().isoformat()),
#                     'feedback_at': item.get('created_at') if item.get('confirmed') else None
#                 }
#             
#             # ë¬¸ì¥ ì¶”ê°€
#             user_dict[user_id]['evidence_sentences'].append(item['sentence'])
#             
#             # ê°€ì¥ ë†’ì€ risk_score ì‚¬ìš©
#             if item['risk_score'] > user_dict[user_id]['risk_score']:
#                 user_dict[user_id]['risk_score'] = item['risk_score']
#                 user_dict[user_id]['chunk_id'] = item['chunk_id']
#         
#         # ì‚¬ìš©ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
#         users = []
#         for user_data in user_dict.values():
#             # Priority ê²°ì • (risk_score >= 0.7: HIGH, >= 0.5: MEDIUM, ê·¸ ì™¸: LOW)
#             if user_data['risk_score'] >= 0.7:
#                 priority = 'HIGH'
#             elif user_data['risk_score'] >= 0.5:
#                 priority = 'MEDIUM'
#             else:
#                 priority = 'LOW'
#             
#             # ì œì•ˆ ì¡°ì¹˜ì‚¬í•­ ìƒì„±
#             if priority == 'HIGH':
#                 suggested_action = "ì¦‰ì‹œ ì—°ë½ ë° ê°œì„  ì¡°ì¹˜ í•„ìš”. ê³ ìœ„í—˜ ì´íƒˆ ì§•í›„ ê°ì§€ë¨."
#             elif priority == 'MEDIUM':
#                 suggested_action = "ëª¨ë‹ˆí„°ë§ ê°•í™” ë° ì˜ˆë°©ì  ì¡°ì¹˜ ê¶Œì¥."
#             else:
#                 suggested_action = "ì •ê¸° ëª¨ë‹ˆí„°ë§ ê¶Œì¥."
#             
#             users.append({
#                 **user_data,
#                 'priority': priority,
#                 'similar_patterns_count': len(user_data['evidence_sentences']),
#                 'suggested_action': suggested_action
#             })
#         
#         # risk_score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
#         users.sort(key=lambda x: x['risk_score'], reverse=True)
#         
#         # í†µê³„ ê³„ì‚°
#         high_priority_count = sum(1 for u in users if u['priority'] == 'HIGH')
#         medium_priority_count = sum(1 for u in users if u['priority'] == 'MEDIUM')
#         avg_risk_score = sum(u['risk_score'] for u in users) / len(users) if users else 0.0
#         
#         return {
#             "summary": {
#                 "total_users": len(users),
#                 "high_priority_count": high_priority_count,
#                 "medium_priority_count": medium_priority_count,
#                 "avg_risk_score": round(avg_risk_score, 2)
#             },
#             "users": users
#         }
#         
#     except Exception as e:
#         import traceback
#         traceback.print_exc()
#         raise HTTPException(status_code=500, detail=f"ê³ ìœ„í—˜ ì‚¬ìš©ì ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


class RiskFeedbackRequest(BaseModel):
    """ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ìš”ì²­"""
    chunk_id: str
    confirmed: bool


class CheckNewPostRequest(BaseModel):
    """ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ìš”ì²­"""
    text: str
    user_id: str
    post_id: str
    created_at: str


# @router.post("/risk/feedback", tags=["risk"])
# async def submit_risk_feedback(request_data: RiskFeedbackRequest):
#     """
#     ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ
#     
#     - **chunk_id**: í”¼ë“œë°±í•  chunk_id
#     - **confirmed**: ìœ„í—˜ í™•ì¸ ì—¬ë¶€ (true: ìœ„í—˜ ë§ìŒ, false: ìœ„í—˜ ì•„ë‹˜)
#     
#     Returns:
#     - ì„±ê³µ ë©”ì‹œì§€
#     """
#     try:
#         from chrun_backend.rag_pipeline.high_risk_store import update_feedback, get_chunk_by_id
#         
#         # 1. ê¸°ì¡´ SQLite í”¼ë“œë°± ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
#         update_feedback(request_data.chunk_id, request_data.confirmed)
#         
#         # 2. confirmed=trueì¸ ê²½ìš°ì—ë§Œ ë²¡í„°DBì— ì €ì¥
#         if request_data.confirmed:
#             try:
#                 # 2-1. SQLiteì—ì„œ í•´ë‹¹ chunk ì •ë³´ ì¡°íšŒ
#                 chunk_data = get_chunk_by_id(request_data.chunk_id)
#                 
#                 if not chunk_data:
#                     # chunkë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
#                     print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: chunk_id {request_data.chunk_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
#                 else:
#                     # 2-2. ì„ë² ë”© ìƒì„±
#                     from chrun_backend.rag_pipeline.embedding_service import get_embedding
#                     sentence = chunk_data.get('sentence', '')
#                     
#                     if sentence.strip():
#                         embedding = get_embedding(sentence)
#                         
#                         # 2-3. ë²¡í„°DBì— ì €ì¥í•  ë©”íƒ€ë°ì´í„° êµ¬ì„±
#                         from chrun_backend.rag_pipeline.vector_db import build_chunk_id
#                         
#                         # ì•ˆì •ì ì¸ chunk_id ìƒì„± (ê¸°ì¡´ chunk_idì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
#                         vector_chunk_id = build_chunk_id(sentence, chunk_data.get('post_id', ''))
#                         
#                         meta = {
#                             "chunk_id": vector_chunk_id,  # ë²¡í„°DBìš© ì•ˆì •ì  ID
#                             "original_chunk_id": chunk_data.get('chunk_id'),  # ì›ë³¸ SQLite chunk_id
#                             "user_id": chunk_data.get('user_id', ''),
#                             "post_id": chunk_data.get('post_id', ''),
#                             "sentence": sentence,
#                             "risk_score": float(chunk_data.get('risk_score', 0.0)),
#                             "created_at": chunk_data.get('created_at', ''),
#                             "confirmed": True
#                         }
#                         
#                         # 2-4. ë²¡í„°DBì— upsert (idempotent)
#                         from chrun_backend.rag_pipeline.vector_db import get_client, upsert_confirmed_chunk
#                         
#                         client = get_client()  # ê¸°ë³¸ ê²½ë¡œ "./chroma_store" ì‚¬ìš©
#                         upsert_confirmed_chunk(client, embedding, meta)
#                         
#                         print(f"[INFO] í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„°DBì— ì €ì¥ ì™„ë£Œ: {vector_chunk_id}")
#                     else:
#                         print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: ë¹ˆ ë¬¸ì¥ (chunk_id: {request_data.chunk_id})")
#                         
#             except Exception as vector_error:
#                 # ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
#                 import traceback
#                 print(f"[ERROR] ë²¡í„°DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {vector_error}")
#                 traceback.print_exc()
#                 # ì—ëŸ¬ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  APIëŠ” ì„±ê³µìœ¼ë¡œ ì‘ë‹µ
#         
#         return {
#             "status": "ok",
#             "message": f"í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (chunk_id: {request_data.chunk_id}, confirmed: {request_data.confirmed})"
#         }
#         
#     except Exception as e:
#         import traceback
#         traceback.print_exc()
#         raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


# @router.post("/risk/check_new_post", tags=["risk"])
# async def check_new_post_risk(request_data: CheckNewPostRequest):
#     """
#     ìƒˆ ê²Œì‹œë¬¼ì˜ ìœ„í—˜ë„ë¥¼ ì²´í¬í•˜ì—¬ ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
#     
#     - **text**: ë¶„ì„í•  ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸
#     - **user_id**: ì‚¬ìš©ì ID
#     - **post_id**: ê²Œì‹œë¬¼ ID
#     - **created_at**: ìƒì„± ì‹œê°„ (ISO í˜•ì‹, ì˜ˆ: "2024-11-04T10:30:00")
#     
#     Returns:
#     - ìœ„í—˜ë„ ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ (ê·¼ê±° ë¬¸ì¥ë“¤ê³¼ í†µê³„ ì •ë³´)
#     """
#     try:
#         from chrun_backend.rag_pipeline.rag_checker import check_new_post
#         
#         # RAG ê¸°ë°˜ ìœ„í—˜ë„ ì²´í¬ ìˆ˜í–‰
#         context = check_new_post(
#             text=request_data.text,
#             user_id=request_data.user_id,
#             post_id=request_data.post_id,
#             created_at=request_data.created_at
#         )
#         
#         return context
#         
#     except Exception as e:
#         import traceback
#         traceback.print_exc()
#         raise HTTPException(status_code=500, detail=f"ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")