"""
ğŸ¯ Mock API ì—”ë“œí¬ì¸íŠ¸
ì‹œë‹ˆì–´ì˜ ì„¤ëª…:
- ì‹¤ì œ ë°±ì—”ë“œ ì™„ì„± ì „ê¹Œì§€ ì‚¬ìš©í•  ê°€ì§œ ë°ì´í„°
- í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ ì‹œ ìœ ìš©
- ë‚˜ì¤‘ì— ì‹¤ì œ DBë¡œ êµì²´
"""

import logging
import os

from fastapi import APIRouter, Query, HTTPException, Request
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime, timedelta
from collections import Counter
import random
import time
import httpx
from chrun_backend.rag_pipeline.models import AnalysisRequest
from chrun_backend.rag_pipeline.service import analyze_and_store
from chrun_backend.rag_pipeline.report_repository import get_recent_results

router = APIRouter(tags=["api"])
logger = logging.getLogger(__name__)

# Ethics Analyzer ì „ì—­ ë³€ìˆ˜ (main.pyì—ì„œ ì´ˆê¸°í™”ë¨)
ethics_analyzer = None

# ============================================
# ğŸ“Š ë°ì´í„° ëª¨ë¸ (Pydantic)
# ============================================

class SearchResult(BaseModel):
    """ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    id: int
    title: str
    content: str
    author: str
    date: str
    category: str

class BounceMetrics(BaseModel):
    """ì´íƒˆë¥  ë©”íŠ¸ë¦­"""
    avg_bounce_rate: float
    total_visitors: int
    bounced_visitors: int
    period: str

class TrendItem(BaseModel):
    """íŠ¸ë Œë“œ ì•„ì´í…œ"""
    keyword: str
    mentions: int
    change: float
    category: str

class ReportCategory(BaseModel):
    """ì‹ ê³  ì¹´í…Œê³ ë¦¬"""
    name: str
    count: int
    status: str
    avg_processing_time: str

class EthicsScoreRequest(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ìš”ì²­"""
    text: str

class EthicsScoreResponse(BaseModel):
    """ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ ì‘ë‹µ"""
    ethics_score: float
    detected_expressions: List[dict]
    recommendations: List[dict]

# ============================================
# ğŸ” ê²€ìƒ‰ API
# ============================================

@router.get("/search")
async def search(q: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ")):
    """
    ìì—°ì–´ ê²€ìƒ‰ API
    
    **ì‹œë‹ˆì–´ì˜ íŒ:**
    - Query(...) : í•„ìˆ˜ íŒŒë¼ë¯¸í„°
    - Query(None) : ì„ íƒì  íŒŒë¼ë¯¸í„°
    """
    
    if not q:
        raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # Mock ë°ì´í„° ìƒì„±
    results = [
        {
            "id": i,
            "title": f"{q}ì— ê´€í•œ ê²Œì‹œê¸€ {i+1}",
            "content": f"ì´ê²ƒì€ '{q}' í‚¤ì›Œë“œì™€ ê´€ë ¨ëœ ìƒ˜í”Œ ê²Œì‹œê¸€ì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ë©ë‹ˆë‹¤.",
            "author": f"ì‚¬ìš©ì{random.randint(1, 100)}",
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "category": random.choice(["ììœ ê²Œì‹œíŒ", "ì§ˆë¬¸", "ì •ë³´", "í† ë¡ "])
        }
        for i in range(5)
    ]
    
    return {
        "query": q,
        "total": len(results),
        "results": results
    }

# ============================================
# ğŸ“Š ì´íƒˆë¥  ë©”íŠ¸ë¦­ API
# ============================================

@router.get("/metrics/bounce")
async def get_bounce_metrics():
    """
    ë°©ë¬¸ê° ì´íƒˆë¥  ë°ì´í„°
    
    **Mock ë°ì´í„°:**
    ì‹¤ì œë¡œëŠ” Google Analyticsë‚˜ ìì²´ ë¶„ì„ ì‹œìŠ¤í…œì—ì„œ ê°€ì ¸ì˜´
    """
    
    return {
        "metrics": {
            "avg_bounce_rate": 42.5,
            "total_visitors": 15234,
            "bounced_visitors": 6474,
            "period": "2025-01-01 ~ 2025-01-31"
        },
        "details": [
            {
                "date": f"2025-01-{i+1:02d}",
                "visitors": random.randint(300, 800),
                "bounced": random.randint(100, 400),
                "bounce_rate": random.uniform(30, 60)
            }
            for i in range(7)
        ]
    }

# ============================================
# ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„ API (ì‹¤ì œ ë°ì´í„°)
# ============================================

@router.get("/trends")
async def get_trends(limit: int = Query(100, ge=1, le=1000)):
    """
    MySQL ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¸ë Œë“œ ë°ì´í„° ë°˜í™˜
    
    **ì„¤ëª…:**
    - trend_keywords í…Œì´ë¸”ì—ì„œ ìµœê·¼ 7ì¼ê°„ í‚¤ì›Œë“œ ë°ì´í„° ì¡°íšŒ
    - ë‚ ì§œë³„ íƒ€ì„ë¼ì¸ ìƒì„±
    - ì¦ê°ë¥  ê³„ì‚° (ì „ì¼ ëŒ€ë¹„)
    - ê²Œì‹œê¸€/ëŒ“ê¸€ í†µê³„ í¬í•¨
    """
    from app.database import get_db_connection
    
    try:
        print(f"\n[INFO] MySQLì—ì„œ íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ (limit={limit})")
        
        with get_db_connection() as conn:
            cursor = conn.cursor()
            
            # 1. ìµœê·¼ 7ì¼ê°„ì˜ ì „ì²´ í‚¤ì›Œë“œ ì¡°íšŒ (ë‚ ì§œë³„)
            seven_days_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            
            cursor.execute("""
                SELECT 
                    keyword,
                    SUM(search_count) as total_count,
                    search_date,
                    category
                FROM trend_keywords
                WHERE search_date >= %s
                GROUP BY keyword, search_date, category
                ORDER BY search_date DESC, total_count DESC
            """, (seven_days_ago,))
            
            keyword_data = cursor.fetchall()
            
            # 2. í‚¤ì›Œë“œë³„ ì´ ì§‘ê³„ (ì „ì²´ ê¸°ê°„)
            cursor.execute("""
                SELECT 
                    keyword,
                    SUM(search_count) as total_count,
                    category
                FROM trend_keywords
                WHERE search_date >= %s
                GROUP BY keyword, category
                ORDER BY total_count DESC
                LIMIT %s
            """, (seven_days_ago, limit))
            
            top_keywords = cursor.fetchall()
            
            # 3. ê²Œì‹œê¸€/ëŒ“ê¸€ í†µê³„ ì¡°íšŒ
            cursor.execute("""
                SELECT 
                    COALESCE(total_posts, 0) as total_posts,
                    COALESCE(total_comments, 0) as total_comments
                FROM trend_stats_cache
                WHERE stat_date = CURDATE()
                LIMIT 1
            """)
            
            stats = cursor.fetchone()
            
            if stats:
                total_posts = stats['total_posts']
                total_comments = stats['total_comments']
            else:
                # ìºì‹œê°€ ì—†ìœ¼ë©´ ì‹¤ì œ í…Œì´ë¸”ì—ì„œ ì¡°íšŒ
                cursor.execute("SELECT COUNT(*) as cnt FROM board")
                posts_result = cursor.fetchone()
                total_posts = posts_result['cnt'] if posts_result else 0
                
                cursor.execute("SELECT COUNT(*) as cnt FROM comment")
                comments_result = cursor.fetchone()
                total_comments = comments_result['cnt'] if comments_result else 0
            
            cursor.close()
        
        print(f"[INFO] ì¡°íšŒëœ í‚¤ì›Œë“œ: {len(top_keywords)}ê°œ")
        
        # í‚¤ì›Œë“œ ëª©ë¡ ìƒì„±
        keywords = [
            {
                "word": item['keyword'],
                "count": item['total_count']
            }
            for item in top_keywords
        ]
        
        # ë‚ ì§œë³„ ë°ì´í„° êµ¬ì¡°í™”
        date_word_counts = {}
        for item in keyword_data:
            date = str(item['search_date'])
            keyword = item['keyword']
            count = item['total_count']
            
            if date not in date_word_counts:
                date_word_counts[date] = {}
            date_word_counts[date][keyword] = count
        
        # ì¦ê°ë¥  ê³„ì‚°
        dates = sorted(date_word_counts.keys())
        trends = []
        
        for kw in keywords[:20]:  # ìƒìœ„ 20ê°œë§Œ íŠ¸ë Œë“œ ë¶„ì„
            word = kw["word"]
            
            # ìµœê·¼ ë‚ ì§œì™€ ì´ì „ ë‚ ì§œì˜ ê²€ìƒ‰ íšŸìˆ˜ ë¹„êµ
            if len(dates) >= 2:
                recent_count = date_word_counts.get(dates[-1], {}).get(word, 0)
                previous_count = date_word_counts.get(dates[-2], {}).get(word, 0)
                
                if previous_count > 0:
                    change = ((recent_count - previous_count) / previous_count) * 100
                else:
                    change = 100.0 if recent_count > 0 else 0.0
            else:
                change = 0.0
            
            # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
            if change > 50:
                category = "ê¸‰ìƒìŠ¹"
            elif change > 0:
                category = "ìƒìŠ¹"
            elif change < -50:
                category = "ê¸‰ê°"
            elif change < 0:
                category = "í•˜ë½"
            else:
                category = "ìœ ì§€"
            
            trends.append({
                "keyword": word,
                "mentions": kw["count"],
                "change": round(change, 1),
                "category": category
            })
        
        # íƒ€ì„ë¼ì¸ ë°ì´í„° ìƒì„± (ë‚ ì§œë³„ ì´ ê²€ìƒ‰ íšŸìˆ˜)
        timeline = []
        for date in sorted(dates):
            total_count = sum(date_word_counts[date].values())
            timeline.append({
                "date": date,
                "count": total_count
            })
        
        print(f"[INFO] íŠ¸ë Œë“œ {len(trends)}ê°œ, íƒ€ì„ë¼ì¸ {len(timeline)}ê°œ ìƒì„±")
        
        # í†µê³„ ê³„ì‚°
        total_searches = sum(kw['count'] for kw in keywords)
        unique_keywords = len(keywords)
        
        return {
            "summary": {
                "total_posts": total_posts,
                "total_comments": total_comments,
                "total_searches": total_searches,
                "unique_keywords": unique_keywords,
                "total_trends": len(keywords),
                "new_trends": len([t for t in trends if t["change"] > 50]),
                "rising_trends": len([t for t in trends if t["change"] > 0])
            },
            "keywords": keywords,
            "trends": trends,
            "timeline": timeline,
            "source": "mysql",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"[ERROR] MySQL íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback: ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        print("[FALLBACK] ë”ë¯¸ ë°ì´í„° ì‚¬ìš©")
        
        mock_keywords = [
            {"word": "ì¸ê³µì§€ëŠ¥", "count": 450},
            {"word": "ì±—GPT", "count": 380},
            {"word": "ê²€ìƒ‰", "count": 320},
            {"word": "ì¶”ì²œ", "count": 280},
            {"word": "Python", "count": 250},
            {"word": "ì§ˆë¬¸", "count": 230},
            {"word": "ë§›ì§‘", "count": 210},
            {"word": "ì—¬í–‰", "count": 195},
            {"word": "ì˜í™”", "count": 180},
            {"word": "ë¦¬ë·°", "count": 175}
        ]
        
        mock_trends = [
            {"keyword": "ì¸ê³µì§€ëŠ¥", "mentions": 450, "change": 12.5, "category": "ìƒìŠ¹"},
            {"keyword": "ì±—GPT", "mentions": 380, "change": 8.6, "category": "ìƒìŠ¹"},
            {"keyword": "ê²€ìƒ‰", "mentions": 320, "change": 6.7, "category": "ìƒìŠ¹"},
            {"keyword": "ì¶”ì²œ", "mentions": 280, "change": 7.7, "category": "ìƒìŠ¹"},
            {"keyword": "Python", "mentions": 250, "change": 4.2, "category": "ìƒìŠ¹"}
        ]
        
        mock_timeline = [
            {"date": "2025-01-06", "count": 1450},
            {"date": "2025-01-07", "count": 1680},
            {"date": "2025-01-08", "count": 1920},
            {"date": "2025-01-09", "count": 2150},
            {"date": "2025-01-10", "count": 2380},
            {"date": "2025-01-11", "count": 2610},
            {"date": "2025-01-12", "count": 2850}
        ]
        
        return {
            "summary": {
                "total_posts": 1250,
                "total_comments": 6780,
                "total_searches": sum(k['count'] for k in mock_keywords),
                "unique_keywords": len(mock_keywords),
                "total_trends": len(mock_keywords),
                "new_trends": 0,
                "rising_trends": 5
            },
            "keywords": mock_keywords,
            "trends": mock_trends,
            "timeline": mock_timeline,
            "source": "fallback",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ============================================
# ğŸš¨ ì‹ ê³ ê¸€ ë¶„ë¥˜ API
# ============================================

@router.get("/reports/moderation")
async def get_reports():
    """ì‹ ê³ ê¸€ í†µê³„ ë°ì´í„°"""
    
    categories = [
        ("ìŠ¤íŒ¸/ê´‘ê³ ", "pending"),
        ("ìš•ì„¤/ë¹„ë°©", "resolved"),
        ("ìŒë€ë¬¼", "resolved"),
        ("ê°œì¸ì •ë³´ ë…¸ì¶œ", "pending"),
        ("ì €ì‘ê¶Œ ì¹¨í•´", "rejected"),
        ("ê¸°íƒ€", "pending")
    ]
    
    total = sum(random.randint(10, 100) for _ in categories)
    
    return {
        "stats": {
            "total": total,
            "pending": random.randint(20, 50),
            "resolved": random.randint(30, 60),
            "rejected": random.randint(5, 15)
        },
        "categories": [
            {
                "name": name,
                "count": random.randint(10, 100),
                "status": status,
                "avg_processing_time": f"{random.randint(1, 48)}ì‹œê°„"
            }
            for name, status in categories
        ]
    }

# ============================================
# âš ï¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„ API
# ============================================

@router.post("/moderation/ethics-score")
async def analyze_ethics_score(request: EthicsScoreRequest):
    """
    í…ìŠ¤íŠ¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ì§€ìˆ˜ ë¶„ì„
    
    **ì‹¤ì œë¡œëŠ”:**
    - NLP ëª¨ë¸ ì‚¬ìš©
    - AI ê¸°ë°˜ ë¶„ì„
    - ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    """
    
    text = request.text.strip()
    
    if not text:
        raise HTTPException(status_code=400, detail="ë¶„ì„í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ Mock ë¶„ì„
    ethics_keywords = ["ë°”ë³´", "ë©ì²­", "ì“°ë ˆê¸°", "ì£½ì–´", "êº¼ì ¸"]
    detected = []
    
    for keyword in ethics_keywords:
        if keyword in text:
            detected.append({
                "text": keyword,
                "type": "ë¹„ìœ¤ë¦¬ì  í‘œí˜„",
                "severity": "high" if len(keyword) > 2 else "medium"
            })
    
    ethics_score = min(len(detected) * 25, 100)
    
    recommendations = []
    if ethics_score >= 70:
        recommendations.append({
            "priority": "high",
            "message": "ì‹¬ê°í•œ ë¹„ìœ¤ë¦¬ì  í‘œí˜„ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    elif ethics_score >= 40:
        recommendations.append({
            "priority": "medium",
            "message": "ë¶€ì ì ˆí•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        })
    else:
        recommendations.append({
            "priority": "low",
            "message": "íŠ¹ë³„í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        })
    
    return {
        "ethics_score": ethics_score,
        "detected_expressions": detected,
        "recommendations": recommendations
    }

# ============================================
# ğŸ“Š ëŒ€ì‹œë³´ë“œ í†µê³„ API
# ============================================

@router.get("/dashboard/stats")
async def get_dashboard_stats():
    """ëŒ€ì‹œë³´ë“œìš© ì‹¤ì‹œê°„ í†µê³„"""
    
    return {
        "users": {
            "total": 12345,
            "active": 1234,
            "new_today": 56
        },
        "posts": {
            "total": 45678,
            "today": 234
        },
        "reports": {
            "total": 234,
            "pending": 45
        },
        "system": {
            "uptime": "99.9%",
            "response_time": "120ms",
            "status": "healthy"
        }
    }

# ============================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# ============================================

@router.get("/test")
async def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return {
        "status": "success",
        "message": "APIê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0"
    }

@router.get("/test/error")
async def test_error():
    """ì—ëŸ¬ í…ŒìŠ¤íŠ¸"""
    raise HTTPException(status_code=500, detail="í…ŒìŠ¤íŠ¸ìš© ì—ëŸ¬ì…ë‹ˆë‹¤")

# ============================================
# ğŸ›¡ï¸ Ethics ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ API (ì‹¤ì œ êµ¬í˜„)
# ============================================

class EthicsAnalyzeRequest(BaseModel):
    """Ethics ë¶„ì„ ìš”ì²­ ëª¨ë¸"""
    text: str = Field(..., description="ë¶„ì„í•  í…ìŠ¤íŠ¸", min_length=1, max_length=1000)
    
    class Config:
        json_schema_extra = {
            "example": {
                "text": "ë„ˆ ì •ë§ ë©ì²­í•˜êµ¬ë‚˜"
            }
        }

class RagSimilarCase(BaseModel):
    sentence: str
    similarity: float
    immoral_score: float
    spam_score: float
    confidence: float
    confirmed: bool
    feedback_type: Optional[str] = None
    created_at: Optional[str] = None


class RagAnalysis(BaseModel):
    enabled: bool
    adjustment_applied: bool
    adjustment_weight: float
    similar_cases_count: int
    max_similarity: float
    adjusted_score: Optional[float] = None
    adjusted_spam_score: Optional[float] = None
    similar_cases: List[RagSimilarCase] = Field(default_factory=list)


class DetailedAnalysis(BaseModel):
    """ìƒì„¸ ë¶„ì„ ì •ë³´"""
    bert_score: Optional[float] = None
    bert_confidence: Optional[float] = None
    llm_score: Optional[float] = None
    llm_confidence: Optional[float] = None
    llm_spam_score: Optional[float] = None
    rule_spam_score: Optional[float] = None
    base_score: Optional[float] = None
    profanity_boost: Optional[float] = None
    weights: dict
    spam_weights: dict
    rag: RagAnalysis

class EthicsAnalyzeResponse(BaseModel):
    """Ethics ë¶„ì„ ì‘ë‹µ ëª¨ë¸"""
    text: str
    score: Optional[float] = Field(None, description="ë¹„ìœ¤ë¦¬ ì ìˆ˜ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    confidence: Optional[float] = Field(None, description="ë¹„ìœ¤ë¦¬ ì‹ ë¢°ë„ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    spam: Optional[float] = Field(None, description="ìŠ¤íŒ¸ ì§€ìˆ˜ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    spam_confidence: Optional[float] = Field(None, description="ìŠ¤íŒ¸ ì‹ ë¢°ë„ (0-100, ì¦‰ì‹œ ì°¨ë‹¨ ì‹œ null)")
    types: List[str] = Field(..., description="ë¶„ì„ ìœ í˜• ëª©ë¡")
    auto_blocked: Optional[bool] = Field(False, description="ì¦‰ì‹œ ì°¨ë‹¨ ì—¬ë¶€")
    detailed: DetailedAnalysis = Field(..., description="ìƒì„¸ ë¶„ì„ ì •ë³´")


def simplify_result(result: dict) -> dict:
    """ë¶„ì„ ê²°ê³¼ë¥¼ ê°„ê²°í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì†Œìˆ˜ì  1ìë¦¬)"""
    rag_similar_cases = []
    for case in result.get('rag_similar_cases', []) or []:
        rag_similar_cases.append({
            'sentence': case.get('sentence', ''),
            'similarity': round(case.get('similarity', 0.0), 3),
            'immoral_score': round(case.get('immoral_score', 0.0), 1),
            'spam_score': round(case.get('spam_score', 0.0), 1),
            'confidence': round(case.get('confidence', 0.0), 1),
            'confirmed': bool(case.get('confirmed', False)),
            'feedback_type': case.get('feedback_type'),
            'created_at': case.get('created_at')
        })

    adjustment_applied = bool(result.get('adjustment_applied', False))
    auto_blocked = bool(result.get('auto_blocked', False))
    
    # ì¦‰ì‹œ ì°¨ë‹¨ ì¼€ì´ìŠ¤ëŠ” None ê°’ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
    def safe_round(value, digits=1):
        """None-safe rounding"""
        return round(value, digits) if value is not None else None
    
    return {
        'text': result['text'],
        'score': safe_round(result.get('final_score')),
        'confidence': safe_round(result.get('final_confidence')),
        'spam': safe_round(result.get('spam_score')),
        'spam_confidence': safe_round(result.get('spam_confidence')),
        'types': result.get('types', []),
        'auto_blocked': auto_blocked,
        # ìƒì„¸ ì •ë³´ ì¶”ê°€
        'detailed': {
            'bert_score': safe_round(result.get('bert_score')),
            'bert_confidence': safe_round(result.get('bert_confidence')),
            'llm_score': safe_round(result.get('llm_score', 0.0)) if not auto_blocked else None,
            'llm_confidence': safe_round(result.get('llm_confidence', 0.0)) if not auto_blocked else None,
            'llm_spam_score': safe_round(result.get('llm_spam_score', 0.0)) if not auto_blocked else None,
            'rule_spam_score': safe_round(result.get('rule_spam_score')),
            'base_score': safe_round(result.get('base_score')),
            'profanity_boost': safe_round(result.get('profanity_boost')),
            'weights': {
                'bert': round(result.get('weights', {}).get('bert', 0.0), 2),
                'llm': round(result.get('weights', {}).get('llm', 0.0), 2)
            },
            'spam_weights': {
                'llm': 0.6 if result.get('rule_spam_score', 0) < 80 else 0.3,
                'rule': 0.4 if result.get('rule_spam_score', 0) < 80 else 0.7
            },
            'rag': {
                'enabled': bool(result.get('rag_enabled', False)),
                'adjustment_applied': adjustment_applied,
                'adjustment_weight': round(result.get('adjustment_weight', 0.0), 2) if adjustment_applied else 0.0,
                'similar_cases_count': result.get('similar_cases_count', 0),
                'max_similarity': round(result.get('max_similarity', 0.0), 2),
                'adjusted_score': safe_round(result.get('adjusted_immoral_score')) if adjustment_applied and result.get('adjusted_immoral_score') is not None else None,
                'adjusted_spam_score': safe_round(result.get('adjusted_spam_score')) if adjustment_applied and result.get('adjusted_spam_score') is not None else None,
                'similar_cases': rag_similar_cases
            }
        },
        'rag_applied': adjustment_applied
    }


@router.post("/ethics/analyze", response_model=EthicsAnalyzeResponse, tags=["ethics"])
async def ethics_analyze(request_data: EthicsAnalyzeRequest, request: Request):
    """
    í…ìŠ¤íŠ¸ ë¹„ìœ¤ë¦¬/ìŠ¤íŒ¸ ë¶„ì„ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
    
    - **text**: ë¶„ì„í•  í…ìŠ¤íŠ¸ (ìµœëŒ€ 1000ì)
    
    Returns:
    - ë¹„ìœ¤ë¦¬ ì ìˆ˜, ì‹ ë¢°ë„, ìŠ¤íŒ¸ ì§€ìˆ˜, ìœ í˜• ì •ë³´ ë“±
    """
    global ethics_analyzer
    
    # ì§€ì—° ë¡œë”©: ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨í•œ ê²½ìš° ì¬ì‹œë„
    if ethics_analyzer is None:
        try:
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘ (ì¬ì‹œë„)...")
            from ethics.ethics_hybrid_predictor import HybridEthicsAnalyzer
            ethics_analyzer = HybridEthicsAnalyzer()
            print("[INFO] Ethics ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. models/ ë””ë ‰í† ë¦¬ì™€ .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    if ethics_analyzer is None:
        raise HTTPException(status_code=503, detail="ë¶„ì„ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    start_time = time.time()
    
    try:
        result = ethics_analyzer.analyze(request_data.text)
        simplified = simplify_result(result)
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time = time.time() - start_time
        
        # ë¡œê·¸ ì €ì¥
        try:
            from ethics.ethics_db_logger import db_logger
            log_id = db_logger.log_analysis(
                text=simplified['text'],
                score=simplified['score'],
                confidence=simplified['confidence'],
                spam=simplified['spam'],
                spam_confidence=simplified['spam_confidence'],
                types=simplified['types'],
                ip_address=request.client.host,
                user_agent=request.headers.get('user-agent'),
                response_time=response_time,
                rag_applied=simplified.get('rag_applied', False),
                auto_blocked=result.get('auto_blocked', False)
            )
            
            # RAG ìƒì„¸ ì •ë³´ ì €ì¥ (RAGê°€ ì ìš©ëœ ê²½ìš°)
            if simplified.get('rag_applied', False) and log_id:
                try:
                    rag_info = simplified.get('detailed', {}).get('rag', {})
                    db_logger.log_rag_details(
                        ethics_log_id=log_id,
                        similar_case_count=rag_info.get('similar_cases_count', 0),
                        max_similarity=rag_info.get('max_similarity', 0.0),  # ì´ë¯¸ 0-1 ë²”ìœ„
                        original_immoral_score=simplified.get('detailed', {}).get('base_score', simplified['score']),
                        original_spam_score=result.get('base_spam_score', simplified.get('spam', 0.0)),  # RAG ë³´ì • ì „ ìŠ¤íŒ¸ ì ìˆ˜
                        adjusted_immoral_score=rag_info.get('adjusted_score', simplified['score']),
                        adjusted_spam_score=rag_info.get('adjusted_spam_score', simplified['spam']),
                        adjustment_weight=rag_info.get('adjustment_weight', 0.0),
                        confidence_boost=0.0,  # ë³„ë„ ê³„ì‚° í•„ìš” ì‹œ ì¶”ê°€
                        similar_cases=rag_info.get('similar_cases', []),
                        rag_response_time=response_time
                    )
                except Exception as rag_log_error:
                    print(f"[WARN] RAG ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {rag_log_error}")
        except Exception as log_error:
            print(f"[WARN] ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {log_error}")
        
        return simplified
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@router.get("/ethics/logs", tags=["ethics"])
async def get_ethics_logs(
    limit: int = Query(100, description="ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜"),
    offset: int = Query(0, description="ì‹œì‘ ìœ„ì¹˜"),
    min_score: Optional[float] = Query(None, description="ìµœì†Œ ì ìˆ˜ í•„í„°"),
    max_score: Optional[float] = Query(None, description="ìµœëŒ€ ì ìˆ˜ í•„í„°"),
    start_date: Optional[str] = Query(None, description="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
):
    """
    Ethics ë¶„ì„ ë¡œê·¸ ì¡°íšŒ
    
    - **limit**: ìµœëŒ€ ì¡°íšŒ ê°œìˆ˜ (ê¸°ë³¸ê°’: 100)
    - **offset**: ì‹œì‘ ìœ„ì¹˜ (ê¸°ë³¸ê°’: 0)
    - **min_score**: ìµœì†Œ ì ìˆ˜ í•„í„°
    - **max_score**: ìµœëŒ€ ì ìˆ˜ í•„í„°
    - **start_date**: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
    - **end_date**: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
    """
    try:
        from ethics.ethics_db_logger import db_logger
        logs = db_logger.get_logs_with_rag(
            limit=limit,
            offset=offset,
            min_score=min_score,
            max_score=max_score,
            start_date=start_date,
            end_date=end_date
        )
        return {
            "logs": logs,
            "count": len(logs),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/ethics/logs/stats", tags=["ethics"])
async def get_ethics_statistics(days: int = Query(7, description="ì¡°íšŒí•  ì¼ìˆ˜")):
    """
    Ethics í†µê³„ ì •ë³´ ì¡°íšŒ
    
    - **days**: ì¡°íšŒí•  ì¼ìˆ˜ (ê¸°ë³¸ê°’: 7ì¼)
    
    Returns:
    - ì „ì²´ ê±´ìˆ˜, í‰ê·  ì ìˆ˜, ê³ ìœ„í—˜ ê±´ìˆ˜, ìŠ¤íŒ¸ ê±´ìˆ˜, ì¼ë³„ í†µê³„
    """
    try:
        from ethics.ethics_db_logger import db_logger
        stats = db_logger.get_statistics(days=days)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/{log_id}", tags=["ethics"])
async def delete_ethics_log(log_id: int):
    """
    íŠ¹ì • Ethics ë¡œê·¸ ì‚­ì œ
    
    - **log_id**: ì‚­ì œí•  ë¡œê·¸ì˜ ID
    
    Returns:
    - ì‚­ì œ ì„±ê³µ ë©”ì‹œì§€
    """
    try:
        from ethics.ethics_db_logger import db_logger
        success = db_logger.delete_log(log_id)
        if success:
            return {
                "success": True,
                "message": f"ë¡œê·¸ ID {log_id} ì‚­ì œ ì™„ë£Œ"
            }
        else:
            raise HTTPException(status_code=404, detail="í•´ë‹¹ ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.delete("/ethics/logs/batch/old", tags=["ethics"])
async def delete_old_ethics_logs(days: int = Query(90, description="ë³´ê´€ ê¸°ê°„ (ì¼)")):
    """
    ì˜¤ë˜ëœ Ethics ë¡œê·¸ ì‚­ì œ
    
    - **days**: ë³´ê´€ ê¸°ê°„ (ê¸°ë³¸ê°’: 90ì¼, 0ì´ë©´ ëª¨ë“  ë¡œê·¸ ì‚­ì œ)
    
    Returns:
    - ì‚­ì œëœ ë¡œê·¸ ìˆ˜
    """
    try:
        from ethics.ethics_db_logger import db_logger
        if days == 0:
            # ëª¨ë“  ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_all_logs()
            return {
                "deleted_count": deleted_count,
                "message": f"ëª¨ë“  ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
        else:
            # ì§€ì •ëœ ê¸°ê°„ ì´ì „ ë¡œê·¸ ì‚­ì œ
            deleted_count = db_logger.delete_old_logs(days=days)
            return {
                "deleted_count": deleted_count,
                "message": f"{days}ì¼ ì´ì „ ë¡œê·¸ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ"
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¡œê·¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/risk/top", tags=["risk"])
async def get_risk_top_users(limit: int = Query(10, ge=1, le=100, description="ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜")):
    """
    ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
    
    - **limit**: ì¡°íšŒí•  ì‚¬ìš©ì ìˆ˜ (ê¸°ë³¸ê°’: 10, ìµœëŒ€: 100)
    
    Returns:
    - summary: í†µê³„ ìš”ì•½ ì •ë³´
    - users: ê³ ìœ„í—˜ ì‚¬ìš©ì ëª©ë¡
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import get_recent_high_risk, init_db
        from datetime import datetime
        
        # DB ì´ˆê¸°í™” (ì—†ìœ¼ë©´ ìƒì„±)
        init_db()
        
        # ê³ ìœ„í—˜ ë°ì´í„° ì¡°íšŒ (confirmed=0ì¸ í•­ëª©ë§Œ - ì•„ì§ ì²˜ë¦¬í•˜ì§€ ì•Šì€ ê²ƒë“¤)
        risk_data = get_recent_high_risk(limit=limit, only_unconfirmed=True)
        
        if not risk_data:
            return {
                "summary": {
                    "total_users": 0,
                    "high_priority_count": 0,
                    "medium_priority_count": 0,
                    "avg_risk_score": 0.0
                },
                "users": []
            }
        
        # ì‚¬ìš©ìë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ user_idì˜ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ ì‚¬ìš©ìë¡œ)
        user_dict = {}
        for item in risk_data:
            user_id = item['user_id']
            if user_id not in user_dict:
                user_dict[user_id] = {
                    'chunk_id': item['chunk_id'],
                    'user_id': user_id,
                    'username': f"ì‚¬ìš©ì_{user_id}",
                    'post_id': item.get('post_id', ''),
                    'risk_score': item['risk_score'],
                    'confirmed': bool(item.get('confirmed', 0)),
                    'evidence_sentences': [],
                    'last_activity': item.get('created_at', datetime.now().isoformat()),
                    'feedback_at': item.get('created_at') if item.get('confirmed') else None
                }
            
            # ë¬¸ì¥ ì¶”ê°€
            user_dict[user_id]['evidence_sentences'].append(item['sentence'])
            
            # ê°€ì¥ ë†’ì€ risk_score ì‚¬ìš©
            if item['risk_score'] > user_dict[user_id]['risk_score']:
                user_dict[user_id]['risk_score'] = item['risk_score']
                user_dict[user_id]['chunk_id'] = item['chunk_id']
        
        # ì‚¬ìš©ì ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        users = []
        for user_data in user_dict.values():
            # Priority ê²°ì • (risk_score >= 0.7: HIGH, >= 0.5: MEDIUM, ê·¸ ì™¸: LOW)
            if user_data['risk_score'] >= 0.7:
                priority = 'HIGH'
            elif user_data['risk_score'] >= 0.5:
                priority = 'MEDIUM'
            else:
                priority = 'LOW'
            
            # ì œì•ˆ ì¡°ì¹˜ì‚¬í•­ ìƒì„±
            if priority == 'HIGH':
                suggested_action = "ì¦‰ì‹œ ì—°ë½ ë° ê°œì„  ì¡°ì¹˜ í•„ìš”. ê³ ìœ„í—˜ ì´íƒˆ ì§•í›„ ê°ì§€ë¨."
            elif priority == 'MEDIUM':
                suggested_action = "ëª¨ë‹ˆí„°ë§ ê°•í™” ë° ì˜ˆë°©ì  ì¡°ì¹˜ ê¶Œì¥."
            else:
                suggested_action = "ì •ê¸° ëª¨ë‹ˆí„°ë§ ê¶Œì¥."
            
            users.append({
                **user_data,
                'priority': priority,
                'similar_patterns_count': len(user_data['evidence_sentences']),
                'suggested_action': suggested_action
            })
        
        # risk_score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        users.sort(key=lambda x: x['risk_score'], reverse=True)
        
        # í†µê³„ ê³„ì‚°
        high_priority_count = sum(1 for u in users if u['priority'] == 'HIGH')
        medium_priority_count = sum(1 for u in users if u['priority'] == 'MEDIUM')
        avg_risk_score = sum(u['risk_score'] for u in users) / len(users) if users else 0.0
        
        return {
            "summary": {
                "total_users": len(users),
                "high_priority_count": high_priority_count,
                "medium_priority_count": medium_priority_count,
                "avg_risk_score": round(avg_risk_score, 2)
            },
            "users": users
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"ê³ ìœ„í—˜ ì‚¬ìš©ì ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")


class RiskFeedbackBase(BaseModel):
    chunk_id: str
    sentence: str
    pred_score: float
    final_label: str


class RiskFeedbackRequest(RiskFeedbackBase):
    """ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ìš”ì²­"""
    confirmed: bool


class CheckNewPostRequest(BaseModel):
    """ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ìš”ì²­"""
    text: str
    user_id: str
    post_id: str
    created_at: str


class AutoAnalyzeRequest(BaseModel):
    """ìë™ RAG ë¶„ì„ ìš”ì²­"""
    user_id: str
    post_id: str
    post_type: str = Field("post", description="post/comment ë“±")
    text: str
    created_at: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class AutoAnalyzeResponse(BaseModel):
    id: int
    risk_score: float
    priority: str
    decision: Dict[str, Any]
    evidence_count: int


@router.get("/risk/collection_stats", tags=["risk"])
async def get_risk_collection_stats():
    """
    ë²¡í„° DB ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ

    Returns:
        Dict[str, Any]: ì»¬ë ‰ì…˜ ì´ë¦„ê³¼ ë¬¸ì„œ ìˆ˜, ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°
    """
    try:
        from chrun_backend.rag_pipeline.vector_db import get_client, get_collection_stats

        client = get_client()
        stats = get_collection_stats(client)

        if "error" in stats:
            raise HTTPException(status_code=500, detail=f"ë²¡í„°DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {stats['error']}")

        return {
            "name": stats.get("collection_name", "confirmed_risk"),
            "count": stats.get("total_documents", 0),
            "status": stats.get("status", "unknown")
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("[RISK] ë²¡í„°DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ë²¡í„°DB í†µê³„ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")


def _build_safe_risk_response(
    request_data: CheckNewPostRequest,
    error: Optional[str] = None
) -> Dict[str, Any]:
    """ì—ëŸ¬ ìƒí™©ì—ì„œ ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    from chrun_backend.rag_pipeline.rag_checker import _create_safe_decision

    decision = _create_safe_decision()
    decision["confidence"] = "Uncertain"

    response: Dict[str, Any] = {
        "post": {
            "user_id": request_data.user_id,
            "post_id": request_data.post_id,
            "created_at": request_data.created_at,
            "original_text": request_data.text,
        },
        "decision": decision,
        "evidence": [],
    }

    if error:
        response["error"] = error

    return response


def _ensure_risk_response_schema(
    result: Dict[str, Any],
    request_data: CheckNewPostRequest
) -> Dict[str, Any]:
    """ì‘ë‹µ ê°ì²´ê°€ í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ(post/decision/evidence)ë¥¼ ë§Œì¡±í•˜ë„ë¡ ë³´ì •í•©ë‹ˆë‹¤."""
    if not isinstance(result, dict):
        logger.warning("[RISK] check_new_post ê²°ê³¼ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤. ì•ˆì „ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        return _build_safe_risk_response(request_data, error="Invalid response type")

    post_payload = result.get("post") or {}
    decision_payload = result.get("decision") or {}
    evidence_payload = result.get("evidence") or []

    if not isinstance(evidence_payload, list):
        logger.warning("[RISK] evidenceê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        evidence_payload = []

    post_data = {
        "user_id": post_payload.get("user_id") or request_data.user_id,
        "post_id": post_payload.get("post_id") or request_data.post_id,
        "created_at": post_payload.get("created_at") or request_data.created_at,
        "original_text": post_payload.get("original_text") or request_data.text,
    }

    # â­ Evidenceê°€ ì—†ì–´ë„ LLM ê²°ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš© (EvidenceëŠ” ì°¸ê³  ìë£Œì¼ ë¿)
    if not isinstance(decision_payload, dict):
        logger.warning("[RISK] decisionì´ dictê°€ ì•„ë‹™ë‹ˆë‹¤. ì•ˆì „ ê²°ì •ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        decision_payload = {}
    
    # LLMì´ ì •ìƒ ë¶„ì„í–ˆëŠ”ì§€ í™•ì¸ (risk_scoreê°€ ìˆê³  ê¸°ë³¸ê°’ ì•„ë‹˜)
    has_valid_llm_decision = (
        decision_payload.get("risk_score") is not None and 
        decision_payload.get("priority") and
        decision_payload.get("reasons") and
        # ê¸°ë³¸ fallback ë©”ì‹œì§€ê°€ ì•„ë‹Œì§€ í™•ì¸
        "ìœ ì‚¬í•œ ìœ„í—˜ ë¬¸ì¥ì´ ë°œê²¬ë˜ì§€ ì•ŠìŒ" not in str(decision_payload.get("reasons", []))
    )
    
    # Evidence ì—†ê³  LLM ê²°ì •ë„ ì—†ìœ¼ë©´ safe_response ì‚¬ìš©
    if not evidence_payload and not has_valid_llm_decision:
        logger.warning("[RISK] Evidenceì™€ ìœ íš¨í•œ LLM ê²°ì •ì´ ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤. ì•ˆì „ ì‘ë‹µ ë°˜í™˜")
        safe_response = _build_safe_risk_response(request_data)
        if "fallback_reason" in decision_payload:
            safe_response["decision"]["fallback_reason"] = decision_payload["fallback_reason"]
        return safe_response

    # Evidence ì—†ì–´ë„ LLM ê²°ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if not evidence_payload:
        logger.info("[RISK] Evidence ì—†ìŒ. LLMì´ ì›ë¬¸ë§Œìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ ì‚¬ìš©")
    
    decision_payload.setdefault("confidence", "Uncertain" if not evidence_payload else "Low")

    return {
        "post": post_data,
        "decision": decision_payload,
        "evidence": evidence_payload,
    }


@router.post("/risk/feedback", tags=["risk"])
async def submit_risk_feedback(request_data: RiskFeedbackRequest):
    """
    ê³ ìœ„í—˜ ì‚¬ìš©ì í”¼ë“œë°± ì œì¶œ
    
    - **chunk_id**: í”¼ë“œë°±í•  chunk_id
    - **confirmed**: ìœ„í—˜ í™•ì¸ ì—¬ë¶€ (true: ìœ„í—˜ ë§ìŒ, false: ìœ„í—˜ ì•„ë‹˜)
    
    Returns:
    - ì„±ê³µ ë©”ì‹œì§€
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import update_feedback, get_chunk_by_id, log_feedback_event
        
        sentence = request_data.sentence.strip() if request_data.sentence else ""
        if not sentence:
            raise HTTPException(status_code=422, detail="sentence í•„ë“œëŠ” ë¹„ì›Œë‘˜ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        try:
            pred_score = float(request_data.pred_score)
        except (TypeError, ValueError):
            raise HTTPException(status_code=422, detail="pred_scoreëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")

        final_label = request_data.final_label.strip().upper()
        if final_label not in {"MATCH", "MISMATCH", "UPDATE"}:
            raise HTTPException(status_code=422, detail="final_labelì€ MATCH/MISMATCH/UPDATE ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

        # 1. ê¸°ì¡´ SQLite í”¼ë“œë°± ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        update_feedback(request_data.chunk_id, request_data.confirmed)
        chunk_snapshot: Optional[Dict[str, Any]] = None
        
        # 2. confirmed=trueì¸ ê²½ìš°ì—ë§Œ ë²¡í„°DBì— ì €ì¥
        if request_data.confirmed:
            try:
                # 2-1. SQLiteì—ì„œ í•´ë‹¹ chunk ì •ë³´ ì¡°íšŒ
                chunk_data = get_chunk_by_id(request_data.chunk_id)
                chunk_snapshot = chunk_data
                
                if not chunk_data:
                    # chunkë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                    print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: chunk_id {request_data.chunk_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                else:
                    # 2-2. ì„ë² ë”© ìƒì„±
                    from chrun_backend.rag_pipeline.embedding_service import get_embedding
                    sentence = chunk_data.get('sentence', '')
                    
                    if sentence.strip():
                        embedding = get_embedding(sentence)
                        
                        # 2-3. ë²¡í„°DBì— ì €ì¥í•  ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        from chrun_backend.rag_pipeline.vector_db import build_chunk_id
                        
                        # ì•ˆì •ì ì¸ chunk_id ìƒì„± (ê¸°ì¡´ chunk_idì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                        vector_chunk_id = build_chunk_id(sentence, chunk_data.get('post_id', ''))
                        
                        meta = {
                            "chunk_id": vector_chunk_id,  # ë²¡í„°DBìš© ì•ˆì •ì  ID
                            "original_chunk_id": chunk_data.get('chunk_id'),  # ì›ë³¸ SQLite chunk_id
                            "user_id": chunk_data.get('user_id', ''),
                            "post_id": chunk_data.get('post_id', ''),
                            "sentence": sentence,
                            "risk_score": float(chunk_data.get('risk_score', 0.0)),
                            "created_at": chunk_data.get('created_at', ''),
                            "confirmed": True
                        }
                        
                        # 2-4. ë²¡í„°DBì— upsert (idempotent)
                        from chrun_backend.rag_pipeline.vector_db import get_client, upsert_confirmed_chunk
                        
                        client = get_client()  # ê¸°ë³¸ ê²½ë¡œ "./chroma_store" ì‚¬ìš©
                        upsert_confirmed_chunk(client, embedding, meta)
                        
                        print(f"[INFO] í™•ì¸ëœ ìœ„í—˜ ë¬¸ì¥ì„ ë²¡í„°DBì— ì €ì¥ ì™„ë£Œ: {vector_chunk_id}")
                    else:
                        print(f"[WARN] ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨: ë¹ˆ ë¬¸ì¥ (chunk_id: {request_data.chunk_id})")
                        
            except Exception as vector_error:
                # ë²¡í„°DB ì €ì¥ ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ í”¼ë“œë°±ì€ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                import traceback
                print(f"[ERROR] ë²¡í„°DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {vector_error}")
                traceback.print_exc()
                # ì—ëŸ¬ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  APIëŠ” ì„±ê³µìœ¼ë¡œ ì‘ë‹µ
        
        if chunk_snapshot is None:
            chunk_snapshot = get_chunk_by_id(request_data.chunk_id)
        user_id_for_hash = chunk_snapshot.get('user_id') if chunk_snapshot else None

        event_id = log_feedback_event(
            chunk_id=request_data.chunk_id,
            sentence=sentence[:500],
            pred_score=max(0.0, min(1.0, pred_score)),
            final_label=final_label,
            confirmed=request_data.confirmed,
            user_id=user_id_for_hash
        )

        return {
            "status": "ok",
            "feedback_id": event_id,
            "chunk_id": request_data.chunk_id,
            "final_label": final_label,
            "pred_score": round(max(0.0, min(1.0, pred_score)), 3),
            "confirmed": request_data.confirmed
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")


@router.get("/risk/feedback", tags=["risk"])
async def list_risk_feedback(limit: int = Query(50, ge=1, le=200)):
    """
    í”¼ë“œë°± ì´ë²¤íŠ¸ ëª©ë¡ ì¡°íšŒ
    """
    try:
        from chrun_backend.rag_pipeline.high_risk_store import get_feedback_events

        events = get_feedback_events(limit=limit)
        return {
            "items": events,
            "count": len(events)
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("[RISK] í”¼ë“œë°± ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"í”¼ë“œë°± ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")


@router.post("/risk/analyze", response_model=AutoAnalyzeResponse, tags=["risk"])
async def auto_analyze_risk(request_data: AutoAnalyzeRequest):
    """
    ìë™ RAG ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    """
    analysis_request = AnalysisRequest(
        user_id=request_data.user_id,
        post_id=request_data.post_id,
        post_type=request_data.post_type,
        text=request_data.text,
        created_at=request_data.created_at,
        metadata=request_data.metadata,
    )
    result = analyze_and_store(analysis_request)
    context = result["context"]
    decision = context.get("decision", {})
    return AutoAnalyzeResponse(
        id=result["id"],
        risk_score=float(decision.get("risk_score", 0.0)),
        priority=decision.get("priority", "LOW"),
        decision=decision,
        evidence_count=len(context.get("evidence", [])),
    )


@router.get("/risk/analysis_results", tags=["risk"])
async def list_analysis_results(limit: int = Query(50, ge=1, le=200)):
    """
    ì €ì¥ëœ RAG ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ
    """
    try:
        items = get_recent_results(limit=limit)
        return {
            "items": items,
            "count": len(items),
        }
    except Exception as e:
        logger.exception("[RISK] ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")

@router.post("/risk/check_new_post", tags=["risk"])
async def check_new_post_risk(request_data: CheckNewPostRequest):
    """
    ìƒˆ ê²Œì‹œë¬¼ì˜ ìœ„í—˜ë„ë¥¼ ì²´í¬í•˜ì—¬ ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    - **text**: ë¶„ì„í•  ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸
    - **user_id**: ì‚¬ìš©ì ID
    - **post_id**: ê²Œì‹œë¬¼ ID
    - **created_at**: ìƒì„± ì‹œê°„ (ISO í˜•ì‹, ì˜ˆ: "2024-11-04T10:30:00")
    
    Returns:
    - ìœ„í—˜ë„ ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ (ê·¼ê±° ë¬¸ì¥ë“¤ê³¼ í†µê³„ ì •ë³´)
    """
    try:
        from chrun_backend.rag_pipeline.rag_checker import check_new_post

        if not os.getenv("OPENAI_API_KEY"):
            logger.warning("[RISK] OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²°ì •ì´ ë°˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        context = check_new_post(
            text=request_data.text,
            user_id=request_data.user_id,
            post_id=request_data.post_id,
            created_at=request_data.created_at
        )

        return _ensure_risk_response_schema(context, request_data)

    except Exception as e:
        logger.exception("[RISK] ìƒˆ ê²Œì‹œë¬¼ ìœ„í—˜ë„ ì²´í¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        return _build_safe_risk_response(request_data, error=str(e))
